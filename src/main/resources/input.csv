9,"버져닝
major.minor.patch
major & minor 버젼
정식 개발 프로세스 (일반적인 개발 프로세스)
Sprint Development Life Cycle
일정 산출 > Sprint 에서 진행할 작업들을 추린다 > 개발 진행 > 작업들을 모아서 Release!
image

patch 버젼
작업 Sprint 와는 별개로, 별도의 플래닝 없이 자유롭게 작업 & 배포 가능
버그 수정
기능에 변화가 없거나 변화가 미미한 작업
하위 버젼 호환 필수
git 개발 flow
Issues 에서 이슈 생성 (작업 티켓) 생성
dev 에서 작업 브랜치 딴다
[2-1] 일반 개발: feat/[issue number]-optional-explanatory-postfix (e.g. feat/123-sthg-task)
[2-2] 핫픽스: hotfix/[issue number]-optional-explanatory-postfix (e.g. hotfix/123-urgent-task)
작업 완료 후, 작업 브랜치 -> dev 로 PR
작업 브랜치 -> dev PR 머지 시, Squash and merge 로 머지
개발 환경에 dev 브랜치로 배포하여 기능동작 확인! (BE 배포 방법, FE 배포 방법)
dev -> main 으로 PR
[6-1] sprint (major, minor version up) 개발: 모든 작업물들이 dev에 머지 완료되면 PR 날린다
[6-2] patch (patch version up) 개발: 작업 진행자가 필요 작업들이 dev에 모두 머지 완료되었다고 판단한다면 PR 날린다
dev -> main PR 머지 시, Create a merge commit 로 머지
Releases 에 작업 내역 관련하여 릴리즈 노트 작성 (main 브랜치 기준)
상용 환경에 main 브랜치로 배포하여 기능동작 확인! (BE 배포 방법, FE 배포 방법)
commit message 컨벤션
커밋 내용에 맞게 아래 나열된 커밋 메세지 포맷 중 하나를 골라 사용한다
- feat:
- fix:
- refactor:
- docs:
- style:
- chore:
- test:

e.g.
feat: 로그인 구현
test: XXX 테스트 작성
...
PR 컨벤션
제목에 커밋 컨벤션과 같이 prefix를 붙여 기능 구현인지 수정인지 명시
e.g.
feat: login 기능 구현

or

hotfix: login XXX 에러 버그 수정
기본적으로 템플릿이 존재하므로, 템플릿에 맞춰 작성하면 된다
Close된 PR들 둘러보고 참고해봐도 좋을 것 같다
주의 사항
FE Semantic Versioning 이 버젼 (major, minor, path 모두) 이 올라가는 상황마다 항상 적용 되어줘야 한다 (링크 참고)
코딩 컨벤션
BE 컨벤션
FE 컨벤션
찜꽁 초창기에 작성된 컨벤션이라 outdated 된 내용이 있을 수 있음. 코드로 직접 확인하는 것이 가장 정확!",NON_TECH
10,"LocalDateTime
타임존 정보가 없는 datetime
찜꽁 서버 내부에서는 모든 LocalDateTime 데이터는 따로 변환이 필요한 상황이 아닌 이상 UTC로 간주한다
객체 생성
기본적인 생성은 정적 팩토리 메서드 패턴을 통해서 생성 가능
이외에도 많은 메서드들이 존재해서 날짜 데이터를 만들어 낼 수 있음
// LocalDate 생성
LocalDate targetDate = LocalDate.of(2021, 7, 9);

// LocalDateTime 생성
LocalDateTime targetDateTime = targetDate.atTime(1, 0, 0);

// 혹은 LocalDateTime.of를 통해 바로 생성 가능
LocalDateTime targetDateTime = LocalDateTime.of(2021, 7, 9, 1, 0, 0);

// 현재 시간 (system 상 시간)
LocalDate now = LocalDate.now();

// 년, 월, 일, 시간 더하기도 가능
LocalDate theDayAfterTargetDate = targetDate.plusDays(1L);
Request and Response
Request상황 (@RequestBody, @RequestParam @ModelAttribute…)
@DateTimeFormat 사용
단, @RequestBody (JSON)의 경우에는 @JsonFormat 도 사용가능
두 어노테이션 모두 있으면 @JsonFormat이 진행된다
@JsonFormat이 틀리면 @DateTimeFormat이 맞더라도 직렬화는 실패한다
단, @DateTimeFormat이 있다면 @DateTimeFormat의 포맷으로 직렬화가 진행된다.
예시
// @RequestBody (DTO)

public class ReservationSaveRequest {
 @NotNull(message = ""비어있는 항목을 입력해주세요."")
 private Long spaceId;

 @DateTimeFormat(pattern = ""yyyy-MM-dd'T'HH:mm:ss"")
 @NotNull(message = ""비어있는 항목을 입력해주세요."")
 private LocalDateTime startDateTime;

 …
}

// @ReqeustParam
@GetMapping(""/reservations"")
public ResponseEntity<ReservationFindAllResponse> findAll(
 @PathVariable Long mapId,
 @RequestParam @DateTimeFormat(pattern = ""yyyy-MM-dd"") LocalDate date) {
 …
}

Response 상황
Response Body에서는 @JsonFormat만 가능
public class ReservationResponse {
 @JsonProperty
 private Long id;
 @JsonFormat(shape = JsonFormat.Shape.STRING, pattern = ""yyyy-MM-dd'T'HH:mm:ss"")
 private LocalDateTime startDateTime;

 …
}
@JsonFormat 사용시 다른 필드들이 인식되지 않는 이슈가 발생
@JsonFormat을 붙여준 필드 이외의 필드들에 모두 @JsonProperty 어노테이션을 붙여줘야 다른 response DTO 필드들도 인식됨
그리고 최상위 DTO에서 @JsonIgnoreProperties(ignoreUnknown = true)를 붙여줘야 했음
안그러면 ‘timestamp’라는 Unknown 필드가 ignore 되지 않았다는 에러 메세지가 떴음
아직도 원인을 모르겠음
@JsonIgnoreProperties(ignoreUnknown = true)
public class ReservationFindAllResponse {
 @JsonProperty
 private List<ReservationSpaceResponse> data;
 …
}
ZonedDateTime
타임존 정보를 포함한 datetime
LocalDateTime + TimeZone의 결합 형태라고 생각하면 된다
객체 생성
// LocalDateTime에 timezone 정보를 입혀서 생성
LocalDateTime dateTime = LocalDateTime.now();
ZonedDateTime zonedDateTime = dateTime.atZone(TimeZone.getTimeZone(""Asia/Seoul"").toZoneId());

// 즉, zonedDateTime은 dateTime이 한국 시간대임을 나타내고있는 시간데이터 객체

// 팩토리 메서드도 있다
ZonedDateTime zonedDateTime = ZonedDateTime.of(dateTime, ZoneId.of(""Asia/Seoul""))
변환하기
// step
1 LocalDateTime dateTime = LocalDateTime.now();
2 ZonedDateTime zonedDateTimeKST = ZonedDateTime.of(dateTime, ZoneId.of(""Asia/Seoul""));
3 ZonedDateTime zonedDateTimeUTC = zonedDateTimeKST.withZoneSameInstant(ZoneId.of(""UTC"")); // 다른 타임존으로 변환된 ZonedDateTime
4 LocalDateTime dateTimeConverted = zonedDateTimeUTC.toLocalDateTime();
위의 변환이 의미하는 바를 구체적인 예시로 풀어서 설명하면 다음과 같다:
// 위와 비교해서 보세요
1 LocalDateTime dateTime = 2022-02-17 10:00:00 // 순수한 DateTime (어떠한 타임존 정보도 없다. 걍 2022년 2월 17일 오전 10시)
2 ZonedDateTime zonedDateTimeKST = 한국 2022-02-17 10:00:00
3 ZonedDateTime zonedDateTimeUTC = 한국 2022-02-17 10:00:00을 UTC로 변환한 시간. 즉, UTC 2022-02-17 01:00:00 입니다 (한국은 UTC 표준 시간대 보다 9시간 앞선다)
4 LocalDateTime dateTimeConverted = 2022-02-17 01:00:00
이 로직은 TimeZoneUtils의 convert 메서드에 구현되어있다
이런 Zone에 따른 변환과정이 이해가 가기 시작했다면 이제 타임존관련 코드는 가소로울 것
비교
isAfter, isBefore, isEqual 세가지 메서드 존재
between도 있음
예약 일시를 비교하는 로직이 필요할 때 위의 메서드들을 조합해서 구현 가능 (e.g. 예약 가능한 시간인지 검증하는 로직)
예시: Reservation Enttiy의 hasConflictWith 메서드 참고
Reference
https://jojoldu.tistory.com/361
JavaDoc
Pages 27
✅ 찜꽁 ✅
🏡 Home
💻 공통
작업 매뉴얼
🛠 백엔드
API 문서화
EC2 Ubuntu 접속
Jenkins(deprecated)
JWT
Java 시간 다루기
정규표현식 (Regular Expression)
DB flyway
DB 백업
DB 백업 자동화
DB Indexing
DB dummy data loop INSERT
서브모듈
docker image
Time Zone Issue
키바나 활용하기
카프카 도입기
BE 배포
zzimkkong CLI
Troubleshooting 매뉴얼
Application local 실행
슬랙 메세지 발송 실패 이슈 트러블 슈팅
🧩프론트엔드
컴포넌트 API
시맨틱 버저닝
FE 배포
Clone this wiki locally
https://github.com/woowacourse-teams/2021-zzimkkong.wiki.git
Footer",BE
11,"MemberSaveRequestDto에서 비밀번호와 조직명 입력값을 검증하기 위해서 사용
비밀번호
(필수) 영어 + 숫자 8자 이상, (선택) 특수문자 입력가능
@NotBlank(message = ""비밀번호는 공백일 수 없습니다"")
@Pattern(regexp = ""^(?=.*[a-zA-Z])(?=.*[0-9]).{8,20}$"", message = ""비밀번호는 영어와 숫자를 포함해서 8자 이상 20자 이내로 입력해주세요"")
private String password;
^ : 문자열의 시작을 나타냄
$ : 문자열의 끝을 나타냄
?= : 전방탐색(lookahead)패턴, 일치 영역을 발견해도 그 값을 반환하지 않는 패턴
// 예시
http://www.forta.com
https://mail.forta.com
ftp://ftp.forta.com

// 정규 표현식
.+(?=:)

// 결과
http:
https:
ftp:
.* : any character zero or more times
[0-9] : 0에서 9까지의 숫자
[a-zA-Z] : a에서 z까지의 대소문자 알파벳
{8, 20} : 8자에서 20자 사이
조직명
조직명 : 1자 이상, 특수문자 일부 허용([ - , _ , !, ?, ., , ])
@NotBlank(message = ""조직명은 공백일 수 없습니다"")
@Pattern(regexp = ""^[-_!?.,a-zA-Z0-9가-힣ㄱ-ㅎㅏ-ㅣ]{1,20}$"", message = ""조직명은 특수문자(-_!?.,)를 포함하여 20자 이내로 작성 가능합니다"")
private String organization;
[-_!?.,a-zA-Z0-9가-힣ㄱ-ㅎㅏ-ㅣ] : 가능한 특수문자 + 모든 알파벳 + 모든 숫자 + 모든 한글
{1, 20} : 1자에서 20자 사이
Regex test site
https://regex101.com/

References
https://stackoverflow.com/questions/22937618/reference-what-does-this-regex-mean/22944075#22944075

https://mkyong.com/tutorials/java-regular-expression-tutorials/",BE
12,"Flyway란?
Flyway는 오픈소스 마이그레이션 툴이다.
소스코드를 형상관리하는 Git과 같이 Flyway는 버전 관리 목적인 SCHEMA_VERSION 테이블을 통해 SQL 스크립트의 변화를 추적하면서 자동적으로 DB를 관리한다.
준비
스프링 부트 build.gradle에서 의존성을 추가한다.
dependencies {
 implementation 'org.flywaydb:flyway-core:6.4.2'
}
application.properties에 아래 설정을 추가한다.
# flyway
spring.flyway.baseline-on-migrate=true
spring.flyway.baseline-version=0
spring.flyway.useMysqlMetadata=true
spring.flyway.baseline-on-migrate

기본값 : false
false로 하려면 flyway_schema_history 테이블이 생성되어 있어야 함
true로 하면 flyway_schema_history 테이블이 없는 경우 생성함
spring.flyway.baseline-version

기본값 : 1
보통 버전파일이 V1__의 형태로 버전 1부터 시작하기 때문에 0으로 하는 것을 추천
flyway 적용 후 DB 변경방법
1. sql 파일 생성
resources/db/migration/prod 위치에 sql 파일을 생성한다.
파일의 네이밍 규칙은 다음과 같다.
naming
왼쪽부터 prefix, version, description, suffix 순
prefix : default로 V는 버전 마이그레이션, R은 반복 마이그레이션 용 접두사 (반드시 V 혹은 R로 시작)
version : version은 버전 마이그레이션에서만 사용되며 숫자나 언더바 조합으로 사용
version과 description 사이의 __ (언더바 2개)는 반드시 있어야 함 (1개면 에러 발생)
description : schema_version 테이블에 저장시 설명으로 사용
suffix : 확장자, 기본은 .sql
2. 파일에 DB에 변경하고자 하는 내용의 SQL문을 작성한다.
컬럼 추가
ALTER TABLE 테이블이름 ADD COLUMN 컬럼이름 varchar(20) NOT NULL;
컬럼 추가 (디폴트값 지정)
ALTER TABLE 테이블이름 ADD COLUMN 컬럼이름 varchar(20) NOT NULL DEFAULT 기본값;
컬럼 변경
ALTER TABLE 테이블이름 MODIFY COLUMN 컬럼이름 varchar(20) NOT NULL;
컬럼 이름 변경
ALTER TABLE 테이블이름 CHANGE COLUMN 기존컬럼이름 변경할컬럼이름 varchar(20) NOT NULL;
컬럼 삭제
ALTER TABLE 테이블이름 DROP COLUMN 컬럼이름;
3. 작성한 sql 문과 함께 기능에 대한 PR을 날린 뒤, dev에 머지가 되면 빌드시 DB에 변경사항이 반영된다.
❗️주의❗️
한번 작성 후 빌드가 된 sql문은 수정 혹은 삭제를 하면 안 됩니다. 변경이 필요하다면 버전을 올린 sql문을 추가로 작성해주셔야 합니다.

적용 때 발생한 이슈
초기 flyway sql 버젼 V1__init.sql 적용 불가 이슈
문제 상황
다음과 같은 history 내역을 남기며 DB에는 V1__init.sql 내부 쿼리문들이 적용되지 않는 이슈가 있었음
mysql> select * from flyway_schema_history;
+----------------+---------+-----------------------+----------+-----------------------+----------+--------------+---------------------+----------------+---------+
| installed_rank | version | description | type | script | checksum | installed_by | installed_on | execution_time | success |
+----------------+---------+-----------------------+----------+-----------------------+----------+--------------+---------------------+----------------+---------+
| 1 | 1 | << Flyway Baseline >> | BASELINE | << Flyway Baseline >> | NULL | root | 2019-11-19 10:47:52 | 0 | 1 |
+----------------+---------+-----------------------+----------+-----------------------+----------+--------------+---------------------+----------------+---------+
원인
application-prod.properties 파일에 spring.flyway.baseline-version=0 설정이 누락되어 있었음
위 baseline version의 default 설정은 1이기 때문에 flyway가 우리가 처음 작성한 V1 파일을 무시했던 것임
해결
단순히 V2 파일을 추가로 만들어서 해결함
reference를 참고했음
Pages 27
✅ 찜꽁 ✅
🏡 Home
💻 공통
작업 매뉴얼
🛠 백엔드
API 문서화
EC2 Ubuntu 접속
Jenkins(deprecated)
JWT
Java 시간 다루기
정규표현식 (Regular Expression)
DB flyway
DB 백업
DB 백업 자동화
DB Indexing
DB dummy data loop INSERT
서브모듈
docker image
Time Zone Issue
키바나 활용하기
카프카 도입기
BE 배포
zzimkkong CLI
Troubleshooting 매뉴얼
Application local 실행
슬랙 메세지 발송 실패 이슈 트러블 슈팅
🧩프론트엔드
컴포넌트 API
시맨틱 버저닝
FE 배포
Clone this wiki locally
https://github.com/woowacourse-teams/2021-zzimkkong.wiki.git
Footer",BE
13,"개요
DB 서버를 분리하는 과정에서 한 인스턴스에서 다른 인스턴스로 데이터 백업을 진행한 과정을 정리합니다.
이 글에서는 백업 자동화가 아닌 일회성 백업에 대해 다룹니다.
과정
백업 데이터들이 있는 기존 인스턴스에서 해야할 일
스크린샷 2021-08-25 오후 8 49 41
백업 하려는 파일을 저장할 폴더를 생성합니다.
mkdir backup
생성한 폴더에 들어가서 DB 백업 명령어를 사용해 백업파일을 만듭니다.
cd backup

// mysqldump -u 사용자계정 -p 기존데이터베이스명 > 백업파일이름.sql
mysqldump -u root -p zzimkkong > backup.sql
로컬 터미널에서 해야할 일
스크린샷 2021-08-25 오후 8 56 40
scp를 사용해 만든 백업파일을 새 인스턴스로 옮깁니다.
현재 보안그룹 관련 설정때문에 원격 > 원격 전송이 불가능해서 원격 > 로컬 > 원격으로 전송했습니다.
// 원격 > 로컬
scp -i KEY위치 ubuntu@[기존 인스턴스 IP주소]:[전송할 파일 경로] [받을 경로]
scp -i ~/Downloads/KEY-zzimkkong.pem ubuntu@52.78.60.9:/home/ubuntu/backup/backup.sql ~/Downloads

// 로컬 > 원격
scp -i KEY위치 [전송할 파일 경로] ubuntu@[새 인스턴스 IP주소]:[받을 경로]
scp -i ~/Downloads/KEY-zzimkkong.pem ~/Downloads/backup.sql ubuntu@ 13.124.27.102:/home/ubuntu/backup
데이터를 가져오려는 새 인스턴스에서 해야할 일
스크린샷 2021-08-25 오후 8 59 34
새 인스턴스에서 DB 복원 명령어를 사용해 백업파일을 데이터로 복원합니다.
cd backup

// mysql -u 사용자계정 -p 새데이터베이스명 < 백업파일이름.sql
mysql -u root -p zzimkkong < backup.sql
끝!

참고
MYSQL 백업, 복원
scp로 파일 전송하기",BE
14,"개요
DB 백업 글에서 이어지는 글입니다.
이 글은 백업 방법이 아닌 mysqldump를 이용한 백업을 crontab을 통해 자동화시키는 과정에 대해 다룹니다.
과정
1. DB 자동 백업 쉘 스크립트 작성
$ vi backup.sh

#!/bin/bash
DATE=$(date +%Y%m%d)
BACKUP_DIR=/home/ubuntu/backup_auto/
mysqldump -uroot -p1234 zzimkkong > $BACKUP_DIR""backup_""$DATE.sql

find $BACKUP_DIR -ctime +3 -exec rm -f {} ;
DATE 변수에 현재 날짜 저장 (date 뒤에 띄어쓰기가 있는 것에 주의)

BACKUP_DIR 변수에는 백업 파일들이 저장될 디렉토리 경로 저장

mysqldump 명령을 이용하여 백업 파일 생성

find 명령을 이용하여 3일이 지난 백업파일은 삭제

-ctime n 옵션은 파일의 퍼미션을 마지막으로 변경시킨 날짜를 기준으로 파일을 찾아낸다.
+n : n일 또는 n일 이전에 퍼미션이 변경된 파일
-n : 오늘 부터 n일 전 사이에 퍼미션이 변경된 파일
n : 정확히 n일 전에 퍼미션이 변경된 파일
-exec command {} ; 옵션은 찾아낸 파일들을 대상으로 특정 명령을 수행한다.
쉘 스크립트 파일에 실행 권한 주기

$ chmod 754 backup.sh
2. crontab 을 이용하여 스크립트 자동 실행
예약된 작업 리스트 확인
$ crontab -l
예약된 작업 수정
$ crontab -e

0 20 * * * /home/ubuntu/backup_auto/script/backup.sh >> /home/ubuntu/backup_auto/crontab_log/backup.log 2>&1
위 크론탭 명령에서 로그 파일이 잘 생성되도록, /home/ubuntu/backup_auto/crontab_log라는 디렉토리 만들어놓기 (디렉토리를 미리 만들어놓지 않으면 에러 발생함!!)
작업 명령 이용 방법
* * * * * 수행할 명령어
┬ ┬ ┬ ┬ ┬
│ │ │ │ └───────── 요일 (0 - 6) (0:일요일, 1:월요일, 2:화요일, …, 6:토요일)
│ │ │ └───────── 월 (1 - 12)
│ │ └───────── 일 (1 - 31)
│ └───────── 시 (0 - 23)
└───────── 분 (0 - 59)
현재 저희 DB에는 매일 20시(UTC 기준) 정각마다 명령을 수행하도록 설정해놓은 상태입니다.

20시(UTC 기준, KST 기준으로 새벽 5시)로 해놓은 이유?

DB 부하가 가장 적은 시간으로 백업시간을 정해놓는 것이 좋을 것 같다고 생각했고 저희 서비스는 새벽 시간대에 트래픽이 가장 적기 때문에 새벽 5시로 설정했습니다. 추가적으로 UTC 기준으로 작성한 이유는 EC2 서버 시간이 UTC 기준이기 때문이고 crontab을 위해서 EC2 서버 시간을 바꾸기는 리스크가 있을 것 같아서 그냥 명령수행 시간을 UTC 기준으로 작성해 놓았습니다.

cron 재시작

$ sudo service cron restart
Untitled

3. 백업 파일 확인
이렇게 쉘 스크립트 작성 후 crontab으로 해당 스크립트를 자동 실행되도록 설정하면 아래와 같이 backup sql 파일들이 자동 생성되는 것을 확인할 수 있습니다.
참고로 3일이 경과한 sql 파일들은 삭제하도록 작성했습니다.
Untitled (1)

끝!

참고
MySQL 자동 백업
Linux crontab 이용하여 작업 명령 예약하기",BE
20,"개요
Git 저장소 안에 다른 Git 저장소를 디렉토리로 분리해 넣는 것이 서브모듈이다. - git-scm.com

하나의 레포지토리안에, 또 다른 레포지토리를 두는 것을 뜻한다.


특성
기본적으로 깃 레포지토리는 .gitignore에 의해 무시되는 파일과 빈 디렉토리를 제외하고 자신의 모든 하위 디렉토리 및 파일을 관리하지만, 서브모듈로 만들어진 디렉토리의 하위 내용은 관리하지 않는다.
온라인 저장소에 올려 브라우저 상으로 확인하더라도 서브모듈의 존재는 알 수 있지만, 내부는 알 수 없다. // todo public 저장소도 그러한지 확인하기

활용
private 저장소를 서브모듈로 등록해둔다면, 해당 저장소에 대한 접근 권한이 있는 일부의 사람들만 슈퍼 프로젝트 레포지토리를 내용의 누락없이 온전히 clone 할 수 있다는 의미가 된다.
이를 응용하여, 일부의 권한이 있는 깃 계정을 사용하는 유저들에게만 공유되어야 하는 파일들(주로 보안과 관련있는 값들이 있는 파일들)을 서브모듈에 넣어 외부로부터 감출 수 있다.

사용법
전제
서브 모듈로 사용할 레포지토리는 원격 저장소에 미리 만들어두고, 필요한 파일을 세팅해두었다고 전제한다.
private 저장소를 서브모듈로 사용하고자 하는 경우, 로컬 환경에 등록된 git 계정은 해당 레포지토리에 대한 접근 권한이 있다고 전제한다.

1. 서브모듈을 등록할 때
i) 슈퍼 프로젝트 레포지토리에서 서브모듈을 추가하고자 하는 디렉토리로 이동한다.

$ cd backend/src/main/resources
ii) git submodule add 커맨드를 이용해, 서브모듈을 등록한다.

git submodule add -b ${브랜치명} ${서브모듈로 쓰고자하는 레포지토리 Url}
$ git submodule add -b main https://github.com/zzimkkong/config
iii) 최상위디렉토리/.gitmodules에 잘 추가되었다면 성공이다.

[submodule ""backend/src/main/resources/config""]
 path = backend/src/main/resources/config
 url = https://github.com/zzimkkong/config.git
 branch = main
iv) 젠킨스와의 연동은 문서 하단에 별도로 추가 서술한다. // Todo 젠킨스 직접 설정해보고 세팅법 올리기


2. 서브모듈을 사용할 때
case 1. 레포지토리를 클론 받는 경우
i) 클론 옵션에 --recurse-submodules을 붙여준다.

git clone --recurse-submodules ${슈퍼 프로젝트 레포지토리 Url}
$ git clone --recurse-submodules https://github.com/woowacourse-teams/2021-zzimkkong.git

case 2. 이미 작업중인 레포지토리가 있는 경우 (원격 저장소에 누군가 서브모듈을 등록해 Push한 경우)
i) 프로젝트를 최신화 해준다.

$ git pull
ii) 서브 모듈들을 일괄적으로 클론해온다.

$ git submodule update --init --remote

case 3. 서브모듈의 최신화
슈퍼 프로젝트 레포지토리에서 pull을 한다고 해서 서브모듈의 커밋까지 최신화 되지 않는다.

i) 각 서브 모듈을 일괄적으로 업데이트 해준다. git submodule update --remote --merge

$ git submodule update --remote --merge",BE
21,"결론 (최종 해결책)
찐 최종
[20230713 기준] 서버 내 타임존은 UTC로 관리한다!! 관련 PR (https://github.com/woowacourse-teams/2021-zzimkkong/pull/793) 참고

우리는 여태까지 타임존 역경을 헤쳐오며 마침내 문제 상황을 정확하게 파악할 수 있게 되었다. 그리고 나름대로 우리만의 괜찮은 해법도 적용해서 해결했다. 그 과정은 아래 과거의 유물들들 파트에 나와있으니 확인 바란다.

결론적으로, 최종 해결법은 허무하게도 다음과 같다.

public static void main(String[] args) {
 TimeZone.setDefault(TimeZone.getTimeZone(""Asia/Seoul""));
 SpringApplication.run(ZzimkkongApplication.class, args);
}
단순히 @PostConstruct를 안쓰고 timezone setting을 application run 전에 해주면 되는 것이었다.... TimeZone 클래스는 내부적으로 다음과 같은 default time zone static field를 가지고있다

private static volatile TimeZone defaultTimeZone;
그리고 TimeZone.setDefault()메서드는 해당 필드에 대한 setter이다. static field 이기 때문에 단순히 application이 구동 (run)에 들어가기 전에 setting만 해주면 되는 것이었다...

실험결과는 당연히 잘된다!!!!

Application 구동
Application을 구동할 때 타임존 설정 과정이 어떻게 되는지 로그를 직접 찍어봤다

// 흐름
(JVM) default timezone setting -> application context (DB connection session time zone setting) -> app running




예약 시 time sync 확인
예약 시작 시간: 오후 1시 10분 (13:10)

예약 끝나는 시간: 오후 1시 40분 (13:40)

save와 find 로직시 time sync 확인


DB에 저장상태 확인


너무 명백한 기초적인 사실이었다. 이미 모든 상황을 이해하고 있었음에도 왜 진작에 이렇게 생각 못했을까. 그 이유는 구글링해서 나온 지식을 비판적으로 수용하지않고 그대로 적용하려고만 생각했기 때문이 아닐까 (=@PostConstruct의 무지성 적용). 구글링에 의존하되 그 마법같은 해결법을 마법으로만 남겨놓으면 안되겠다는 생각이 들었다. 우리 모두 항상 조심합시다!!

마지막으로 해당 해결책에 대한 인사이트를 주신 당근마켓 플랫폼 서버 개발팀 모 개발자님께 감사의 말씀을 전합니다. 깨우침을 주셔서 정말 감사합니다...

과거의 유물들
아래의 설명은 저희가 겪은 타임존 문제 상황에 대한 이해의 용도로 읽어주시길 바랍니다.
어떠한 문제상황이 있었고, 이에 대한 원인을 파악하고 이해하는 과정 정도로 이해해주시면 될 것 같습니다.
1번 설정
ZzimkkongApplication 클래스에 default timezone 설정

// ZzimkkongApplication
@PostConstruct
void started() {
TimeZone.setDefault(TimeZone.getTimeZone(""Asia/Seoul""));
}

2번 설정
Spring Boot Profile 설정 (Application - DB 사이 session time zone 설정)

// application-prod.properties
app.datasource.master.url=jdbc:mysql://[DB서버IP]:3306/zzimkkong?characterEncoding=UTF-8&useLegacyDatetimeCode=false
…
spring.jpa.properties.hibernate.jdbc.time_zone=Asia/Seoul

// applicaiton-dev.properties
spring.datasource.url=jdbc:mysql://localhost:3306/zzimkkong?characterEncoding=UTF-8&useLegacyDatetimeCode=false
spring.jpa.properties.hibernate.jdbc.time_zone=Asia/Seoul

설정들이 필요한 이유
1번 설정
현재시간 이전의 예약시간은 예약이 불가능하도록 검증하는 부분 때문.

EC2 OS의 timezone은 UTC이므로 LocalDateTime.now()를 하면 UTC의 현재시간이 나온다. 예약자는 KST기준으로 예약하는데 검증은 UTC 현재시간으로 하니 timezone 이슈가 발생한 것. 1번 설정을 통해 Application상의 timezone을 OS 세팅에 관계없이 KST로 맞춰줄 수 있다. 이로써 이 문제는 해결.

하지만 이제 DB에 날짜 시간 관련 데이터 저장하거나 가져오는 부분에서 데이터 정합성(timezone에 따른 날짜 conversion) 문제가 간헐적으로 발생. 그래서 2번 설정이 필요하게 됨

2번 설정 (좀 깁니다 ㅠ)
사실 1번 설정을 통해 모든 것은 해결된 상태이다. DB와 데이터 정합성 문제도 사실은 1번 설정으로 해결된다!!

문제는 웹 애플리케이션을 새롭게 띄우는 경우 (i.e. 젠킨스 CI/CD로 인한 재배포) 이다. 애플리케이션을 새로 띄우면 DB와의 커넥션을 새롭게 생성한다. 이 connection session의 time zone은 별도의 설정이 없다면 Application (JVM)의 timezone을 토대로 timezone sync를 맞춰준다.

근데 1번 설정을 통해 Application timezone을 분명히 KST로 바꾸어 줬는데 왜 sync가 맞지않는 문제가 발생할까?

그 이유는 @PostConsturct 때문이다. @PostConstruct의 설명은 다음과 같다

The PostConstruct annotation is used on a method that needs to be executed after dependency injection is done to perform any initialization.

즉, 필요한 모든 Bean들이 모두 DI된 후 실행되어야하는 메서드인 것이다.

ZzimkkongApplication에 선언된 @PostConstruct이므로 모든 bean들이 application context에 올라간 후 1번 설정 메서드가 실행될 것이다. 그리고 그 bean들 중에는 DB 관련 bean들도 존재할 것이다. @PostConstruct가 선언된 메서드가 실행되기 이전에 DB 관련 bean 등록이 이루어 지는 것이다. 그리고 그 당시의 Application의 timezone은 아직 UTC이다! (@PostConstruct 실행 이전이기 때문). 결과적으로, Application이 새로 띄워지면 Application의 timezone은 분명히 KST 이지만 DB와의 connection에서는 UTC로 session timezone이 세팅되는 것이다. 그래서 DB와 데이터를 주고받을 때, db connection session은 application과 db사이에 타임존 간극이 존재한다고 생각한다. 결과적으로, Application과 DB사이의 날짜 교환이 일어날 때, 날짜 관련 데이터들은 자동으로 변환 (time conversion)된다.

// 흐름
Application 재실행 (UTC)
-> Application Context 생성 중… DB 관련 bean 설정 도 이때 됨 (UTC)
-> @PostConstruct 메서드 실행 후 Application 띄움 (KST)
로그로 직접 확인한 결과는 다음과 같다

로그

로그에서 알 수 있듯이 HikariPool, Hibernate와 같은 DB 관련 설정이 모두 끝나고 난 후에야 @PostConstruct 메서드가 실행되는 것을 확인할 수 있다. 그러면 당연히 이 과정속에서 DB connection session 설정이 이루어졌을 테고 해당 session의 time zone은 그 당시 application의 timezone인 UTC로 설정 되었을 것이다. 빨간색 WARN 로그에서 확인할 수 있듯이 time zone setting이 가장 마지막에 이루어진다. 이 부분이 문제가 되는 것이다.

하지만 어느정도 시간이 지나면 (실험 결과 대략 40 ~ 50분, 아래 사진 참고) timezone sync가 또 자동으로 맞춰지긴 한다. 여태까지 계속 timezone 문제 해결 해오면서, 해결 됐다가 안됐다가 한다고 느꼈던게 이 부분 때문이라고 생각한다.

(뇌피셜 주의) 이는 Application과 DB 사이의 커넥션 refresh 주기(?)가 있는 것으로 추정된다. 이부분은 아직 확실하게 밝혀지지 않았다. 하지만 가능성 농후

(뇌피셜 → 오피셜)

HikariPool은 connection의 life cycle을 max-lifetime 옵션을 통해 관장한다. default는 30분이다. 즉, 30분동안 커넥션을 유지하고 이 시간이 지나면 커넥션 풀의 커넥션들을 새로운 커넥션으로 갈아끼운다. 위에 취소선 그어진 뇌피셜이 맞았던 것이다.

이를 실제로 확인해보기 위해서 HikariPool의 max-lifetime 설정을 1분으로 설정해주고 실제로 1분이 지나면 커넥션을 갈아끼우는지 실험을 해봤다.

그 결과 아래 사진과 같이 진짜 1분 후에 db와의 커넥션이 다시 맺어지면서 time zone 이슈가 해결되는 것을 확인할 수 있다!! 이제서야 모든 비밀이 풀렸다. 이 HikariPool의 max-liftime개념 때문에, 배포후 time zone이 정상화되는데 30분의 시간이 소요됐던 것이다. 그리고 이 텀 때문에, 타임존 문제가 자꾸 오락가락 하는 것 처럼 보였던 것이다.



어찌 됐건, 우리가 원하는 것은 Application과 DB의 connection시 timezone sync가 Application을 띄우는 순간 바로 맞아 떨어지는 것이다.

그래서 2번 설정이 필요하다!

2번 설정을 통해, Hibernate가 직접 명시적으로 time zone을 지정 해주도록 설정할 수 있다.

그렇다면 Application과 DB connection이 맺어질 때, Application (JVM) timezone을 사용하지 않고 Hibernate가 직접적으로 session timezone에 대한 컨트롤이 들어간다. 그러면 DB connection session이 만들어지는 순간 time zone을 KST로 명시해줄 수가 있고, Application이 올라가는 순간 DB와 sync가 즉시 맞아 떨어지게 되는 것이다.

추가로, Hibernate가 직접 session timezone을 컨트롤하기 때문에 DB 서버의 timezone 세팅이 어떻게 되어있는지는 상관이 없다. 이는 현재 우리가 DB에 날짜 시간 데이터를 DATETIME 데이터를 저장하고 있기 때문이다. 참고로 DATETIME은 timezone 정보가 없는 날짜 데이터 타입이고(그냥 VARCHAR라고 생각하면 됨) TIMESTAMP는 timezone 정보가 포함된 날짜 데이터 타입 (UTC 기준으로 변환하여 저장)이다. 그래서 DB의 timezone setting은 우리 서비스에서는 크게 의미가 없다고 보면 된다.

번외편
OS 설정 변경을 통한 Time Zone issue 해결법
요구사항

JVM의 time zone은 KST여야 한다 (LocalDateTime.now()의 필요성 때문)
모든 시간 데이터에 대해서 Time Conversion이 일어나지 않아야 한다
JVM 타임존 설정 OS (EC2 Ubuntu) 단에서 바꾸기
초기 timezone issue를 해결하기 위한 방법으로 OS의 timezone을 변경하려고 시도한 적 있었다.

정확히 아래와 같은 커맨드로 말이다.


sudo rm /etc/localtime

sudo ln -s /usr/share/zoneinfo/Asia/Seoul /etc/localtime

분명히 OS가 KST로 바뀌었음을 두 눈으로 확인도 했었다.

그런데, 이 방법으로 해결되지 않았다.

분명히 JVM은 OS의 timezone을 따라간다고 했었는데 왜그럴까? 이론대로라면 JVM도 이제 KST로 바뀌어야하는데, 실험 결과 JVM은 그대로 UTC로 timezone을 설정하고 있었다.

그 이유는 JVM은 OS (EC2 Ubuntu 기준)의 timezone을 읽어올 때, /etc/timezone이라는 파일을 읽어서 timezone을 세팅하기 때문이다. 그래서 심볼릭 링크를 통해 OS의 timezone을 변경해줘도 Application이 올라갈 때 JVM의 timezone에 아무런 영향을 주지 못한 것이다. 즉, OS 자체의 시간대를 바꾸는 것은 의미가 없다.

그래서 /etc/timezone이라는 파일을 다음과 같이 변경한 후 재실행 해보았다.

// 기존 /etc/timezone
Etc/UTC

// 변경 후 /etc/timezone
Asia/Seoul
이론대로라면, 이제 JVM은 KST로 세팅되어야하고 결과적으로 이 방법도 현재 우리의 요구사항을 모두 해결해 줄 수 있어야한다. 확인해보자.

1번 요구사항
예약 현재시간 이전 validation


예약을 생성했을 때, 제대로 검증함을 확인할 수 있다!

2번 요구사항
Application - DB에서 time conversion이 일어나는지 검사해봤다 이는 공간 생성을 통해서 실험 해보았다.

available start time: 07:00, available end time: 23:00

Application -> DB


DB -> Application


모두 sync가 맞아 떨어지는 것을 확인 할 수 있다!!

이 해결책은 scale-out에 불리하다. OS의 설정변경을 통해 JVM을 컨트롤하는 방법이 OS 마다 다를 수 있고, 매번 새 EC2로 확장할 때 마다 이 번거로운 작업을 해줘야하는 것은 구리다고 생각한다. 현재 우리의 해결책은 Application 상에서 단순한 설정으로 OS 환경에 걱정없이 타임존 이슈를 해결할 수 있기 때문에 더 좋은 해결책이라고 생각한다.

Reference
https://www.baeldung.com/mysql-jdbc-timezone-spring-boot
https://moelholm.com/blog/2016/11/09/spring-boot-controlling-timezones-with-hibernate
https://docs.jboss.org/hibernate/orm/5.4/userguide/html_single/Hibernate_User_Guide.html#basic-datetime-time-zone
https://coderanch.com/t/729990/frameworks/spring-jpa-properties-hibernate-jdbc
http://abh0518.net/tok/?p=652
https://hyunsoori.tistory.com/2",BE
22,"Kibana (With ElasticSearch)
목차
기본 사항
키바나 접속 링크
계정 정보
로그 조회
레벨 별로 로그 모아보기
스택 트레이스 내용 확인하기
통계
메소드 누적 호출 및 처리 시간 통계 확인하기
기본 사항
Prod 서버

바로가기
Dev 서버

바로가기
계정 정보

ID: elastic
PW: 항상 쓰는 그것
로그 조회
레벨 별로 로그 모아보기
좌측 메인 메뉴(작대기 3개) -> Analytics -> Discover 란을 클릭합니다.
우 상단에 Open을 클릭합니다.
미리 만들어논 필터링 프리셋 중에 원하는 것을 선택합니다.
스택 트레이스 내용 확인하기
스택 트레이스는 내용이 매우 길기 때문에 그냥은 보이지 않습니다. image

해당 로그 좌측의 토글 버튼을 눌러 열어줍니다. image

하단의 Table탭이 아닌 JSON 탭을 눌러줍니다. image

통계
메소드 누적 호출 및 처리 시간 통계 확인하기
좌측 메인 메뉴(작대기 3개) -> Analytics -> Dashboard를 클릭합니다.
미리 만들어논 프리셋을 확인합니다.",BE
23,"카프카 도입기
Kimun Kim edited this page on Oct 12, 2021 · 11 revisions
레퍼런스: AWS에 카프카 클러스터 설치하기(ec2, 3 brokers) by 데브원영

위 레퍼런스를 주로 참고하였으나, 구축 환경이 좀 다른지 권한 문제가 있어 찜꽁 환경에 맞게 풀어 쓴다.

들어가기
Spring Boot와 Kafka 연동까지의 과정을 크게 3가지로 이루어진다고 볼 수 있다.

주키퍼 설치
카프카 설치
Logging 프레임워크(e.g., Logback) <-> 카프카 연동
다음과 같은 환경 설정하에서 진행하였다.

서버 인스턴스: AWS EC2, Ubuntu 18.04
Java 11 (인스턴스에 미리 깔아 놓는다)
로깅 프레임워크: Logback
클러스터를 구성하는 서버 개수: 3
이벤트 브로커는 고가용성에 상당한 목적의식을 두고 있기 때문에, 3대의 서버를 이용해서 카프카 클러스터를 구축한다.

0. 구성의 편리를 위한 호스트 별칭 세팅
localhost -> 127.0.0.1 로 연결되는 것처럼, 아이피 번호를 직접 타이핑하지 않고 별칭을 지정할 수 있다면 환경 구성이 상당히 편리해진다.

i) 서버 3대의 IP를 미리 확보해놓는다.
ii) 각 서버에 접속해 /etc/hosts(확장자 없음)내에 별칭을 등록한다.
$ sudo vim /etc/hosts
iii) 자신의 서버를 지칭하는 별칭의 IP는 0.0.0.0로 등록한다.
e.g., 2번 카프카 서버를 작업중인 경우

${1번 서버의 IP} ${1번 서버 별칭}
0.0.0.0 ${2번 서버 별칭}
${3번 서버의 IP} ${3번 서버 별칭}
예시 (2번 서버를 작업중인 경우)

192.168.2.112 KAFKA-1
0.0.0.0 KAFKA-2
192.168.2.158 KAFKA-3
# 같은 네트워크에 속해있다면 위와 같이 사설 IP를 사용해도 상관 없다.
1. Zookeeper 설치
Zookeeper란?
카프카의 클러스터링을 관리해주는 역할
카프카는 이벤트를 주고 받는 기능에 치중되어 있다면, 주키퍼는 클러스터를 구성하는 일부 서버의 장애 상황에 대응하는 역할(e.g., 다른 카프카로 요청을 선회시킨다든가, 리더 노드를 다시 선출한다든가 등)을 한다.
설치 과정
설치 파일이 분산되는 apt-get보다 압축을 받아 풀어 사용하는 것이 편하므로, 압축 파일을 받아 사용하는 방법을 서술한다.

i) 압축 파일을 다운받고 풀어준다.
$ wget https://archive.apache.org/dist/zookeeper/zookeeper-x.x.x/zookeeper-x.x.x.tar.gz #버전은 x로 표시했다. 상황에 맞게 알맞는 버전을 다운받는다.
$ tar xvf zookeeper-x.x.x.tar.gz
ii) 압축을 푼 폴더 -> conf 폴더에 들어가 zoo.cfg 파일을 만들고 다음의 설정을 입력한다. *필요에 따라 적절히 커스터마이징한다. 예를 들어 찜콩은 보안 그룹 정책 때문에 clientPort를 8000으로 둔 상태
$ cd zookeeper-x.x.x/conf
$ vim zoo.cfg
# zoo.cfg
tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181 # 클라이언트 통신 포트
initLimit=20
syncLimit=5
server.1=KAFKA-1:2888:3888 # 각 서버들의 별칭 : 주키퍼 간 동기화 포트 : 주키퍼 간 마스터 선출 포트
server.2=KAFKA-2:2888:3888
server.3=KAFKA-3:2888:3888
iii) /var/lib/zookeeper/myid 파일(확장자 없음)을 만들고 자기 카프카 서버의 번호를 작성해야한다.
$ cd /var/lib
$ mkdir zookeeper
$ cd zookeeper
$ sudo vim myid
myid 파일에 서버 번호(1, 2, 3...)을 입력한다.

3번 서버에서 작업중이라면 다음의 cat 명령어의 출력값이 3이어야한다.

$ cat /var/lib/zookeeper/myid # 3
iv) 실행 및 확인
$ cd ~/zookeeper-x.x.x/bin # (주키퍼 설치 경로/bin 폴더)
$ sudo sh zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /home/ec2-user/zookeeper-x.x.x/bin/../conf/zoo.cfg
Strating zookeeper ... STARTED
2. Kafka 설치
주키퍼의 포트와 카프카의 포트를 헷갈리지 않는다.
i) 다운 받고 압축을 푼다. 마찬가지로 버전은 상황에 맞게 다운받는다.
$ wget https://archive.apache.org/dist/kafka/2.1.0/kafka_2.11-2.1.0.tgz
$ tar xvf kafka_2.11-2.1.0.tgz
ii) 설치폴더 -> config 폴더의 server.properties를 열어 다음의 값들을 수정한다. *모두 주석처리되어 있는 옵션 항목들이니 찾아서 주석을 해제하고 값을 세팅하거나, 그냥 마지막 줄에 추가한다.
# test-broker02인 경우

broker.id=2
listeners=PLAINTEXT://:9092 # 카프카 기본 외부 오픈 포트
advertised.listeners=PLAINTEXT://${현 서버의 공인 IP}:9092 # 이 옵션 값으로는 반드시 공인 IP + 카프카 포트가 들어가야 한다. 클라이언트에게 접속할 IP를 알려줄 때 사용되기 때문!
zookeeper.connect=KAFKA-DEV-1:2181,KAFKA-DEV-2:2181,KAFKA-DEV-3:2181 # 각 서버들의 별칭과 주키퍼의 포트를 쉼표로 구분지어 나열한다.
iii) 실행 및 확인
$ cd ~/kafka_x.x-x.x.x/bin # (kafka 설치 경로/bin)
$ sudo sh bin/kafka-server-start.sh config/server.properties",BE
60,"CloudWatch 로그, 지표 및 이벤트 형태로 모니터링 및 운영 데이터를 수집하여 AWS와 온프레미스 서버에서 실행되는 AWS 리소스, 애플리케이션 및 서비스에 대한 통합된 보기를 제공 사용한 이유 Web Server와 Was는 로그 파일이 생성됩니다. 이 로그 파일을 확인하기 위해서는 직접 EC2 인스턴스에 접속해서 로그 파일을 열어보는 수 밖에 없었습니다. 때문에 개발의 편의와 손쉬운 정보 수집, 빠른 문제 상황에 대응하기 위한 로그 시각화 수단이 필요했습니다. 일반적으로 로그 시각화 용도로 ELK(Elastic Logstash Kibana) 스택 중 하나인 Kibana, 매트릭스 지표 시각화에 특화된 Grafana를 많이 사용하지만, 저희 팀 프로젝트는 AWS를 사용하는 환경이고, CloudWatch에서도 필요한 기능이 모두 제공되었기 때문에 CloudWatch를 통해 로그 시각화를 진행했습니다. 적용하기 1. web-server 에서 cloud-watch 디렉토리를 생성합니다. $ mkdir cloud-watch 2. Python 이 없을 경우 설치를 합니다. $ sudo apt-get install python 3. cloud-watch 디렉토리로 이동 후 설치합니다. $ curl <https://s3.amazonaws.com/aws-cloudwatch/downloads/latest/awslogs-agent-setup.py> -O $ sudo python ./awslogs-agent-setup.py --region ap-northeast-2 4. 아래 명령어를 실행해서 설치해 줍니다. $ sudo python ./awslogs-agent-setup.py --region ap-northeast-2 기본 설정은 전부 Default 로 진행했습니다. More log files to configure? 는 N 하셔야 루프 탈출이 가능합니다. 연결할 EC2 인스턴스의 IAM 역할을 CloudWatch 로 설정합니다. IAM 역할이 지정되지 않으면 Default 가 아닌 추가 설정을 했어야 합니다. 설정중에 Choose Log Event timestamp format(3번), Choose initial position of upload(2번) 으로 테스트 서버는 설정함 5. 로그 그룹이름을 설정해 줍니다. (팀 이름으로 설정, babble- 뒤에 각 sys, access, error 로 지정) $ sudo vi /var/awslogs/etc/awslogs.conf

[/var/log/syslog]
datetime_format = %b %d %H:%M:%S
file = /var/log/syslog
buffer_duration = 5000
log_stream_name = {instance_id} # i-0af359bf101a6c5d4
initial_position = start_of_file
log_group_name = babble-sys # AWS 에서 표시될 그룹 이름

[/var/log/nginx/access.log]
datetime_format = %d/%b/%Y:%H:%M:%S %z
file = /var/log/nginx/access.log
buffer_duration = 5000
log_stream_name = access.log
initial_position = end_of_file
log_group_name = babble-access

[/var/log/nginx/error.log]
datetime_format = %Y/%m/%d %H:%M:%S
file = /var/log/nginx/error.log
buffer_duration = 5000
log_stream_name = error.log
initial_position = end_of_file
log_group_name = babble-error 6. 설정이 끝나면 재시작 합니다. $ sudo service awslogs restart 7. EC2-Matric 을 수집합니다. 다음 명령어를 순차적으로 입력해줍니다. $ wget <https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb>
$ sudo dpkg -i -E ./amazon-cloudwatch-agent.deb 8. 해당 경로의 파일을 수정합니다. $ sudo vi /opt/aws/amazon-cloudwatch-agent/bin/config.json 9. 입력 후 다음 단계를 진행합니다. {
 ""agent"": {
 ""metrics_collection_interval"": 60,
 ""run_as_user"": ""root""
 },
 ""metrics"": {
 ""metrics_collected"": {
 ""disk"": {
 ""measurement"": [
 ""used_percent"",
 ""used"",
 ""total""
 ],
 ""metrics_collection_interval"": 60,
 ""resources"": [
 ""*""
 ]
 },
 ""mem"": {
 ""measurement"": [
 ""mem_used_percent"",
 ""mem_total"",
 ""mem_used""
 ],
 ""metrics_collection_interval"": 60
 }
 }
 }
} 10. AWS 에서 Cloud Watch 의 대시보드로 이동합니다. 11. 대시보를 생성합니다. 12. 유형은 행을 선택합니다. 13. 원본 데이터로 지표를 선택합니다. 14. CPU Utilization, Network In / Out, mem_used_percent, disk_used_percent 등 원하는 지표를 선택합니다. 15. 더 추가할 내용을 추가합니다. 9번 작업 을 통해 추가해준 디스크, 메모리 사용량 관련 위젯은 시간이 어느정도(10분 정도) 지나야 노출되는 것으로 보입니다. 16. 추가가 완료된 모습을 볼 수 있습니다. 앞서 web-server 에서 CloudWatch 설정을 통해 메모리 사용률, 디스크 사용률(ext4)을 확인할 수 있습니다. 앞서 서버에서 새롭게 설치하고 설정해준 시점을 기점으로 기록이 되므로 일정 시간이 지나고 나야 쌓인 로그를 확인할 수 있습니다. 이름은 제목에 마우스를 올리면 편집 가능합니다. 항상 변경하면 중간중간 대시보드 저장을 잊지말고 눌러줍시다. 17. Nginx 에서 생성된 log 파일을 CloudWatch 에서 볼 수 있도록 설정을 추가합니다. 18. 로그 그룹을 선택합니다. 19. 쿼리 실행을 누르면 잠시 후 하단에 로그가 출력됩니다. 마지막으로 우측상단의 위젯 생성을 누르면 정상적으로 추가됩니다. 20. test 서버 혹은 was 의 spring log 파일도 등록해줍니다. $ sudo vi /var/awslogs/etc/awslogs.conf
// 테스트 서버에 적용한 파일 내용
// 테스트 서버 기준 5번에서 작성한 내용에 이어서 아래쪽에 추가해 줬습니다.
// 배포 서버는 was 서버를 따로 운영하기 때문에 따로 설치하고 아래 내용을 작성하는 작업을 해야합니다.

[/home/ubuntu/was/was-logs/error.log]
datetime_format = %d/%b/%Y:%H:%M:%S %z
file = /home/ubuntu/was/was-logs/error.log
buffer_duration = 5000
log_stream_name = babble/test/was/error.log
initial_position = end_of_file
log_group_name = babble-was-test-error

[/home/ubuntu/was/was-logs/warn.log]
datetime_format = %d/%b/%Y:%H:%M:%S %z
file = /home/ubuntu/was/was-logs/warn.log
buffer_duration = 5000
log_stream_name = babble/test/was/warn.log
initial_position = end_of_file
log_group_name = babble-was-test-warn

[/home/ubuntu/was/was-logs/info.log]
datetime_format = %d/%b/%Y:%H:%M:%S %z
file = /home/ubuntu/was/was-logs/info.log
buffer_duration = 5000
log_stream_name = babble/test/was/info.log
initial_position = end_of_file
log_group_name = babble-was-test-info

[/home/ubuntu/was/was-logs/debug.log]
datetime_format = %d/%b/%Y:%H:%M:%S %z
file = /home/ubuntu/was/was-logs/debug.log
buffer_duration = 5000
log_stream_name = babble/test/was/debug.log
initial_position = end_of_file
log_group_name = babble-was-test-debug

[/home/ubuntu/was/was-logs/trace.log]
datetime_format = %d/%b/%Y:%H:%M:%S %z
file = /home/ubuntu/was/was-logs/trace.log
buffer_duration = 5000
log_stream_name = babble/test/was/trace.log
initial_position = end_of_file
log_group_name = babble-was-test-trace 21. 20번 작업이 끝나면 잊지말고 새로고침을 해줍니다. $ sudo service awslogs restart CloudWatch 를 활용하여 시각화된 그래프로 표현이 가능합니다. 로그의 경우 실제 에러 메세지를 확인해야하는 상황이 많았기 때문에 Text 로 출력되도록 구성하였습니다. 트러블 슈팅 error.log 가 로그 그룹에 추가되지 않음 - 에러 레벨 설계를 잘못하여 error 레벨의 로그가 발생하지 않았습니다. - error, debug, info 레벨의 수준을 변경했습니다. - 로그의 경우 새로운 로그가 정상적으로 생성되면 로그 그룹에 추가됩니다. error 메세지의 모호함 - ""해당 방은 입장할 수 없습니다."" 이와 같은 메세지는 어떤 방에서 에러가 발생했는지 추측하기가 어려웠습니다. - ""[54]번방은 입장할 수 없습니다."" 이처럼 에러가 발생한 곳을 특정하여 신속히 에러가 발생한 부분을 파악할 수 있었습니다. Timestamp 이슈 - UTC+0 의 시간이 나왔기 때문에 9시간의 시간적 오차가 발생했습니다. - 로그의 시간을 UTC+9 로 설정해주고 CloudWatch 의 쿼리에 포함된 @Timestamp 옵션을 제거했습니다.",BE
61,"NFS Network File System NFS란? 한줄로 요약하면, 유닉스 시스템에서의 저장공간 공유를 위한 프로토콜입니다. A라는 인스턴스에 unluckyJung이라는 디렉토리를 만들어두면
B, C라는 인스턴스에서도 A의 unluckyJung 디렉토리에 접근할수 있게 해주는 분산 파일시스템 프로토콜을 이야기합니다. 사용한 이유 Scale Out 작업을 수행하면서 Babble 인프라내 WAS가 N개로 늘어나게 되었습니다. 각 WAS별로 찍히는 로그를 CloudWatch를 통해서 확인하려면 WAS의 개수에 맞춰 N개의 추가 설정이 필요했습니다. 즉 WAS가 100개가 된다면, CloudWatch로 추적되는 화면이 100개가 되겠죠. 이것은 로그 추적에 있어서도 불편함을 야기하게 되었고, NFS를 이용해 한개의 서버(A서버)에서 LOG 파일을 관리하고 다른 WAS들은, A서버에 접근해서 LOG 파일을 수정 하도록 하게 구성하였습니다. 어떤 WAS에서 발생하는 에러인지는 Docker의 Container ID로 구분이 가능했습니다. docker service logs babble-was 설치 서버(매니저 측) sudo apt-get update
sudo apt install nfs-kernel-server nfs server를 설치합니다. sudo mkdir -p {공유할 폴더 경로}
sudo mkdir -p /mnt/sharedfolder 공유할 폴더를 만듭니다. sudo chown nobody:nogroup /mnt/sharedfolder
sudo chmod 777 /mnt/sharedfolder 폴더 접근, 수정권한을 열어줍니다. sudo vi /etc/exports

// vi 창

/mnt/공유할폴더명 클라이언트IP(rw,sync,no_subtree_check) 실제로 보이는 vi 창 예시 # /etc/exports: the access control list for filesystems which may be exported
# to NFS clients. See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4 gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes gss/krb5i(rw,sync,no_subtree_check)
#

/mnt/sharedfolder 192.168.0.0/16(rw,sync,no_subtree_check) 클라이언트에 호스트에 접근 할 수 있는 권한을 열어줍니다. 중간에 띄어쓰기가 들어가지 않게 조심해야 합니다. sudo exportfs -a
sudo systemctl restart nfs-kernel-server 설정을 마무리합니다. 설치 클라이언트쪽 sudo apt-get update
sudo apt-get install nfs-common nfs 접속 클라이언트를 설치합니다. sudo mkdir -p /mnt/sharedfolder_client

sudo mount 호스트서버IP:호스트서버공유폴더 클라이언트공유폴더
sudo mount 192.168.100.5:/mnt/sharedfolder /mnt/sharedfolder_client 클라이언트쪽에서 호스트의 공유폴더에 접근할 폴더를 만듭니다. 공유폴더를 마운트합니다. 이후 클라이언트에서 호스트서버의 폴더에 접근할 수 있게됩니다. client : /mnt/sharedfolder_client host server : /mnt/sharedfolder Reference https://vitux.com/install-nfs-server-and-client-on-ubuntu/ 구동 docker service create 
--name babble-was --replicas=3 
--mount type=bind,source=/home/ubuntu/was-logs,target=/was-logs 
--publish mode=host,target=8080,published=8080 
-e PROFILES_ACTIVE=prod -e DATASOURCE_PASSWORD=password -e -e REDIS_PASSWORD=password 
2021babble/babble-was:latest 상태체크 nfsstat -s // 서버 상태 확인
nfsstat --version // nfs 버전확인 https://linoxide.com/example-linux-command-to-find-nfs-version/ 삭제 sudo apt-get remove nfs-kernel-server
sudo apt-get remove --auto-remove nfs-kernel-server
sudo apt-get purge nfs-kernel-server
sudo apt-get purge --auto-remove nfs-kernel-server 이부분 까지만 진행하면 nfsstat -s 를 할경우에도 계속 nfs 서버가 살아있는것이 확인되었습니다. sudo apt-get --purge remove nfs-kernel-server nfs-common portmap 위의 명령어를 추가적으로 더 입력해줍니다. Reference https://sxi.io/debian-ubuntu-linux-disable-remove-all-nfs-services/ https://installlion.com/ubuntu/vivid/main/n/nfs-kernel-server/uninstall/index.html Conclustion 각각의 WAS에서 공유 디렉토리를 이용해 같은 파일에 읽고, 쓰기가 가능해졌습니다. 트러블 슈팅 NFS 가 V3 - V4를 왔다갔다 하는 현상 발생. 2021-10-26 한개의 인스턴스에서 Host와 Client를 동시에 두었을때 Docker Run 과정에서 문제가 발생했었다. Host용 인스턴스를 분리해서 해결했다. (WAS-MANAGER를 완전히 분리) https://unluckyjung.github.io/infra/2021/10/27/NFS/#",BE
62,"기존에 내가 진행하던 프로젝트는 WAS가 한 대 뿐이었다. 이번에 성능 향상을 목적으로 스케일 아웃을 진행하기로 결정했다. 스케일 아웃을 진행하면서 처리해야될 작업들에는 다음이 있었다. 서버가 여러 대인 점을 이용한 무중단 배포 WAS가 늘어나면서 NGINX에서 요청을 전달한 WAS 선택 방법 (로드 밸런싱) 이를 해결하기 위해서 나는 도커 스웜을 사용해보기로 결정했다. 그 이유는 다음과 같다. 도커 스웜은 자체적으로 롤링 업데이트를 지원하며, 매우 간단하게 사용할 수 있다. ingress 네트워크를 통해 NGINX에 추가적인 설정을 하지않아도, ingress의 로드 밸런서가 라운드 로빈 방식을 사용해서 WAS의 트래픽을 분산시킬 수 있다. 간단해서 학습하는데 비용이 적게든다. 도커 스웜 모드 클러스터 구축 스웜 모드 클러스터 구축 자체는 간단하다. 매니저 노드에서 위와 같은 명령어를 입력하면 스웜 클러스터가 시작된다. --advertise-addr는 외부 노드가 매니저 노드에 접근할 때 사용할 수 있는 IP를 지정하는 옵션이다. 실행하면 다음과 같이 토큰 값을 받을 수 있다. 워커 노드로 사용할 서버에 접속하여 다음의 명령어를 입력한다. 다시 매니저로 돌아가서 클러스터의 상태를 보면 워커 노드가 2개 추가된 것을 확인할 수 있다. 배포용 이미지 만들기 Dockerfile을 이용하여 이미지를 만들도록 구성할 것이다. FROM openjdk:8
COPY babble-0.0.1-SNAPSHOT.jar app.jar
ENTRYPOINT [""java"", ""-jar"", ""-Dspring.profiles.active=${PROFILES_ACTIVE}"", ""-Dspring.datasource.password=${DATASOURCE_PASSWORD}"", ""-Dspring.redis.password=${REDIS_PASSWORD}"", ""-Duser.timezone=Asia/Seoul"", ""app.jar""] 환경 변수를 통해 컨테이너를 실행할 때 비밀 번호를 입력받도록 구성할 것이다. 비밀번호는 github secrets를 이용하여 관리한다. 다음과 같은 방법으로 환경 변수를 지정하여 실행할 수 있다. $ sudo docker run -it -v /home/ubuntu/deploy/was-logs:/was-logs -p 8080:8080 --name junroot-was -e DATASOURCE_PASSWORD=password -e REDIS_PASSWORD=password test-was/app 이미지 배포하기 이미지는 Docker hub에 push되어 배포가 진행된다. docker push 2021babble/babble-was:latest WAS에서 매니저 노드가 이미지를 pull한 뒤 서비스를 생성한다. docker service create 
--name babble-was --replicas=3 
--mount type=bind,source=/home/ubuntu/was-logs,target=/was-logs 
--publish mode=host,target=8080,published=8080 
-e PROFILES_ACTIVE=prod -e DATASOURCE_PASSWORD=password -e -e REDIS_PASSWORD=password 
2021babble/babble-was:latest 이후에는 서비스를 새로만들지 않고 pull 한뒤에 서비스 롤링 업데이트를 진행하면된다. docker service update --image 2021babble/babble-was:latest babble-was NGINX 로드 밸런싱 이제 NGINX에서 로드밸런싱이 가능하도록 구성한다. NGINX의 nginx.conf 파일에 upstream을 다음과 같이 명시하면된다. upstream app {
least_conn;
server 192.168.2.137:8080;
server 192.168.2.142:8080;
server 192.168.3.5:8080;
} least_conn 은 로드밸린싱 방식을 지정한 것이다. 가장 커넥션 수가 적었던 서버로 요청을 보낸다. round-robin(디폴트) - 그냥 돌아가면서 분배한다.
hash - 해시한 값으로 분배한다 쓰려면 hash <키> 형태로 쓴다. ex)hash $remote_addr <- 이는 ip_hash와 같다.
ip_hash - 아이피로 해싱해서 분배한다.
random - 그냥 랜덤으로 분배한다.
least_conn - 연결수가 가장 적은 서버를 선택해서 분배, 근데 가중치를 고려함
least_time - 연결수가 가자 적으면서 평균 응답시간이 가장 적은 쪽을 선택해서분배 최종적으로 다음 그림과 같은 인프라를 구성할 수 있었다. NFS을 이용한 볼륨 매핑 이렇게 끝내고 싶었지만, 새로운 문제점이 발견되었다. --mount type=bind,source=/home/ubuntu/was-logs,target=/was-logs 옵션으로 컨테이너를 실행한다면 각 인스턴스마다 따로 로그 파일이 남게되어서, 로그 파일 집계가 힘들다는 단점이 있다. 로그 파일을 수집하여 Cloudwatch에 뿌려주는 상황이라면 더 골치아프다. 아쉽게도 도커 스웜은 여러 노드에 대한 볼륨 클러스터링은 지원하지 않는다. 따라서, NFS를 이용하여 여러 노드가 파일을 공유하도록 구성할 필요가 있다. NFS 서버 실행 먼저 로그 수집을 위한 별도의 노드를 만들고 거기에 NFS 서버를 실행한다. 아래와 같이 NFS 커널 서버 패키지를 설치한다. $ sudo apt update
$ sudo apt install nfs-kernel-server 그 다음으로는 서버가 외부에 공개할 디렉토리를 생성한 후, 모든 서버가 이 디렉토리에 접근이 가능하도록 모든 권한을 부여한다. $ sudo chown -R nobody:nogroup /mnt/main-was-logs/
$ sudo chmod 777 /mnt/main-was-logs/ NFS 서버의 엑세스 권한은 /etc/exports 파일에서 설정이 가능하다. 아래와 같이 파일을 연다. $ sudo vim /etc/exports 위에서 생성한 디렉토리에 192.168.xxx.xxx IP가 접근할 수 있도록 서브넷을 명시한다. /mnt/main-was-logs 192.168.0.0/16(rw,sync,no_subtree_check) 변경 사항을 적용하고 NFS 서버를 재시작하면 성공적으로 NFS 서버가 실행된다. $ sudo exportfs -a
$ sudo systemctl restart nfs-kernel-server NFS 클라이언트 실행 NFS 클라이언트에는 nfs-common 패키지만 설치하면된다. $ sudo apt install nfs-common 서비스 실행 docker service create 
--name babble-was --replicas=3 
--mount type=volume,source=nfsvolume,target=/was-logs,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/mnt/main-was-logs,volume-opt=o=addr=192.168.1.20 
--publish mode=host,target=8080,published=8080 
-e PROFILES_ACTIVE=prod -e DATASOURCE_PASSWORD=password -e REDIS_PASSWORD=password 
2021babble/babble-was:latest 위 명령어와 같이 마운트 옵션을 주면 서비스를 실행할 때 지정된 NFS에 볼륨이 매핑된다. 이 때 주의할 점이 있다. 위 명령어로 생성된 도커 볼륨은 서비스를 종료시켜도 남아 있으니 docker volume rm <volume id> 로 볼륨을 지우고 진행해야된다. 마스터 노드 뿐만 아니라 워커 노드에도 볼륨을 지우는 작업이 필요하다. 참고 자료 시작하세요! 도커/쿠버네티스(용찬호) https://kamang-it.tistory.com/entry/WebServernginxnginx로-로드밸런싱-하기 https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/ https://www.tecmint.com/install-nfs-server-on-ubuntu/",BE
72,"이미지 처리 방법 Java에서 이미지 처리하는 방법에는 여러 가지가 있다. AWT: Java 기본 라이브러리로 창 생성, 버튼 등의 GUI와 관련된 처리를 할 수 있도록 도와주는 라이브러리다. ImageJ: 이미지 작업을 위해 만들어진 라이브러리다. 만들어진 목적이 GUI가 아니라 처음부터 이미지 처리였기때문에 이미지 처리에대해 좋은 기능이 많이 있다. OpenIMAJ: 비디오, 오디오 처리나 머신 러닝 등의 작업을 할 수 있다. 이 라이브러리를 통해서도 이미지 처리를 할 수 있다. TwelveMonkeys ImageIO: Java 기본 라이브러리인 ImageIO를 확장한 형태다. 기존 ImageIO에서 다른 이미지 포맷도 지원해준다. AWT를 선택한 이유 복잡한 이미지 처리는 발생하지 않을 것으로 추정해서, 기본 라이브러리를 사용하기로 결정했다. 또한, 이미지 리사이징에대한 처리 성능을 측정한 결과를 찾아 봤는데 awt가 가장 빨랐다. 또한 Java의 이미지 프로세싱에 대한 자료가 awt를 사용한 경우가 제일 많이 발견되어서 선택했다. java.awt.Graphics2D – 34ms Image.getScaledInstance() – 235ms Imgscalr – 143ms Thumbnailator – 547ms Marvin – 361ms https://www.baeldung.com/java-resize-image#best-practice 참고 AWT 시작하기 이미지 정보를 메모리로 가져오기 BufferedImage bufferedImage = ImageIO.read(image.toFile()); ImageIO.read() 메소드를 사용하면된다. 파라미터로 파일뿐만 아니라 URL이나 InputStream을 넘겨줄 수도 있다. 이미지 리사이징 Graphics2D 를 이용한 방법을 사용할 것이다. BufferedImage resizedImage = new BufferedImage(targetWidth, targetHeight, BufferedImage.TYPE_INT_RGB);
Graphics2D graphics2D = resizedImage.createGraphics();
graphics2D.drawImage(originalImage, x, y, targetWidth, targetHeight, null);
graphics2D.dispose();
return resizedImage; BufferedImage 생성자로 넘기는 타입의 경우는 아래의 링크에서 설명을 볼 수 있다. ()하지만, 일반적으로 투명 채널이 필요한경우는 TYPE_INT_ARGB 를 쓰고, 필요없는 경우는 TYPE_INT_RGB 를 사용하면 되는 것으로 보인다. Graphics2D 클래스는 Java에서 2차원 도형, 텍스트, 이미지를 렌더링하기 위한 클래스다. BufferedImage 에 있는 데이터를 2D 좌표에서 연산을 하기위해 사용했다. drawImage() 메소드에 마지막으로 들어가는 파라미터는 이미지에 수정이 발생했을 때 알려주는 용도의 옵저버 객체인 ImageObserver를 추가한다. 사용하지 않을 경우 null을 넘기면된다. Graphics2D 는 다 사용했다면 리소스 해제를 시켜주기 위해서 dispose()를 호출해야된다. 이미지 리사이징 하면서 발생하는 문제 이미지를 아무 알고리즘 없이 리사이징 하게되면, 앨리어싱(계단현상)이나 어색한 리샘플링 기술로인해 이미지에 의도하지 않은 패턴이 생길 수 있다. 이를 해결하기 위해 interpolation이 필요하다. VALUE_INTERPOLATION_BICUBIC VALUE_INTERPOLATION_BILINEAR VALUE_INTERPOLATION_NEAREST_NEIGHBOR 위 같이 3개의 interpoliation이 있고 위로 올라갈수록 처리속도는 느려지지만 퀄리티는 좋아진다. cubic까지는 많이 사용하는 것으로 알고 있으므로 BICUBIC을 사용했다. Graphics2D graphics2D = resizedPicture.createGraphics();
graphics2D.setRenderingHint(RenderingHints.KEY_INTERPOLATION, RenderingHints.VALUE_INTERPOLATION_BICUBIC);
graphics2D.drawImage(myPicture, 0, 0, newWidth, newHeight, null);
graphics2D.dispose(); 이미지 압축 이미지 파일의 크기를 최소화하기 위해서 압축을 했다. ImageWriter 를 사용하면 압축이 가능하다. File input = new File(""digital_image_processing.jpg"");
BufferedImage image = ImageIO.read(input);

File compressedImageFile = new File(""compressed_image.jpg"");
OutputStream os = new FileOutputStream(compressedImageFile);
ImageOutputStream ios = ImageIO.createImageOutputStream(os);

Iterator<ImageWriter> writers = ImageIO.getImageWritersByFormatName(""jpg""); // 1
if (!writers.hasNext())
{
throw new IllegalStateException(""No writers found"");
}
ImageWriter writer = writers.next();
writer.setOutput(ios);

ImageWriteParam param = writer.getDefaultWriteParam(); // 2

param.setCompressionMode(ImageWriteParam.MODE_EXPLICIT); // 3
param.setCompressionQuality(0.05f); // 4
writer.write(null, new IIOImage(image, null, null), param); // 5

os.close();
ios.close();
writer.dispose(); 1: jpg 형식을 처리할 수 있는 ImageWriter를 모두 가져온다. 2: ImageWriteParam을 통해서 ImageWriteStream이 인코딩하는 방법을 설정할 수 있다. 3: 사용자가 지정한 방법으로 압축을 하도록 설정한다. 기본값은 MODE_COPY_FROM_METADATA로 이미지 메타데이터에 저장된 방식으로 압축을 하는 것이다. 4: 압축률을 지정한다. 숫자가 높을수록 화질이 좋아지고 파일 크기가 커진다. 5: BufferedImage의 이미지 데이터를 ImageWriter에 입력한다. 결과 가로 길이 640px로 리사이징한 결과인데 생각보다 화질이 별로였다. 인터포레이션도 설정도 제일 좋은 것으로 설정했는데 결과가 이런 걸보면 한계가 있어보였다. Image.getScaledInstance() 그래서 아까 언급했던 방법인 Java에서 기본으로 제공하는 라이브러리 중 또 다른 방법을 선택해서 시도해봤다. Image resultingImage = original.getScaledInstance(imageSize.getWidth(), imageSize.getHeight(), Image.SCALE_SMOOTH);
BufferedImage outputImage = new BufferedImage(imageSize.getWidth(), imageSize.getHeight(), BufferedImage.TYPE_INT_RGB);
outputImage.getGraphics().drawImage(resultingImage, 0, 0, null);
return outputImage; 스케일링 할 때 파라미터로 Image.SCALE_SMOOTH를 사용하게 되니 아래와 같이 깔끔한 이미지가 나왔다. 왜 이런 차이가 발생하는지 잘 모르겠다. getScaledInstance() 메소드 내부를 보니 SCALE_SMOOTH일 경우 AreaAveragingScaleFilter 즉 box filter를 사용하는 건데 그럼 앞에서 사용한 방법인 VALUE_INTERPOLATION_BICUBIC 이 이미지 퀄리티가 더 좋아야 정상이다. 좀 찾아보니 Graphics2D 의 interpolation 기술에 버그 리포트 된 것이 많이 보인다. 특히 맥북에서 사용하면 렌더링 힌트가 적용이 안된다는 글이 많다. https://bugs.java.com/bugdatabase/view_bug.do?bug_id=4950176 https://stackoverflow.com/questions/23552126/graphics2d-interpolation-not-playing-well-with-very-small-bufferedimages https://stackoverflow.com/questions/56804144/java-graphics2d-interpolation-doesnt-seem-to-work-on-resized-images https://github.com/qzind/tray/issues/38 참고자료 https://www.baeldung.com/java-images https://www.baeldung.com/java-resize-image https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/java/awt/image/BufferedImage.html https://docs.oracle.com/javase/tutorial/2d/advanced/quality.html",BE
73,"summary 개요 복제(Replication)는 한 서버에서 다른 서버로 데이터가 동기화 되는 것을 말하며, 원본 데이터를 가진 서버를 소스(Source/Master) 서버, 복제된 데이터를 가지는 서버를 레플리카(Replica/Slave) 서버라고 부른다. 소스 서버에서 데이터 및 스키마에 대한 변경이 최초로 발생하며, 레플리카 서버에서는 이러한 변경 내역을 소스 서버로부터 전달 받아 자기 데이터에 반영한다. 복제에는 여러가지 목적이 존재한다. 스케일 아웃: 서버 분리로 트래픽도 분산 시킨다. 데이터 백업: 레플리카 서버를 이용해 백업한다. 데이터 분석: 분석용 쿼리만 별도로 실행되는 서버(DB) 구축이 가능하다. 데이터의 지리적(물리적) 분산: 물리적 거리만큼의 통신속도를 개선할 수 있다. 복제 아키텍쳐 MySQL 서버에서 발생하는 모든 변경 사항은 별도의 로그 파일(Binary Log)에 순서대로 기록된다. 바이너리 로그에는 데이터의 변경 내역뿐만 아니라 데이터베이스나 테이블의 구조 변경과 계정, 권한 변경 정보까지 모두 저장된다. 바이너리 로그에 기록된 각 변경 정보를 이벤트(Event)라고 부른다. MySQL 서버의 복제는 이 바이너리 로그를 기반으로 구현된다. 소스 서버에서 생성된 바이너리 로그가 레플리카 서버로 전송되고, 레플리카 서버에서는 해당 내용을 저장(이렇게 저장된 파일을 Relay Log라 부름)한 뒤 자신이 가진 데이터에 반영한다. 소스서버의 바이너리 로그에 기록된 변경 내역들을 식별하는 방식에 따라 로그 파일 위치 기반 복제(Binary Log File Position Based Replication) 과 글로벌 트랜잭션 ID 기반 복제(Global Transaction Identifiers Based Replication) 으로 나뉜다. 이 글에서는 로그 파일 위치 기반 복제를 적용해본다. Master 서버 생성 및 설정 우선 Master 서버에서 CUD 작업 수행시 Slave 서버에 반영되도록 하는 연결 설정을 진행한다. 가장 먼저 Master 서버 터미널에서 vim을 통해 master db 서버의 설정 파일을 열어 주석처리된 설정을 해제한다. $ vim /etc/mysql/mariadb.conf.d/50-server.cnf

server-id = {id로 사용하고 싶은 정수 값}
log_bin = /var/log/mysql/mysql-bin.log
expire_logs_days = 10
max_binlog_size = 100M

bind-address = 0.0.0.0 server_id는 복제에 참여한 MySQL 서버들이 고유하게 가지고 있는 식별 값이다. 이 값은 바이너리 로그에 이벤트별로 이벤트가 최초로 발생한 MySQL을 식별하기 위해 사용되며, 기본값이 모두 1이다. 이 때문에 Master 서버와 Slave 서버를 구별할 수 있도록 Slave 서버의 server_id를 바꾸어 주는 것이다. 만일 Master 서버와 Slave 서버의 server_id값이 동일할 경우 Slave서버는 바이너리 로그 파일에 기록된 이벤트를 동기화 할 수 없다. Slave 서버는 자신이 발생시킨 이벤트로 간주해서 동기화를 시도하지 않기 때문이다. 이 때문에 복제에 참여한 모든 MySQL 서버가 고유한 server_id를 갖도록 설정해야 한다. log_bin은 앞서 복지 아키텍쳐에서 이야기한 바이너리 로그 파일의 형식을 뜻한다. bind-address를 전체 개방해서, Slave 서버의 접근을 허용하도록 한다. (동시에 WAS 서버의 접근도 허용됨) 설정이 끝났다면 저장 후 $ service mysqld restart를 통해 설정을 적용해준다. 이후 Master 서버의 관리자 계정으로 데이터베이스 서버에 접속해서 Slave 서버가 사용할 계정을 생성한다. $ mysql -u root -p
mysql> USE mysql;

mysql> CREATE USER '{유저 이름}'@'{% 또는 Slave IP}' IDENTIFIED BY '{유저 비밀번호}';
mysql> GRANT REPLICATION SLAVE ON {* 또는 스키마 이름}.{* 또는 테이블 이름} TO '{유저 이름}'@'{% 또는 Slave IP}';
mysql> FLUSH PRIVILEGES; GRANT REPLICATION SLAVE 명령에서 SLAVE ON *.*는 SLAVE 디비에게 모든 스키마에 속한 모든 테이블의 권한을 부여하겠다는 것이며, 'replication_user'@'%'는 replication_user 계정으로 접근한다면 모든 IP에 대해 요청을 허락하겠다는 뜻이 된다. 강의자료에서는 유저 생성 과정에서 비밀번호 등록을 IDENTIFIED WITH mysql_native_password BY '{유저 비밀번호}'를 사용하도록 되어 있으나, mariadb 10.6 버전 기준으로 mysql_native_password 명령어를 해석하지 못하는 에러가 발생했다. ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your > MariaDB server version for the right syntax to use near 'BY '{유저 비밀번호}'' at line 1 MariaDB 공식문서를 살펴보니 별도로 mysql_native_password 플러그인을 명시하지 않아도 자동으로 사용되기 때문인 것으로 보인다. mysql> SHOW master STATUSG;
*************************** 1. row ***************************
File: {바이너리 로그 파일명}
Position: {POS 값}
Binlog_Do_DB:
Binlog_Ignore_DB: 최종적으로 SHOW master STATUS 명령을 통해 바이너리 로그 파일을 제대로 관리 중인지 확인한다. File은 바이너리 로그파일, Position은 바이너리 로그 파일 내부의 위치를 의미한다. 곧이어 Slave 서버에서 사용할 것이므로 기억해두자. Slave 서버 생성 및 설정 Slave 서버 터미널에서 역시 vim을 통해 slave db 서버의 설정 파일을 열어 주석처리된 설정을 해제한다. 이 때 server-id 값은 앞서 이야기했던 대로 Master, Slave들과 겹치지 않는 고유한 값으로 지정해준다. $ vim /etc/mysql/mariadb.conf.d/50-server.cnf

server-id = {id로 사용하고 싶은 정수 값}
log_bin = /var/log/mysql/mysql-bin.log
expire_logs_days = 10
max_binlog_size = 100M 역시나 설정이 끝났다면 저장 후 $ service mysqld restart를 통해 설정을 적용해준다. 곧이어 관리자 계정으로 데이터베이스 서버에 접속 후 Master와의 연결을 위한 설정을 진행한다. % mysql -u root -p
mysql> USE mysql;

mysql> CHANGE MASTER TO MASTER_HOST='{MasterDB IP}', MASTER_PORT={MasterDB 포트번호}, MASTER_USER='{만들어둔 유저 이름}', MASTER_PASSWORD='{만들어둔 유저 비밀번호}', MASTER_LOG_FILE='{바이너리 로그 파일명}', MASTER_LOG_POS={POS 값}; MASTER_USER와 MASTER_PASSWORD는 앞서 Master 서버에서 생성해둔 Replication 전용 계정, 비밀번호를 입력한다. MASTER_LOG_FILE는 앞서 SHOW master STATUS에서 확인한 바이너리 로그 파일의 이름을 명시한다. MASTER_LOG_POS는 앞서 SHOW master STATUS에서 확인한 바이너리 로그 파일 내부 위치를 명시한다. 설정이 완료되었다면 START slave 이후 SHOW slave STATUSG 명령을 통해 상태를 확인해본다. mysql> START slave;
mysql> SHOW slave STATUSG;
*************************** 1. row ***************************
Slave_IO_State: Waiting for master to send event
Master_Host: {MasterDB IP}
Master_User: replication_user
Master_Port: {MasterDB 포트번호}
Connect_Retry: 60
Master_Log_File: {바이너리 로그 파일명}
Read_Master_Log_Pos: {POS 값}
Relay_Log_File: mysqld-relay-bin.000001
Relay_Log_Pos: 4
Relay_Master_Log_File: {바이너리 로그 파일명}
Slave_IO_Running: Yes
Slave_SQL_Running: Yes
...(생략)... Master 서버 쪽에 데이터를 추가하면 Slave 서버 쪽에도 곧장 반영되는 것을 확인할 수 있다. 위와 같이 출력될 경우 연결에 성공한 것이다. 곧바로 Springboot Datasource 설정을 진행하자. Master-Slave 연결 실패 만일 Slave_IO_State, Slave_IO_Running, Slave_SQL_Running 등이 다르게 출력될 경우 연결에 실패한 것이다. 가장 큰 경우의 수는 Master 서버의 IP, 포트번호를 잘못 입력했거나 replication용 계정 비밀번호를 잘못 입력했을 가능성이 있다. 쉽고 빠르게 확인해볼 수 있는 방법으로는 Slave 서버 터미널에서 아래 명령을 통해 replication용 계정으로 직접 Master 서버 DB에 접속해보는 것이다. $ mysql -h {Master 서버 IP} -u {replication 계정} -p
Enter password: {replication 비밀번호} 위 명령어로 접근이 불가능하다면 Master 서버의 bind-address 설정이나 방화벽, replication 계정 권한 등을 다시 확인해보아야 한다. 원인이 파악되었다면 아래 명령을 통해 Slave 서버에서 Master 서버로 연결 설정을 다시 진행할 수 있다. mysql> STOP slave;

mysql> CHANGE MASTER TO MASTER_HOST='{MasterDB IP}', MASTER_PORT={MasterDB 포트번호}, MASTER_USER='{만들어둔 유저 이름}', MASTER_PASSWORD='{만들어둔 유저 비밀번호}', MASTER_LOG_FILE='{바이너리 로그 파일명}', MASTER_LOG_POS={POS 값};

mysql> START slave; 또한 연결에 성공했음에도 연결 성공시점의 Master 서버와 Slave 서버의 스키마, 테이블 구성이 다를 경우 Slave 서버는 Master 서버가 실행한 쿼리를 읽었다가 에러를 일으키고 replication 수행을 중지하게 된다. (Master 서버의 상태를 복제하는것이 아닌, Master 서버가 수행한 쿼리를 따라서 수행하기 때문) 이럴 땐 mysqldump를 이용해서 Master 서버의 스키마를 덤핑한 다음, Slave 서버 쪽에 적용해준 후 Slave-Master 서버 연결 설정을 다시 진행해보자. # Master 서버 터미널
$ mysqldump -u {DB 계정 이름} -p {덤프할 스키마 이름} > {덤프 파일명}.sql

# scp 명령을 통해 Slave 서버로 전송 후
# Slave 서버 터미널
$ sudo mysql -u root -p {스키마 이름} < {덤프 파일명}.sql Springboot Datasource 설정 # 기존
spring:
datasource:
driver-class-name: org.mariadb.jdbc.Driver
url: jdbc:mariadb://{DB 서버 IP}:{DB 서버 포트번호}/{DB 스키마}
username: {계정명}
password: {비밀번호} 기존 스프링 프로필 파일에서 spring.datasource 관련 여러가지 설정을 통해 자동으로 datasource 가 등록 되었다. 하지만 replication을 이용하려면 여러 개의 datasource를 동시에 등록해야하므로, 자동 등록 기능을 이용할 수 없다. 직접 각각의 datasource들을 적용해주어야만 한다. spring:
# 새로운 커스텀
# 어떤 양식을 이용하던지 본인 자유
datasource:
driver-class-name: org.mariadb.jdbc.Driver
url: jdbc:mariadb://{Master DB 서버 IP}:{Master DB 서버 포트번호}/{DB 스키마}
username: {Master DB 계정 이름}
password: {Master DB 계정 비밀번호}

slaves:
slave1:
name: slave1
driver-class-name: org.mariadb.jdbc.Driver
url: jdbc:mariadb://{Slave DB 서버 IP}:{Slave DB 서버 포트번호}/{DB 스키마}
username: {Slave DB 계정 이름}
password: {Slave DB 계정 비밀번호} 또한 자동 등록 기능을 이용하지 않기 때문에 hiernate 설정도 수동 등록해주어야 한다. spring:
# 새로운 커스텀
# 어떤 양식을 이용하던지 본인 자유
datasource:
driver-class-name: org.mariadb.jdbc.Driver
url: jdbc:mariadb://{Master DB 서버 IP}:{Master DB 서버 포트번호}/{DB 스키마}
username: {Master DB 계정 이름}
password: {Master DB 계정 비밀번호}

slaves:
slave1:
name: slave1
driver-class-name: org.mariadb.jdbc.Driver
url: jdbc:mariadb://{Slave DB 서버 IP}:{Slave DB 서버 포트번호}/{DB 스키마}
username: {Slave DB 계정 이름}
password: {Slave DB 계정 비밀번호}

# 여기부턴 JPA가 읽고 해석하므로 자유 아님
jpa:
properties:
hibernate:
show_sql: false
generate-ddl: false
format_sql: true
physical_naming_strategy: org.springframework.boot.orm.jpa.hibernate.SpringPhysicalNamingStrategy
jdbc:
lob:
non_contextual_creation: true physical_naming_strategy: org.springframework.boot.orm.jpa.hibernate.SpringPhysicalNamingStrategy는 네이밍전략 설정이다. 설정을 생략할 경우 테이블/칼럼 명이 자바에서 사용하는 camel case 그대로 사용된다. (datasource 자동 등록시 얼마나 많은 기본설정들이 포함되어 있는지 알 수 있는 대목) non_contextual_creation 설정은 createClob() 메서드를 구현하지 않았다는 Hibernate의 에러로그를 보여주지 않는 설정이다. 스프링 프로필 작성이 완료되었다면, 임의로 작성한 DataSource를 @ConfigurationProperties 어노테이션을 통해 객체로 매핑해주자. Properties 클래스 @Setter
@Getter
@ConfigurationProperties(prefix = ""spring.datasource"")
public class ReplicationDataSourceProperties {

private String driverClassName;
private String url;
private String username;
private String password;
private final Map<String, Slave> slaves = new HashMap<>();

@Setter
@Getter
public static class Slave {

private String name;
private String driverClassName;
private String url;
private String username;
private String password;
}
} 필드 변수명 하나하나가 yml 파일에서 설정한 이름과 매칭되어야 함을 주의하자. @ConfigurationProperties 어노테이션을 처음 사용하면 에러를 마주할 수 있는데, 공식 문서를 확인해보면 의존성 추가를 통해 해결할 수 있음을 알려준다. dependencies {
annotationProcessor ""org.springframework.boot:spring-boot-configuration-processor""
} 다음은 @Transactional(readOnly=true) 분기 처리를 위한 로직을 작성해야한다. 서비스 로직을 수행할 때 메서드에 붙은 어노테이션이 @Transactional(readOnly=true)인 경우 slave datasource로, 나머지는 master datasource로 분기 처리를 하기위한 RoutingDataSource 클래스를 생성한다. RoutingDataSource 클래스 public class ReplicationRoutingDataSource extends AbstractRoutingDataSource {

private static final String DATASOURCE_KEY_MASTER = ""master"";
private static final String DATASOURCE_KEY_SLAVE = ""slave"";

private DataSourceNames<String> slaveNames;

@Override
public void setTargetDataSources(Map<Object, Object> targetDataSources) {
super.setTargetDataSources(targetDataSources);

List<String> replicas = targetDataSources.keySet()
.stream()
.map(Object::toString)
.filter(string -> string.contains(DATASOURCE_KEY_SLAVE))
.collect(toList());

this.slaveNames = new DataSourceNames<>(replicas);
}

// 요청에서 사용할 DataSource Key 값 반환
@Override
protected Object determineCurrentLookupKey() {
boolean isReadOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly();

if (isReadOnly) {
logger.info(""Connection Slave"");
return slaveNames.getNext();
}

logger.info(""Connection Master"");
return DATASOURCE_KEY_MASTER;
}

public static class DataSourceNames<T> {

private final List<T> values;
private int index = 0;

public DataSourceNames(List<T> values) {
this.values = values;
}

public T getNext() {
if (index + 1 >= values.size()) {
index = -1;
}
return values.get(++index);
}
}
} ReplicationRoutingDataSource 클래스는 AbstractRoutingDataSource를 상속하여 구현해줘야한다. AbstractRoutingDataSource는 spring-jdbc 모듈에 포함되어 있는 클래스로, 복수의 datasource를 등록하고 상황에 맞게 원하는 datasource를 사용할 수 있도록 추상화한 클래스이다. determineCurrentLookupKey() 메서드 오버라이딩을 통해 현재 요청에서 필요한 Master/Slave 분기를 진행하고 사용할 datasource의 key값을 반환해준다. 여러 개의 Slave 서버를 골고루 사용해서 부하를 분산시킬 수 있도록 원형 연결리스트 형태의 클래스도 사용한다. 이를 통해 요청마다 인덱스가 증가/감소하면서 모든 Slave 서버를 순회할 수 있게 된다. 다음은 실제 datasource를 스프링부트에 등록하기 위한 config 클래스를 생성한다. DataSourceConfig 클래스 // @EnableAutoConfiguration(exclude = DataSourceAutoConfiguration.class)
@EnableConfigurationProperties(ReplicationDataSourceProperties.class)
@Configuration
public class ReplicationDataSourceConfig {

private final ReplicationDataSourceProperties dataSourceProperties;
private final JpaProperties jpaProperties;

public ReplicationDataSourceConfig(ReplicationDataSourceProperties dataSourceProperties,
 JpaProperties jpaProperties) {
this.dataSourceProperties = dataSourceProperties;
this.jpaProperties = jpaProperties;
}

// DataSourceProperties 클래스를 통해 yml 파일에서 읽어들인 DataSource 설정들을 실제 DataSource 객체로 생성 후 ReplicationRoutingDataSource에 등록
@Bean
public DataSource routingDataSource() {
DataSource masterDataSource = createDataSource(
dataSourceProperties.getDriverClassName(),
dataSourceProperties.getUrl(),
dataSourceProperties.getUsername(),
dataSourceProperties.getPassword()
);

Map<Object, Object> dataSources = new LinkedHashMap<>();
dataSources.put(""master"", masterDataSource);

for (Slave slave : dataSourceProperties.getSlaves().values()) {
DataSource slaveDatSource = createDataSource(
slave.getDriverClassName(),
slave.getUrl(),
slave.getUsername(),
slave.getPassword()
);

dataSources.put(slave.getName(), slaveDatSource);
}

ReplicationRoutingDataSource replicationRoutingDataSource = new ReplicationRoutingDataSource();
replicationRoutingDataSource.setDefaultTargetDataSource(masterDataSource);
replicationRoutingDataSource.setTargetDataSources(dataSources);

return replicationRoutingDataSource;
}

// DataSource 생성
public DataSource createDataSource(String driverClassName, String url, String username, String password) {
return DataSourceBuilder.create()
.type(HikariDataSource.class)
.url(url)
.driverClassName(driverClassName)
.username(username)
.password(password)
.build();
}

// 매 쿼리 수행마다 Connection 연결
@Bean
public DataSource dataSource() {
return new LazyConnectionDataSourceProxy(routingDataSource());
}

// JPA에서 사용하는 EntityManagerFactory 설정. hibernate 설정을 직접 주입한다.
@Bean
public LocalContainerEntityManagerFactoryBean entityManagerFactory() {
EntityManagerFactoryBuilder entityManagerFactoryBuilder = createEntityManagerFactoryBuilder(jpaProperties);
return entityManagerFactoryBuilder.dataSource(dataSource())
.packages(""{프로젝트 패키지 경로 ex) gg.babble.babble}"")
.build();
}

private EntityManagerFactoryBuilder createEntityManagerFactoryBuilder(JpaProperties jpaProperties) {
JpaVendorAdapter vendorAdapter = new HibernateJpaVendorAdapter();
return new EntityManagerFactoryBuilder(vendorAdapter, jpaProperties.getProperties(), null);
}

// JPA에서 사용할 TransactionManager 설정
@Bean
public PlatformTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) {
JpaTransactionManager tm = new JpaTransactionManager();
tm.setEntityManagerFactory(entityManagerFactory);
return tm;
}
} 가장 첫 줄을 보면 주석 처리가 되어있다. exclude 옵션을 이용해 DataSource 자동설정(DataSourceAutoConfiguration)을 제외시킬 수 있으나, Spring 공식문서에서 DataSourceAutoConfiguration에 대해 침투적이지 않다(Auto-configuration is > non-invasive)라고 평가한다. 때문에 굳이 제외시킬 필요 없다고 생각해서 포함하지 않았다. 위 코드에서 눈에 띄는 부분은 LazyConnectionDataSourceProxy()일 것이다. 우선 Spring은 트랜잭션에 진입하는 순간 이미 설정된 DataSource의 커넥션을 가져온다. TransactionManager가 트랜잭션을 식별하면 DataSource의 커넥션을 가져오고, 트랜잭션의 동기화가 시작되어버린다. 이럴 경우 다중 DataSource 환경에서는 DataSource를 선택하는 분기가 불가능하다. 따라서 미리 DataSource를 정하지 않도록 LazyConnectionDataSourceProxy를 사용하여 실제 쿼리가 실행될 때 Connection을 가져오도록 한 것이다. 설정을 모두 마쳤으면 애플리케이션을 실행해보자. 다른 이슈가 없다면 정상적으로 서비스가 진행되며 로거를 통해 Master/Slave 데이터베이스를 번갈아서 사용하고 있음을 확인할 수 있다. 요약 및 이슈 내용 스프링 DataSource 설정 요약 스프링 프로필 파일에 작성된 datasource 정보들을 DataSourceProperties 클래스를 통해 수동으로 매핑한다. isReadOnly 분기를 통해 Master/Slave 서버 선택을 유도한다. 리스트를 순환하면서 Slave 서버를 선택하도록해서, Slave 서버 부하를 분산시킨다. 매핑된 DataSource 설정들을 실제 DataSource 객체로 생성 후 ReplicationRoutingDataSource에 등록한다. JPA가 사용할 EntityManagerFactory를 수동으로 설정한다. JPA가 사용할 TransactionManager를 수동으로 설정한다. 서비스로직 메서드마다 datasource가 바뀌어야하므로, LazyConnectionDataSourceProxy를 통해 proxy datasource를 연결하도록 설정한다. Master DataSource는 왜 컬렉션으로 관리하지 않는가? 현재 글에서 구성된 Master:Slave 는 1:N으로, Master 서버가 1개인 상황을 가정하고 Slave는 컬렉션으로 관리하는 형태의 Replication이 구성되어 있다. 이 때 ‘왜 Master는 1개여야 하지? Master도 여러개 준비하고 컬렉션으로 관리하면 안되나?’ 라는 생각이 들었고, 아래 그림과 같이 Master와 Master 간에도 서로 복제 관계를 가지는 것을 생각했다. 그러나 이 생각에 대해 우테코 질문 채널에 크루들의 의견을 물어보니, 완태가 아래와 같은 이야기와 함께 링크를 공유해주었다. https://bcho.tistory.com/1062 ""Slave에서 처럼 binlog를 사용해서 data를 주고 받으면 data gap이 발생하기 때문에, Master끼리 data동기화를 하는 데에 data 불일치성이 발생할 수 있을듯 뇌피셜 팡팡!"" 완태가 공유해준 링크의 글을 읽다보니, 내가 Master와 Master 간 복제 관계를 통해 해결하고자 했던 문제는 사실 Replication보다는 클러스터링을 통해 해결하는 것이 더 잘 어울리는 것 같았고, Replication을 통해서는 Master-Slave를 1:N 관계로 만드는 것으로 마무리 하기로 했다. Refernces Authentication Plugin - mysql_native_password - MariaDB [SpringBoot] Spring 에서 자동설정의 이해와 구현 (AutoConfiguration) [DB, Spring] Replication 적용하기 - velog.io/@max9106 SpringBoot AutoConfiguration을 대하는 자세 - Tecoble MySQL – Replication 구조 - Rain.i LazyConnectionDataSourceProxy 알아보기 스페샬 땡스투 완태, 에어 https://hyeon9mak.github.io/mysql-mariadb-replication-with-jpa/ MySQL(MariaDB) Replication with JPA 🕵️ summary 개요 복제(Replication)는 한 서버에서 다른 서버로 데이터가 동기화 되는 것을 말하며, 원본 데이터를 가진 서버를 소스(Source/Master) 서버, 복제된 데이터를 가지는 서버를 레플리카(Repli hyeon9mak.github.io",BE
74,"이전에 로컬에서 잘 돌아가고 있는 데이터베이스를 굳이 도커 위로 마이그레이션 시킨 경험이 있었다. 그 후로 큰 이슈 없이 서비스가 잘 동작했기 때문에 도커 위에 데이터베이스를 운영하는 것에 만족하고 있었는데, 얼마전 브라운이 초청해주신 홍정민 DBA님의 특별 강연에서 DBA님이 도커 위에 데이터베이스를 운영하는 것에 대해 부정적인 말씀을 하셔서, 질문을 드리고 답변을 받게 되었다. 그리고 답변 받은 내용을 베이스로 확신을 얻어서 도커 위에 올렸던 데이터베이스를 다시 로컬로 내려보았다! 도커에서 사용중인 데이터베이스 덤프 백업하기 도커에서 동작하고 있는 데이터베이스를 덤프로 추출하여 백업한다. 도커 컨테이너의 bash로 접속한 다음, mysqldump 명령어를 통해 덤핑한다. $ sudo docker exec -it {컨테이너 이름} bash

# dockershell에 접속된 상태
& sudo mysqldump -u {DB 계정 이름} -p {덤프할 스키마 이름} > {덤프 파일명}.sql 명령을 수행하면 도커 컨테이너 내부에 덤프 파일이 생성된다. 컨테이너 내부 덤프 파일을 로컬로 꺼내오면 된다. (혹시나 dockershell 에서 나오지 않았다면 exit 하자.) $ sudo docker cp {컨테이너 이름}:{덤프 파일경로} {복제할 로컬 경로} 명령을 수행하면 도커 컨테이너 내부에 있는 덤프 파일이 로컬 경로로 복제된다. 덤프 파일을 성공적으로 추출했다고 확신이 들면, 도커 컨테이너를 일시중지 시킨다. 일시중지 하지 않을 경우 mariadb 도커 컨테이너와 로컬 mariadb-server가 같은 포트번호(3306)를 사용하려 하기 때문에 충돌이 발생해서 동작하지 않는다. (/var/run/mysqld/mysqld.sock 관련 에러가 발생할 것이다.) $ sudo docker stop {컨테이너 이름} 로컬 mariadb-server 버전업 최근 버전업을 시도하면 더 이상 IP를 찾을 수 없다고 에러가 출력되며, $sudo apt install mariadb-server 명령어를 입력하면 자동으로 10.5 버전이 설치된다. 따라서 이 부분은 참고용으로 지나가도 될 것 같다. 로컬에서 apt install 명령을 통해 mariadb-server를 설치할 경우 10.1 버전이 설치된다. 보통 문제가 없겠지만, Flyway를 사용하는 경우 10.1 버전을 더 이상 사용할 수 없다. FlywayEditionUpgradeRequiredException: Flyway Teams Edition or MariaDB upgrade required: MariaDB 10.1 is no longer supported by Flyway Community Edition, but still supported by Flyway Teams Edition. 도커를 통해 사용하는 mariadb-server는 lastest 버전 설치시 자동으로 10.5 버전이 설치되기 때문에 문제가 없었지만, 로컬 설치에는 별도로 apt 설정을 해주어야 한다. $ sudo apt-get install software-properties-common

$ sudo apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8

$ sudo add-apt-repository 'deb [arch=amd64,arm64,ppc64el] http://mirror.lstn.net/mariadb/repo/10.5/ubuntu {bionic 또는 focal} main' 이 때 add-apt-repository 명령 입력시 Ubuntu 18.04 버전은 binoic을, 20.04 버전은 focal을 이용하면 된다. $ sudo apt update

$ sudo apt install mariadb-server 설정을 마치고 새로 설치를 진행할 때, 기존 mariadb-server가 설치되어 있는 경우 설치 마지막 과정에서 새 버전으로 설치할 것인지, 기존 버전을 유지할 것인지 질문이 나온다. Y 입력으로 새로운 10.5 버전으로 설치를 진행하면 된다. 설치에 성공한지 확인하고 싶다면 아래 명령을 수행하면 된다. $ sudo mysql -u root -p
(초기엔 비밀번호 없음)

> SELECT VERSION();
+----------------------------------------+
| VERSION() |
+----------------------------------------+
| 10.5.12-MariaDB-1:10.5.12+maria~bionic |
+----------------------------------------+ IP 주소 허용 수정 새로운 버전을 설치할 경우 기존 mariadb-server 설정 값이 모두 초기화 된다. bind-address 설정을 다시 바꾸기 위해 /etc/mysql/mariadb.conf.d/50-server.cnf 에 접근한다. # Instead of skip-networking the default is now to listen only on
# localhost which is more compatible and is not less secure.
bind-address = 0.0.0.0 bind-address 설정 기본값이 127.0.0.1 일텐데, 0.0.0.0으로 바꿔서 모든 ip를 허용해주면 된다. 덤프 불러오기 설정을 모두 마쳤다면 덤프 파일을 다시 불러와야 한다. mariadb-server에 접속한 뒤 애플리케이션 서버에서 사용할 계정과 스키마를 생성하고 계정에 스키마 관리 권한을 부여해주자. (권한은 각 프로젝트 정책에 맞게 잘 부여하자.) $ mysql -u root -p

> create user {사용 계정명}@'{애플리케이션 서버 ip}' identified by '{비밀번호}';
> create database {스키마 이름};
> grant all privileges on {DB 이름}.* to {사용 계정명}@'{애플리케이션 서버 ip}'; 설정을 모두 끝냈다면 exit 명령으로 mariadb-server에서 빠져 나온뒤, 아래 명령으로 덤프 파일을 읽게 하자. $ sudo mysql -u root -p {스키마 이름} < {덤프 파일명}.sql 성공적으로 덤프가 불러와졌을 경우, 아래와 같이 결과를 확인할 수 있다. > show databases;
+--------------------+
| Database |
+--------------------+
| babble |
| information_schema |
| mysql |
| performance_schema |
+--------------------+
4 rows in set (0.000 sec)

> use babble
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed

> show tables;
+-----------------------+
| Tables_in_babble |
+-----------------------+
| administrator |
| alternative_game_name |
| alternative_tag_name |
| flyway_schema_history |
| game |
| room |
| session |
| tag |
| tag_registration |
| user |
+-----------------------+ References 과연 도커(Docker) 컨테이너를 통해 데이터베이스를 운영하는 게 좋은 방법일까? - 규도자 개발 블로그 [MariaDB] Docker Container에 올렸던 DB를 도커 바깥으로 빼내기 - 기록기록 How to Install MariaDB 10.5 on Ubuntu 18.04, Ubuntu 20.04 - LinuxBabe https://hyeon9mak.github.io/migrate-docker-database-to-local/ 도커 위의 DBMS를 로컬로 마이그레이션 하기 이전에 로컬에서 잘 돌아가고 있는 데이터베이스를 굳이 도커 위로 마이그레이션 시킨 경험이 있었다. 그 후로 큰 이슈 없이 서비스가 잘 동작했기 때문에 도커 위에 데이터베이스를 운영하는 hyeon9mak.github.io",BE
75,"Goal Reverse Proxy를 거치기전의 Clinet의 IP를 WAS에서 알수 있도록 구성해봅니다. 개요 IP 차단 기능을 구현했는데, 배포환경에선 정상적으로 작동하지 않았습니다. Reverse Proxy를 사용할경우 원인은, WAS측에서 요청한 클라이언트의 IP를 보고 필터링을 하도록 구현했는데, WAS는 실제 Client가 아닌, 본인입장에서 Client인 중간에 있는 리버스 프록시(NGINX)의 IP를 알게되기 때문입니다. 해결 리버스 프록시에서 X-Forwarded-For Header 추가 X-Forwarded-For Header 는 엄밀하게는 표준이 아니지만, 이런 상황에 대해서 사실상 표준처럼 쓰이는 헤더입니다. 리버스 프록시(NGINX)쪽에서 X-Forwarded-For Headerd의 값으로 클라이언트 IP를 넣어줍니다. WAS측에선 X-Forwarded-For 헤더에 담겨있는 IP를 확인하도록 수정합니다. NGINX 설정 추가 NGINX에서는 $proxy_add_x_forwarded_for 라는 변수를 제공합니다. 클라이언트의 IP 주소를 담고있는 $remote_addr 를 X-Forwarded-For Header의 값으로 추가해 주게 됩니다. // nginx.conf
...

location / {
proxy_pass http://app/;
proxy_http_version 1.1;
proxy_set_header Upgrade $http_upgrade;
proxy_set_header Connection ""upgrade"";
proxy_set_header Host $host;

proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 이부분을 추가해주었다.
}
...
WAS 로직 수정 private String getClientIpFrom(final HttpServletRequest request) {
String clientIp = request.getHeader(X_FORWARDED_FOR_HEADER);

if (Objects.isNull(clientIp)) {
clientIp = request.getRemoteAddr();
}

return clientIp;
} Conclusion X_FORWARDED_FOR_HEADER 를 이용해 EndPoint Clinet의 IP를 얻을 수 있다. 최초 요청을 보내는 클라이언트에서는 추가적인 작업이 전혀 필요없다. NGINX에서의 설정, WAS의 구현만 살짝 바꾸어주면 해결되었다. Reference https://developer.mozilla.org/ko/docs/Web/HTTP/Headers/X-Forwarded-For https://www.nginx.com/resources/wiki/start/topics/examples/forwarded/",BE
76,"babble 팀에선 프로젝트 초기 단계에서 아래와 같은 Git branch merge 전략을 사용하고 있었다. 기존 babble 팀에서 사용하던 Merge 전략 신규 기능 개발시 Develop를 기준으로 새로운 브랜치를 생성하고 작업을 진행한다. 개발이 완료되었을 때 작업된 커밋 내용들을 Squarsh Merge를 이용해서 병합한다. Squarsh Merge를 통해 Develop 브랜치에 병합된 내용들을 Release, Main 브랜치에 동기화 시킬 땐 일반적인 Merge를 이용한다. 정리하자면 아래와 같다. 신규기능 브랜치 --Squarsh Merge--> Develop Develop --(Default) Merge--> Release Release --(Default) Merge--> Main 이와 같은 Merge 전략을 사용하고 있었지만, 일반 Merge 특성상 Merge시 완전히 새로운 커밋을 작성하기 때문에 Develop 브랜치와 커밋 내용이 일치하지 않은 경우가 많았고, 이 때문에 Release, Main 브랜치의 커밋기록으로 어떤 기능이 추가되었는지 한 눈에 파악이 어려웠다. 그러다 (Default) Merge를 사용하는 단계에서 Rebase Merge를 사용해보면 어떻겠냐는 의견이 나오게 되었다. Rebase Merge의 경우 병합되기 전 브랜치가 가지고 있는 모든 커밋 정보를 그대로 복사하기 때문에 작업 내용을 추적하기 용이하다는 점과, 기능 개발이 어떤 단위로 이루어졌는지 한 눈에 파악하기 좋다는 장점이 있을거 같았다. 때문에 우리는 아래와 같은 가설을 세우면서 Rebase Merge를 적극 도입하게 되었다. babble 팀의 가설 Develop 브랜치에서 Release 브랜치 쪽으로 Rebase Merge 진행하면 동일한 해시값을 가진 커밋이 그대로 복제되어 생성되고, Main 브랜치까지도 이 커밋을 그대로 복제해서 옮길 수 있을거라 생각했다! 작업이 반복되면, Develop, Release, Main 브랜치간 커밋이 완전히 동기화된 상태로 예쁘게 관리될 것 같았다! 그렇게 꿈에 부푼채로 Rebase Merge를 도입했지만, 얼마 안가 문제가 발생했다. Rebase Merge의 실제 동작와 충돌 Rebase Merge는 같은 해시코드를 가진 커밋을 복제해서 만드는게 아니라, 내용만 동일한 완전히 새로운 커밋을 새로 만드는 방식이었다. 때문에 첫 Develop 브랜치에서 Main 브랜치까지의 Rebase Merge는 성공적으로 수행할 수 있었지만, 곧이은 2번째 Rebase Merge 작업에서는 Develop 브랜치와 Release 브랜치간 서로 브랜치 내용이 다르다고 판단되어 작업이 정상적으로 이루어지지 않았다. 그래서 결국엔... 아쉽게도 기존에 사용중이던 Merge 전략을 다시 사용하고 있다. https://hyeon9mak.github.io/git-rebase-merge-trouble-shooting/ Git rebase merge 트러블 슈팅 babble 팀에선 프로젝트 초기 단계에서 아래와 같은 Git branch merge 전략을 사용하고 있었다. hyeon9mak.github.io",NON_TECH
78,"summary 우아한테크코스 레벨4 HTTP 서버 구현하기 미션을 진행하던 중이었다. 미션대로 구현을 완료하고 PR(Pull Request)를 작성했는데 정적분석 툴 SonarQube 가 자동으로 동작하고, 분석 결과도 PR 페이지에서 곧장 보여주고 있었다?! 자세히보니 SonarQube가 아니라 SonarCloud 였다! SonarQube에서 제공하는 Github 연동 서비스는 유료로 알고 있는데, SonarCloud의 경우 무료로 이용할 수 있었다!! 마침 Babble 팀에서도 배포 했을 때가 아닌 PR을 작성 했을 때 정적분석이 이뤄지길 바라고 있었고, 분석 결과를 PR 페이지에서 곧바로 확인할 수 있다는게 너무 멋져서 도입을 고려하게 되었다. SonarCloud에 대해 조사하다보니, SonarCloud를 이용한 CI-based Analysis랑 Automatic Analysis를 동시에 구성하면 충돌한다는 이야기를 접하게 되었다. SonarQube를 이용한 CI-based Analysis를 유지할지, SonarCloud를 이용한 Automatic Analysis를 새로 적용할지 팀원들과 상의가 필요했다. ""PR 단계에서 정적 분석을 미리 확인하면 배포시에 굳이 확인 안해도 되지 않을까?"" ""그래도 배포시에도 확인하고 싶어."" ""그럼 둘 다 가능한가? 둘 다 해보는 건?"" 결국 SonarQube를 이용한 CI-based Analysis를 유지하면서 SonarCloud를 이용한 Automatic Analysis를 적용해보는 것으로 결정되었다. 적용기 우선 https://sonarcloud.io/에 접속, 깃허브 계정으로 로그인 후 Analyze new project를 클릭해서 Babble 팀 저장소를 등록하려 하니 계속해서 SonarCloud 페이지가 먹통이었다. 마침 HTTP 서버 구현하기 미션 진행중이라 한참 들여다보던 크롬 개발자 모드-네트워크 탭을 살펴보니 에러가 하나 보였다. errors: [{msg: ""User is not admin of the ALM organization""}] Babble 팀 저장소는 woowacourse-teams/2021-babble 로, 팀원 모두 저장소에 대해 관리자 급 권한을 가지고는 있지만 실제 저장소의 주인은 woowacourse-teams 그룹 관리자다. 이 때문에 내 깃허브 계정으로는 SonarCloud에 woowacourse-teams/2021-babble 저장소 연결이 더 이상 불가능하다고 판단했고, 구구 코치님께 DM을 드렸다. '멋져서 하는 건 인정이지' 납득해주신 구구가 woowacourse-teams/2021-babble 저장소에 SonarCloud 깃허브 앱을 연결해주셨지만, 여전히 권한이 없다는 에러(This action must be performed by an organization owner)와 함께 SonarCloud와 연동이 진행되지 않았다. 2021-babble 저장소를 SonarCloud 서버 상에 프로젝트를 등록하기 위해선 woowacourse-teams organization의 관리자임이 증명되어야하는데, 우리 팀 멤버들은 woowacourse-teams organization의 관리자가 아니다. 때문에 This action must be performed by an organization owner 에러가 계속해서 발생한 것이다. 이어서 구구께서 woowacourse-teams_2021-babble 이름의 SonarCloud 프로젝트를 개설해주셨다. 구구께서 공유해주신 링크를 통해 프로젝트가 생성된 것을 확인할 수 있었으나, 프로젝트를 이용할 수 있는 권한이 없었다. 당연히 PR을 작성해도 정적분석이 일어나지 않았다. 그 다음은 구구께서 내 SonarCloud 아이디를 찾아 woowacourse-teams_2021-babble 프로젝트의 멤버로 등록해주셨다. 프로젝트의 멤버로 등록된 이후 프로젝트 이용이 가능해졌고, Github-actions를 이용한 정적분석 workflow 파일 등록을 진행했다. 여러가지 Analysis Method 중 Github-actions를 이용한 방법을 선택한다. Github-actions를 이용하는 가이드에서 깃허브 저장소에 SONAR_TOKEN 이름의 Secret을 등록하고 사용할 것을 권장하는데, 우리 팀은 이미 SONARQUBE_TOKEN 이름의 Secret을 사용하고 있어 혼란이 있을거 같았다. 때문에 SONARCLOUD_TOKEN으로 이름을 바꿔서 등록했다. 프로젝트의 build.gradle에 sonarqube 플러그인과 의존성들을 추가해준다. 이어서 .github/workflows/ 경로에 SonarCloud 정적분석용 workflow 파일을 작성해주면 된다. 우리 팀에선 아래와 같이 작성했다. name: SonarCloud Automatical Analysis Build
on:
push:
branches:
- main
- release
- develop
pull_request:
types: [opened, synchronize, reopened]

jobs:
build:
name: Build
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v2
with:
fetch-depth: 0 # Shallow clones should be disabled for a better relevancy of analysis
- name: Set up JDK 11
uses: actions/setup-java@v1
with:
java-version: 11
- name: Cache SonarCloud packages
uses: actions/cache@v1
with:
path: ~/.sonar/cache
key: ${{ runner.os }}-sonar
restore-keys: ${{ runner.os }}-sonar
- name: Cache Gradle packages
uses: actions/cache@v1
with:
path: ~/.gradle/caches
key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle') }}
restore-keys: ${{ runner.os }}-gradle

- name: update gradlew access authorized
working-directory: ./back/babble
run: chmod +x gradlew

- name: Build and analyze
working-directory: ./back/babble
env:
GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Needed to get PR information, if any
SONAR_TOKEN: ${{ secrets.SONARCLOUD_TOKEN }}
run: ./gradlew build sonarqube --info (update gradlew access authorized 부분에서 gradlew 파일의 권한을 수정하는데, 파일의 권한을 수정하지 않을 경우 Build and analyze 단계에서 gradlew 파일 액세스 권한 관련 에러가 발생한다.) 여기까지 진행하고 PR을 다시 작성해보니, 정적분석이 동작했다!! 그러나 sonarcloud(bot)이 정적분석 결과를 PR에 코멘트로 남겨주지 않았다. 분명 2021-babble 저장소에 sonarcloud가 github app으로 등록되어 있었고, 정적분석도 잘 동작했다. SonarCloud 프로젝트 상에도 정적분석 결과가 잘 전달되어 있었다. 그런데 왜 sonarcloud(bot)은 동작하지 않았을까? 원인 파악을 위해 구글링을 하던 중 SonarCloud 커뮤니티에 'GitHub Pull Request does not show analysis results' 게시글을 발견 할 수 있었다. 글의 내용은 ""정적분석은 완료되었으나 그 분석결과가 Pull Request에 표기되지 않는다.""는, 우리와 완전히 동일한 상황에 처한 글이었다. 답변으로는 Bind this organization to GitHub - Choose the organization on Github 설정이 되어있는지 확인해보라는 것이었다. 실제로 woowacourse-teams 조직과 woowacourse-teams_2021-babble 프로젝트가 바인딩되어 있지 않았다. 2021-babble 저장소의 workflow yml 파일이 sonarcloud의 woowacourse-teams_2021-babble 프로젝트에 분석결과는 전달했지만, 그 분석결과를 PR에 작성하기 위해선 sonarcloud가 2021-babble까지 접근해야하는데, 2021-babble과 연결되어 있지 않으니 작성이 불가능한 것이었다! 직접 바인딩을 하려하니 다시 한 번 권한 문제를 마주쳤다. 이번에도 구구께 바인딩을 요청드리니... 그토록 고대하던 정적분석 결과 sonarcloud(bot)이 등장했다!! PR Merge 전 정적분석 통과를 필수로 만들기 저장소의 Settings-Branches 메뉴로 이동해서 설정을 진행하고 싶은 브랜치 각각의 Edit 화면으로 이동한다. Require status checks to pass before merging을 선택하면 선택한 기준이 통과되기 전까진 브랜치 Merge가 불가능 하다! 그 후 Search for status checks in the last week for this repostiroy 검색을 통해 SonarCloud를 등록한다! Test Coverage 낮추기 SonarCloud를 이용한 Automatical 정적분석을 등록한지 얼마 되지 않아, 루트의 PR이 분석 기준을 통과하지 못했다는 메일이 날아왔다. PR을 직접 확인해보니 Test Coverage가 78.2%로, 기준인 80%를 넘지 못해서였다. Test Coverage를 높게 채울수록 좋겠지만, Test Coverage에 휘둘려 기능개발 시간을 잡아먹히길 원하진 않았다. 팀원 모두가 Test Coverage를 조금 낮추길 원했기 때문에 70%로 변경을 진행했다. 우선 SonarCloud 프로젝트의 Quality Gates 메뉴로 접근한다. 기본으로 제공하는 Sonar way의 우측상단 Copy 버튼을 눌러 Quailty Gate를 복제한다. 모든 통과기준이 동일하게 복제된 상태에서, 커버리지에 대해서만 Edit 버튼을 통해 수정하고, Set as Default 버튼을 통해 기본 Quailty Gate로 등록해주는 방법으로 Test Coverage를 낮출 수 있었다. (따로 Quailty Gate가 등록된 프로젝트가 아니라면 모두 Default Quailty Gate를 사용하게 된다.) SonarQube vs SonarCloud SonarQube와 SonarCloud의 차이가 무엇인지 궁금해서 살짝 조사해보았다. SonarQube는 CI-based 기반 정적분석, SonarCloud는 Cloud 기반 Automatical 정적분석. CI-based는 말 그대로 빌드/배포시에 정적분석 발생하고 SonarQube 스캐너, SonarQube 서버, SonarQube DB 등이 준비 되어 있어야 한다. Automatical은 SonarQube서버, SonarQube 스캐너 등을 준비하지 않아도 SonarCloud에 프로젝트로 등록만 해두면 PR 단계에서 자동으로 정적분석. CI-based도 함께 설정 할 수 있지만, 아쉽게도 gradle는 SonarCloud CI-based 정적분석을 지원하지 않는다고 한다. 정리해보니 SonarQube의 장점이 거의 없는거 같지만? SonarCloud는 어디까지나 SonarCloud에서 제공하는 서버를 사용하고, SonarQube는 별도의 서버와 데이터베이스를 직접 구축할 수 있다. 그만큼 데이터관리나 확장이 자유롭기 때문에 각각의 메리트가 있는거 같다. 후기 babble 팀이 SonarCloud를 도입한게 아니라, 구구께서 babble 팀에 SonarCloud를 도입시켜주신게 아닐까 싶은 경험이었다. (항상 감사합니다 구구! 🙇‍♂️) SonarCloud의 프로젝트 등록이 지나치게 어렵지 않나라고도 생각했지만, 다른 조직 프로젝트의 코드 취약점을 확인해서 공격하는 방식으로도 악용할 수 있기 때문에 이렇게까지 프로젝트 등록을 어렵게 막아둔게 아닐까 싶기도 하다. 개인 프로젝트 저장소 정도는 간단하게 SonarCloud 프로젝트로 등록할 수 있으므로, 주변에서 SonarQube와 SonarCloud 중 하나를 추천해달라하면 SonarCloud를 적극 추천할거 같다. References Github Code & Quality Analysis - SonarQube GitHub Pull Request does not show analysis results - sonarsource community Setting up sonarcloud on gradle project on github - sonarsource community SonarSource/sonarcloud-github-action - Github repository GitHub Actions and SonarCloud - technology.amis.nl https://hyeon9mak.github.io/sonarcloud-trouble-shooting/ SonarCloud 적용 트러블 슈팅 summary 우아한테크코스 레벨4 HTTP 서버 구현하기 미션을 진행하던 중이었다. 미션대로 구현을 완료하고 PR(Pull Request)를 작성했는데 정적분석 툴 SonarQube 가 자동으로 동작하고, 분석 결과도 PR 페이 hyeon9mak.github.io",BE
101,"summary 기존 Web-server (Reverse-proxy) 역할로 사용자와 WAS(Web Application Server)간 통신을 이어주던 NGINX에서 소나큐브 서버도 통신을 지원하도록 설정하고 싶었다. 그림으로 표현하자면 아래와 같다. nginx.conf 설정 # nginx.conf

events {}

http {

# -------------------- spring-boot WAS --------------------
upstream app {
server 192.168.1.20:8080;
}

# Redirect all traffic to HTTPS
server {
listen 80;
server_name api.babble.gg;
return 308 https://$host$request_uri;
}

server {
listen 443 ssl;
server_name api.babble.gg;

ssl_certificate /etc/letsencrypt/live/api.babble.gg/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/api.babble.gg/privkey.pem;

...(생략)

location / {
proxy_pass http://app;
proxy_http_version 1.1;
proxy_set_header Upgrade $http_upgrade;
proxy_set_header Connection ""upgrade"";
proxy_set_header Host $host;
}
}

# -------------------- sonarqube server --------------------
upstream sonarqube {
server 192.168.1.15:9000;
}

server {
listen 80;
server_name sonarqube.babble.gg;
return 308 https://$host$request_uri;
}

server {
listen 443 ssl;
server_name sonarqube.babble.gg;

ssl_certificate /etc/letsencrypt/live/sonarqube.babble.gg/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/sonarqube.babble.gg/privkey.pem;

...(생략)

location / {
proxy_pass http://sonarqube;
}
}
} 서버 별 설정은 서버의 특성에 맞게 지정되므로 크게 중요하지 않아 생략했고, nginx.conf 파일 쪽에 명령어 구성을 어떻게 하느냐가 핵심이다. upstream 우선 upstream 변수를 설정해준다. upstream 변수는 server 설정에서 NGINX가 받아들인 요청을 어떤 서버로 흘려보내 줄 것인지 결정할 때 사용된다. 보통 아래와 같이 IP와 PORT를 지정해주는 것으로 설정이 끝난다. upstream exampleServer {
server 192.168.0.1:1234;
} server - https redirect server 명령어를 통해 http 요청을 https 로 redirecting 시키는 용도로 사용했다. 내부의 listen 명령어를 통해 NGINX가 받아들이는 PORT 번호(80)를 지정하고, server_name 명령어를 통해 도메인 명을 지정해주는 것으로 어떤 서버 쪽의 요청에 반응하는 것인지 설정한다. server {
listen 80;
server_name example.server.com;
return 308 https://$host$request_uri;
} return 명령어를 통해 상태코드와 https://$host$request_uri를 넘겨주는데, 상태코드를 308로 할 것을 추천한다. 301,302,303 Redirect HTTP Code는 POST, PUT 같은 요청 GET으로 바꿔서 Redirect 시킬 가능성이 있어서 302->307, 301->308로 사용하는 것이 좋다고 한다. server - configure server 명령을 통해 서버 설정을 진행한다. 내부의 listen 통해 https(443) 요청을 받아들이게 지정하고, server_name 명령어를 통해 도메인 명을 지정해주는 것으로 어떤 서버 쪽의 요청에 반응하는 것인지 설정한다. 여러가지 서버 설정 이후 location 설정에 upstream 변수로 등록한 주소를 기입하여 최종적으로 요청을 흘려보낸다. letsencrypt pem 키 설정 nginx.conf 파일 내용을 살펴봤다면 예상하다 싶이 letsencrypt 인증을 사용하고 있다. 그런데 기존 Dockerfile을 이용해 nginx.conf를 다룰 때 아래와 같이 pem 키들도 다루고 있었다. # Dockerfile

FROM nginx

COPY nginx.conf /etc/nginx/nginx.conf
COPY fullchain.pem /etc/letsencrypt/live/api.babble.gg/fullchain.pem
COPY privkey.pem /etc/letsencrypt/live/api.babble.gg/privkey.pem 소나큐브와 스프링부트 WAS서버 각각의 인증서를 발급받았으므로 도커 컨테이너 상의 각자의 경로(/etc/letsencrypt/live/[도메인]/)에 복제를 해야하는데, fullchain.pem과 privkey.pem 이름이 겹치게 된다. 원래 로컬상의 pem키도 /etc/letsencrypt/live/[도메인]/ 경로에 존재하지만, Dockerfile에서 /etc/letsencrypt/live/[도메인주소]/까지 접근할 권한이 없기 때문에(=접근할 수 없기 때문에) sudo cp 명령어를 통해 미리 Dockerfile과 같은 경로로 pem 키들을 복제해둔 상태다. 해결방법은 간단하게 각각의 도메인 이름에 맞춰 pem 키 이름을 바꿔서 복제해두고, Dockerfile에서 COPY를 수행할 때만 다시 기존의 이름으로 되돌려주면 된다. # Dockerfile

FROM nginx

COPY nginx.conf /etc/nginx/nginx.conf
COPY api-fullchain.pem /etc/letsencrypt/live/api.babble.gg/fullchain.pem
COPY api-privkey.pem /etc/letsencrypt/live/api.babble.gg/privkey.pem
COPY sonarqube-fullchain.pem /etc/letsencrypt/live/sonarqube.babble.gg/fullchain.pem
COPY sonarqube-privkey.pem /etc/letsencrypt/live/sonarqube.babble.gg/privkey.pem References 하나의 Nginx로 여러 upstream 처리하기 :: 공부 일기 https://hyeon9mak.github.io/nginx-upstream-multi-server/ NGINX 다중 서버 upstream 설정 🚂 summary 기존 Web-server (Reverse-proxy) 역할로 사용자와 WAS(Web Application Server)간 통신을 이어주던 NGINX에서 소나큐브 서버도 통신을 지원하도록 설정하고 싶었다. 그림으로 표현하자면 아래와 같다. hyeon9mak.github.io",BE
104,"들어가기 Web Socket.. 많이 들어봤지만, 들을 때마다 친구들의 곡소리가 끊이지 않던 친구였다. 그래서인지 괜히 '웹소켓'이라는 말을 들으면 지레 겁부터 먹었었다. 한 번 공부해보고 싶은 주제였지만, 지금껏 딱히 사용할 기회가 없었기도 해서(변명변명).. 웹소켓은 점점 더 멀어져갔다. 그런데! 이번에 레벨3 팀 프로젝트 중, 피터가 제안한 '태그 기반의 게임 팀 매칭 서비스'에서 채팅을 메인으로 다룬다고 했다. 서비스 자체도 재미있을 것 같았고, 웹소켓에 대한 알 수 없는 도전정신이 나를 지배해 이 팀에 지원하게 되었다. 지원한 결과, 감사하게도 뽑기 신께서 도움을 주셔서 Babble 팀에 합류할 수 있게 되었다. 후헹헹ㅎㅎ 이제 Babble 프론트엔드 팀이 채팅을 구현하면서 WebSocket에 대해 공부한 내용에 대해 살짝 이야기해보겠다. 헤헹 🔌 Web Socket은 뭐야, 아니 그전에 Socket이 뭐야? Socket의 뜻을 검색해보면, 아래와 같이 나온다. 이 뜻을 생각하면서 네트워크에서의 Socket이 뭔지 한 번 유추해보자. 다른 부분이 들어갈 수 있도록 만들어 놓은 구멍, 서로 다른 어떤 것들이 서로 연결될 수 있도록 만들어놓은 창구 정도로 해석할 수 있다. 즉, 컴퓨터에서 프로그램이 네트워크에서 데이터를 통신할 수 있도록 연결해주는 친구이다. 콘센트를 한 번 생각해보자. 내가 아무런 구멍이 없는 벽에 백번 코드를 넣으려 한다 해도, 전기를 사용할 수 없을 것이다. 마찬가지로 보내는 쪽(Client Socket), 받는 쪽에(Server Socket) 모두 Socket이 열려 있어야 그 Socket을 통해 데이터를 보내고, 받을 수 있다. 여기에서 Client는 우리가 사용하는 컴퓨터, Server는 AWS에 있는 서버 컴퓨터를 생각하면서 글을 읽으면 한결 이해하기 쉽다. 🖥 Socket Programming 프로세스 간 통신에 사용되는 Socket을 이용한 통신 프로그래밍을 Socket Programming이라고 한다. 이제부터 Socket Programming에 대해 알아보겠다. Client Socket과 Server Socket 그러면 Client Socket과 Server Socket은 각각 무슨 역할을 할까? 통신 연결 요청을 받아들이는 Socket을 Server Socket, 통신 연결 요청을 보내는 Socket을 Client Socket이라고 한다. 동일한 구조의 Socket이지만, 역할에 따라 처리되는 흐름이 조금씩 다르다. Client Socket의 처리 흐름 (컴퓨터) Client Socket이 처리되는 흐름은 아래와 같다. 1. Client Socket 생성 - 연결 대상에 대한 정보가 들어있지 않은 껍데기 Socket을 생성한다. 2. 연결 요청(Connection) - 연결하고싶은 대상한테 '연결해줘!' 라고 요청을 보낸다. IP 주소와 Port 번호로 연결하고 싶은 대상을 특정한다. - 요청을 보낸다고 끝나는 것이 아니고, 그 요청에 대한 결과가 돌아와야지 Connect의 실행이 끝난다. 3. 데이터의 송수신 (Send, Recieve) - 이 또한 연결 요청처럼, 요청을 보낸다고 끝나는 것이 아니고 그 요청에 대한 결과가 들어와야 실행이 끝난다. - 하지만 송수신 간의 차이점은 존재한다. - 송신(Send)할 때에는 데이터를 보내는 것이기 때문에 데이터를 언제, 얼마나 보낼 것인지를 알 수 있다. - 수신(Recieve)할 때에는 상대방이 언제, 얼만큼의 데이터를 보낼 것인지 알 수가 없다. - 그렇기 때문에 수신하는 API는 별도의 Thread에서 진행하게 된다. 4. Socket 닫기 - 더 이상의 데이터 송수신이 필요없어지게되면, 소켓을 닫는다. Server Socket의 처리 흐름 Server Socket이 처리되는 흐름에 대해 알아보기 전에! 먼저 살짝 Socket과 Port에 관해 알아보자. 이 데이터를 송수신함에 있어서 중요한 건 뭐다?!! 뭐다?!! 보안이다. 내가 A에게 카톡을 보내면, 동네방네 카톡이 돌아다니는 게 아니라, A의 카톡에 잘 도착해야한다. 내가 내 사진을 웹사이트에 올리면, 다른 웹사이트가 아니라 내가 올리려고 한 그 웹사이트에 사진이 올라가야한다. 이렇게 내가 어떤 데이터를 전송한다고 했을 때, 받는 입장에서 무작정 데이터를 수신하는 것이 아니라 데이터를 받을 호스트의 프로세스 레벨에서 어떤 프로세스가 이 데이터를 받아야하는 지 판단하고나서 데이터를 수신한다. ※ 호스트 - 네트워크 주소(IP 주소)가 할당된 Node (인터넷에 연결된 애들이라고 보면 편하다) ※ Node - 네트워크에 연결된 모든 종류의 장치 그럼 이 프로세스가 그 특정한 프로세스인지 어떻게 식별할까? 바로 Port로 식별한다. Port는 이렇게 네트워크를 통해 데이터를 주고받을 때, 프로세스를 식별하기 위해 호스트 내부적으로 프로세스가 할당받는 고유한 값이다. 데이터를 보낼 때, 데이터의 목적지에 Port를 입력해 전달한다. 그렇게 데이터가 도착하면, 목적지에 적힌 Port 번호를 가지고있는 프로세스를 찾아 데이터를 슉 전달한다. 이 프로세스는 양방향이다. 즉, 데이터를 보낼 때에도 Port 번호가 필요하다. 왜냐하면! 데이터를 받아 요청한 내용을 처리해서 응답을 다시 보내주려면 또 그 프로세스를 식별하기 위해 Port가 필요하기 때문이다. 프로세스를 식별할 때 Port를 사용한다는 것에 유념하면서 Server Socket의 처리과정을 한 번 알아보자. Port에 대해 100% 이해하지 못했더라도 아래 흐름을 읽다보면 조금 더 이해가 잘 될 것이다. 호호호 (아니면죄송합니다) 1. Server Socket 생성 (socket) - Client Socket 생성 시와 마찬가지로, 연결 대상에 대한 정보가 들어있지 않은 껍데기 Socket을 생성한다. 2. Server Socket 바인딩 (bind) - Socket과 Port 번호를 바인딩한다. 이게 당최 뭔소리냐.. 싶으실 수 있겠지만 .. 조금 더 설명해 보겠읍니다.. 우리는 컴퓨터를 사용하면서 한 번에 한가지 일만 하는게 아니라, 엄청나게 많은 일을 한꺼번에 한다. (물론 한 가지 일만 할 수도 있겠찌만 헤헷) 인강을 켜놓고 카카오톡을 켜고 친구와 대화하면서, 웹 브라우저를 열어 Babble에 들어가 익명의 유저들과 채팅을 하기도 하고.. 엄청나게 다양한 일을 할 것이다. (하나에 집중을 못하는 나의 모습과 닮아있따) 이렇게 되면, 컴퓨터에서는 많은 프로세스가 돌아가게 된다. 우리가 자주 쓰는 Activity Monitor, 즉 작업 표시창을 켜보면 이렇게 컴퓨터에서 돌아가는 수많은 프로세스들을 확인할 수 있다. 컴퓨터에서 A부터 Z까지의 프로세스가 돌아간다고 가정하자. 이렇게 돌아가는 다양한 프로세스 중에 프로세스 A, B, C, D, E 가 네트워크 관련 기능을 수행한다고 또 가정해보자. 이 중, A, C, D가 데이터의 송수신에 관련된 프로세스를 다룬다면, 각 프로세스에서 사용하는 Socket은 데이터를 송수신하기 위한 Port 번호를 사용하게 된다. (이 Port 번호는 프로토콜 표준에 따라 시스템이 관리하는 포트 중 하나의 포트번호를 사용하게 된다.) 그런데 만약 A 프로세스의 Socket이 사용하는 Port 번호와, C 프로세스의 Socket이 사용하는 Port 번호가 같게된다면 어떻게 될까? 데이터가 도착했는데 A 프로세스로 보내줘야할지, C 프로세스로 보내줘야 할 지 혼란이 오게 될 것이다. 그렇기 때문에 Server Socket이 고유한 Port 번호를 만들 수 있도록 Socket과 Port 번호를 결합(Bind)해주는 작업이 꼭 필요하다. 근데, 여기서 궁금한 점! 그럼 하나의 프로세스는 Socket, 즉 연결 창구를 하나만 연결할 수 있을까? 그렇지 않다. 하나의 프로세스는 같은 Port를 가진 Socket을 여러개 열 수 있다. 호스트 A가 하나의 Port로 여러 개의 Socket을 만들어 다른 호스트들과 데이터를 주고받을 수 있다. 이러한 이유 덕분에 우리가 하나의 채팅 앱을 사용하더라도 동시에 많은 사람들과 채팅을 주고받을 수 있는 것이라고 생각한다. '그루밍'이라는 Port가 Socket을 1, 2, 3, 4 ... 개 열어서 Socket1로는 피터, Socket2로는 포츈, Socket3으로는 와일더 .. 이렇게 대화할 수 있게 되는 것이다. 3. Client 연결 요청 대기 (listen) - Server Socket에서 Port번호와 바인딩 작업을 마치고 나면, 드디어! Client로부터의 연결 요청을 받아들일 준비가 되었다. - Client가 연결 요청을 할 때까지 계속 기다리고 있는다. (이런 역할을 Listen 한다고 한다. 나 귀기울이고 있어~~!!) - 이렇게 대기상태를 계속 지속하다가, 연결 요청이 오면 대기 상태를 종료하고 리턴한다. (아직 연결이 된 건 아니다!) 4. Client 연결 수립 (accept) - listen은 정말 기다리고 있다가 들리는 게 있으면 어 요청이 들린다! 하고 종료! 리턴! 하는게 끝이고, 실질적인 연결은 여기에서 된다. 도데체 왜일까? accept 단계에서는 최종적으로 연결 요청을 받아들이고, Socket 간의 연결을 수립한다. 여기까지 들으면, 충분히 listen 단계에서도 할 수 있을거라 생각이 드는데, 중요한 이유가 있다. accept 단계에서는 최종 요청을 받아들이면서, 새로운 Socket을 생성한다! 뚜둔! 당연히 Socket 간 연결이라고 했을 때, 당연히 Client - Server 간의 연결이겠찌?!! 했는데, 아니다. Server Socket의 메인 역할은 Client의 연결 요청을 받는 것이었다! 그래서 계속 Client가 오기를 오매불망 기다렸던 것! 연결 요청을 받고 accept를 통해 새로 생성된 Socket과 Client Socket을 매핑해주면 Server Socket의 역할은 끝이 난다. (그렇기 때문에 Client Socket에 대한 내용을 Server Socket은 단순히 받아서 새로 생기는 Socket에게 넘겨주기만 한다.) 역할이 끝나면 Server Socket은 다시 다른 연결 요청을 처리하기 위해 대기(listen)하거나, Server Socket을 닫는다(close). 5. 데이터의 송수신 (Send, Recieve) - 이 또한 Client Socket과 같다. 하지만 또 써준다. 연결 요청처럼, 요청을 보낸다고 끝나는 것이 아니고 그 요청에 대한 결과가 들어와야 실행이 끝난다. - 하지만 송수신 간의 차이점은 존재한다. - 송신(Send)할 때에는 데이터를 보내는 것이기 때문에 데이터를 언제, 얼마나 보낼 것인지를 알 수 있다. - 수신(Recieve)할 때에는 상대방이 언제, 얼만큼의 데이터를 보낼 것인지 알 수가 없다. - 그렇기 때문에 수신하는 API는 별도의 Thread에서 진행하게 된다. 6. Socket 닫기 - 더 이상의 데이터 송수신이 필요없어지게되면, 소켓을 닫는다. - 그런데 여기에서 중요한 점은, Server Socket은 자기 자신만 관리하는 게 아니라, Client와의 연결 수립에서 생성된 새로운 Socket에 대해서도 관리를 해주어야 한다는 거다. 너가 만들었으니까 너가 관리햇! 이렇게 Client Socket, Server Socket의 역할을 통해 Socket Programming에 대해 알아보았다. . . 이제 드디어 Socket에 대해 좀 이해가 갔다. . . 험난한 여정이었따 . . 따흑 . . . 그러면 . . . 도데체 . . . Web Socket은 뭘까?! 🕸🔌 드디어, WebSocket! 그러면 WebSocket은 뭘까? WebSocket은 Web에서 두 프로그램 간의 메시지를 교환하기 위한 통신 방법 중 하나이다. 뒤에 Socket이 달려있어서 위에 나온 Socket과 같은거 아니야?! 라고 생각할 수 있지만, 두 개념은 엄연히 다르다. WebSocket은 위의 Socket과는 IP, Port 통신을 한다는 점에서는 비슷하지만, 분명한 차이가 있다. 간단히 설명해보겠다. 웹 브라우저는 HTTP Protocol을 사용한다. 즉, 요청을 보내면 응답이 오는 단방향적 구조로 통신한다. 그렇기 때문에 TCP/IP(데이터 송수신) Protocol을 사용하는 Socket처럼 계속 connection이 유지되는 실시간 통신을 할 수가 없다. 이런 실시간 통신의 문제점을 해결하기 위해 나온 것이 WebSocket Protocol이다. Web Socket이 나오기 전에도 HTTP Protocol 방식으로도 실시간 통신인 척 하게 만들 수는 있었지만, 사용하기에 한계가 많아 WebSocket이 등장하게 되었다. (이와 관련한 내용은 아래에 따로 적겠다.) WebSocket을 사용하면, 웹 브라우저에서도 Socket 통신을 하는 것처럼 실시간으로 데이터를 주고받을 수 있다. 최근에는 대부분의 브라우저가 WebSocket 프로토콜을 지원하지만, IE의 경우는 10 이상의 버전부터 지원한다. 웹소켓 지원 브라우저 / 출처: https://caniuse.com/websockets 🏗 WebSocket의 동작 방법 그러면, 이제 WebSocket의 동작 방법을 한 번 알아보자. 1. HandShaking 🤝 우리 연결하자! 죠아! 악수! 땅땅! Browser와 Server의 연결을 시작하게 해주는 단계를 Handshake 단계라고 한다. 최초로 접속할 때에 HTTP Protocol 위에서 HandShaking을 하기 때문에, HTTP Header를 사용한다. (좌) https://ko.javascript.info/websocket / (우) google.. WebSocket을 호출해 Socket을 생성하면 즉시 연결이 시작되고, 이러한 연결이 유지되는 동안, 브라우저는 Header를 통해 Server에 WebSocket을 지원하는지 물어본다. 이때 서버가 맞다는 응답을 하면, 그때부터 HTTP Protocol 대신 WebSocket Protocol로 통신된다. 어떻게 통신하는지 보다 자세히 살펴보자. WebSocket은 위에 언급했다시피 HTTP와는 다르지만, HTTP Protocol과 호환된다. 이러한 호환성을 달성하기 위해 WebSocket handshake는 HTTP Upgrade header를 사용해 HTTP Protocol 내의 WebSocket Protocol로 변경된다. 어떻게 변경될까? 아래를 살펴보자. 아래는 Babble에서 채팅을 연결할 때 사용하는 request header이다. Protocol을 바꾸고 싶다는 신호를 아래와 같이 보내고있다. Connection: Upgrade - Client 측에서 Protocol을 Upgrade(바꾸자!)하고 싶다는 신호를 보냈다는 걸 뜻함 Upgrade: websocket - 어떤 걸로 Upgrade 시키고 싶은가? websocket으로 바꾸고 싶다는 것! 요 두가지를 꼭 작성해줘야, WebSocket으로 통신하자는 요청을 보낼 수 있다. Sec-WebSocket-Key는 브라우저에서 보안을 위해 랜덤하게 생성한 Key를 나타내는데, Server는 이 Key를 바탕으로 Token을 생성한 후, 브라우저에게 돌려준다. Client에서 이렇게 요청하면, 이에 대한 응답으로 handshake 응답을 받는다. 이 때, 서버가 Protocol 전환을 승인하면 🟢 응답코드 101이 들어오게 된다. 그렇게 데이터 통신이 시작된다. - 한 줄 요약 WebSocket을 사용하겠다는 HTTP 요청을 하고 OK 승인을 받으면 통신 규약을 HTTP에서 websocket으로 upgrade시키고, socket을 뚫어 그 socket으로 데이터를 주고받을 수 있게 한다는 것이다. 헤헷 2. Data Transfer - 데이터 전송 🗂 1번의 Handshake를 통해 WebSocket 연결이 되면, 이제부터 데이터를 송수신 할 수 있다. Client와 Server가 메시지로 데이터를 주고받는데, 이 메시지는 Frame이라는 한 개 이상의 데이터 조각으로 구성되어 있다. 이 Frame은 Server, Client 양 측에서 모두 보낼 수 있는데, 종류가 다양하다. 텍스트가 담긴 텍스트 프레임, 바이너리 데이터가 담긴 바이너리 데이터 프레임, 커넥션이 유지되는지 확인하기 위한 핑/퐁 프레임.. 등이 있다. 일단 우리는 WebSocket을 사용할 것이기 때문에 WebSocket에서 사용하는 칭구들만 알고있으면 된다. 하하! WebSocket의 send() 메서드는 텍스트나 바이너리 데이터만 보낼 수 있기에 브라우저 환경에서 개발자는 텍스트 프레임이나 바이너리 프레임만 다루게 된다. 3. Close Handshake ✋ 🤚 데이터의 송수신이 완료되면, Client와 Server 모두 Connection을 종료하기 위한 Frame을 전송할 수 있다. Connection을 종료하고 싶은 쪽에서 Connection을 종료한다는 Frame을 보내면, 상대쪽에서 응답으로 Close Frame을 전송한다. 이렇게 WebSocket 연결이 종료된다. 🚨 But.. WebSocket의 한계 이렇게 뭔가 완벽(?)해보이는 WebSocket에도 한계는 분명히 존재한다. 1. 일단 기존의 HTTP로 구현된 Polling, Long Polling, Streaming 방식에 비해 구현이 복잡하다. 2. HTML5 이후에 나왔기 때문에, HTML5 이전의 기술에는 적용이 어렵다. 즉, WebSocket을 지원하지 않는 브라우저가 존재한다. 3. Stateless한 HTTP와는 달리, WebSocket은 Stateful한 Protocol이므로 연결을 항상 유지해야 한다. 이러한 연결 자체도 비용일 뿐더러, 항상 연결이 정상적으로 동작한다는 보장이 없기 때문에 비정상적으로 연결이 끊기게 되는 상황에 대한 대응책이 필요하다. ※ Stateless - Server Side에 Client와 Server의 동작이나 상태 정보를 저장하지 않는 형태. Client의 Session 상태와 관계없이 요청에 대한 응답만을 수행한다. 단일 요청에 대해 하나의 응답만이 나온다. ※ Stateful - Server Side에 Client와 Server의 동작이나 상태 정보를 저장하는 형태. Client의 Session 상태에 따라 Server의 응답이 달라진다. 현재의 상태가 이전의 상태에 영향을 받는다. 💪 한계를 극뽁해보자! WebSocket이 가진 한계 중에, ""HTML5 이전의 기술에 적용이 어렵다"" 는 문제에 대한 해결책으로 나온 몇가지 방법이 있다. 1. Socket.io https://socket.io/ Socket.IO SOCKET.IO 4.0 IS HERE ~/Projects/tweets/index.js const io = require('socket.io')(80); const cfg = require('./config.js socket.io https://github.com/socketio/socket.io GitHub - socketio/socket.io: Realtime application framework (Node.JS server) Realtime application framework (Node.JS server). Contribute to socketio/socket.io development by creating an account on GitHub. github.com Socket.io는 Node.js 기반으로 만들어진 기술로 Javascript를 이용해 브라우저의 종류에 상관없이 실시간 통신을 구현할 수 있도록 한 라이브러리이다. 브라우저가 WebSocket을 지원하면 그대로 WebSocket 방식으로 동작하고, 지원하지 않는 브라우저면 HTTP Protocol을 이용해 실시간 통신을 흉내낸다. Node.js를 기반으로 만들어진 기술이기 때문에, Server는 다른 대안 없이 Node.js를 사용해야 한다. 개인들이 만들어 놓은 몇가지 방법이 있다고는 하지만.. 완전한 방법은 아니므로 추천하지는 않는다. 반면, Client는 Socket.io에서 Javascript 뿐만 아니라 Java, C++ 등에서 사용할 수 있도록 지원해주고 있다. 2. Sock.js sockjs.org GitHub - sockjs/sockjs-client: WebSocket emulation - Javascript client WebSocket emulation - Javascript client. Contribute to sockjs/sockjs-client development by creating an account on GitHub. github.com SockJS도 Socket.io와 마찬가지로 브라우저의 종류와 관계없이 실시간 통신을 구현할 수 있도록 한 라이브러리이다. 마찬가지로 브라우저가 WebSocket을 지원하면 우선적으로 WebSocket 방식으로 동작하고, 사용할 수 없는 경우에는 다양한 브라우저 별 전송 프로토콜을 사용해 WebSocket처럼 동작한다. 여기까지 보면, Socket.io와 크게 다르지 않아 보이지만, 몇가지 다른 점이 존재한다. 먼저, Spring Framework 자체에서 WebSocket을 사용할 때 구형 브라우저에 대한 대비책으로 SockJS Protocol을 사용하고 있다. https://docs.spring.io/spring-framework/docs/4.1.7.RELEASE/spring-framework-reference/html/websocket.html SockJS Protocol은 브라우저 내의 SockJS-client와 SockJS-server 사이에 통신을 하기 위해 만들어진 규약이다. (잠깐 Protocol의 정의에 대해 짚고 넘어가자면, 아래와 같다. Protocol의 정의는 컴퓨터 간의 데이터 통신을 하기 위한 통신 규약이다. google.com 즉, Protocol은 컴퓨터 간 통신을 하기 위한 약속인거다.) 이렇듯 Spring Framework에서의 WebSocket이 SockJS Protocol을 준수하고 있기에, Backend에서 Spring Framework를 사용하고 있다면 Frontend에서도 자연스럽게 SockJS-client를 사용하는 것이 자연스럽다. Babble 팀도 현재 Backend에서 Spring Framework를 사용하고 있기에, Frontend에서도 자연스럽게 SockJS-client를 사용해 구현하게 되었다. 💌 Text 기반 메시징 Protocol, Stomp Stomp는 Simple Text Oriented Messaging Protocol의 약자로, 간단한 Text 중심의 메시징 Protocol이다. 앞서 우리는 WebSocket을 통해 메시지(형식: Text/Binary)를 주고받을 수 있다는 것을 알았다. 하지만 WebSocket 자체는 양방향 데이터 전송을 위한 프로토콜일 뿐, WebSocket만을 이용해 채팅을 구현하게 되면 해당 메시지가 어떤 요청인지, 어떤 포맷으로 오는지, 메시지 통신 과정을 어떻게 처리해야 하는 지 정해져 있지 않아 일일히 정해주어야한다. 이러한 불편함을 줄여주고자, 메시지 전송을 효율적으로 만들어주는 Stomp가 나오게 되었다. Stomp는 한 마디로, WebSocket 상에서 동작하며 Client와 Server가 서로 통신하는 데 있어서 메시지의 형식, 유형, 내용 등을 정의해주는 Protocol이라고 할 수 있다. Stomp를 사용하면 아래와 같은 장점이 있다. 1. WebSocket에서 메시지가 어떤 형식으로 사용될 지 지정해준다. 2. WebSocket의 Session 관리를 도와준다. 3. 발행과 구독이라는 개념을 도입해서, 우리가 어떻게 메시지를 통신할 건지에 대한 과정을 정해뒀다. WebSocket에는 발행이나 구독같은 개념이 따로 없기 때문에, 우리가 이런 상황을 예측하고 일일히 다 설계해놔야한다. 4. Stomp를 이용하면 메시지에 Header를 줄 수 있기 때문에 Header를 기반한 인증 처리가 가능하다. 5. Stomp의 규칙을 잘 지킨다면 여러 언어 및 플랫폼 간의 메시지를 처리할 수 있다는 장점이 있다. 그리고 Stomp version 4까지는 Stomp의 이름대로 텍스트 기반의 메시징만 지원했지만, Stomp version 5부터는 Binary 메시지 전송도 지원한다고 한다. 즉, 이미지나 파일에 대한 전송도 지원한다는 것. 77ㅑ륵! StompJS는 Client에서 이런 Stomp를 사용할 수 있도록 만들어놓은 라이브러리다. Frontend 단에서 이러한 Stomp protocol을 쉽게 사용할 수 있다. ⛳️ (추가) HTTP 방법으로 실시간 통신 구현하기 위에서 잠깐 언급했던, WebSocket이 나오기 이전에는 어떻게 이러한 실시간 통신을 처리했을까? 에 대한 내용이다. 아래 세가지의 방법은 결국 모두 HTTP를 통해 통신하기 때문에, 요청/응답 Header가 모두 불필요하게 크다는 단점이 있다. HTTP Polling Client가 서버로 1초에 한 번, 10초에 한 번.. 이런 식으로 주기적인 요청을 날려 새로운 이벤트가 있는 지 확인한다. Client와 Server 모두 구현이 쉽다는 장점이 있다. (유일한 장점인 듯..) 하지만 주기에 맞추어 요청을 날리는 것일 뿐이기 때문에 실시간성이 보장되지 않는다. 내가 메시지를 보내고 바로 답이 왔더라도, Client 요청 주기가 1분이라면 1분동안 응답을 받을 수 없다. 그 반대로 응답이 30분 간 오지 않더라도 Client가 계속 주기적으로 요청을 보내기 때문에 Server 측의 부담이 크다. 실시간성이 크게 중요하지 않다면, 괜찮은 방법이다. (근데 실시간성이 중요하지 않을리가?!!!!) HTTP Long Polling 이름처럼 Polling과 닮아있다. Client가 Server로 요청을 보내는 것은 동일하다. 하지만 이번에는 Client가 Server로 요청을 보내, 어떤 이벤트가 발생할 때까지 기다린다. 그리고 이벤트가 발생하면, 그 때 Client로 응답을 보낸다. 응답을 받은 Client는 바로 다시 요청을 보내, Connection을 연결시킨다. Polling에 비해서는 요청량이 줄어들어 Server의 부담이 줄어들지만, 만약 이벤트가 많이 발생한다면 결국 주기적으로 요청을 보내는 Polling 방법과 큰 차이가 없게 된다. 또, 다수의 Client에게서 동시에 이벤트가 발생하게 되면, 다수의 Client가 Server에 접속을 시도하기 때문에 결국 이 또한 Server의 부담이 커진다. 예를들어, 갑자기 동시간대에 1000명의 유저가 막 채팅을 한다고 가정해보자. Server에 1000개의 요청이 막 가게 되면서 Server에 엄청난 부하가 오게 될 것이다. HTTP Streaming Streaming 방식은, Client가 Server로 요청을 보내 Connection을 맺은 후, 이벤트가 발생해도 Server는 이 Connection을 끊지 않는다. 뭔가 여기까지는 오오..! 그러면 실시간 통신이 되는건가..! 싶은데, 그건 아니다. Client에서 연결을 끊지 않고 유지하기에 Server는 계속해서 Client로 메시지를 보낼 수 있는 반면, Client는 Streaming을 하면서 TCP Port로 읽기와 쓰기를 동시에 할 수 없기 때문에 더 이상의 요청을 할 수 없다. Streaming 중 요청을 보내고 싶다면, TCP 이외의 다른 Port를 이용해야 한다는 문제점이 있다. 마치며 Socket..에 대해 이해하는 데 꽤나 오래 걸렸다. 하아악 최대한 내가 이해한대로 쉽게 풀어쓰려 하다보니.. 오래걸리기도 했고 정말 길고 긴 포스트가 되었다. <<ㅏ흑 Socket에 대해 찾아볼 때 정말 CS, Network에 대한 지식이 많이 필요한 것 같았다... 너무 전문 용어들이 많이 나오면 읽기 싫어지니까.. 뭉뚱그려 설명해 놓은 부분이 군데군데 있다. 그렇기 때문에.. 혹시라도 잘못된 부분이 있다면 언제든 피드백 주세요..!!!!!!!!! 하하하 https://mangkyu.tistory.com/48 [네트워크 프로그래밍] Http 프로그래밍과 Socket 프로그래밍 차이 일반적으로 단말기에서 필요로 하는 데이터들은 Server에서 관리합니다. 네트워크를 통해 서버로부터 데이터를 가져오기 위한 통신을 구현하기 위해서는 크게 Http 프로그래밍과 Socket 프로그래밍 mangkyu.tistory.com https://recipes4dev.tistory.com/153 소켓 프로그래밍. (Socket Programming) 1. 소켓(Socket) 만약 네트워크와 관련된 프로젝트를 진행하면서, 사용자(User)의 관점이 아닌, 개발자(Developer)의 관점에서 네트워크를 다뤄본 경험이 있다면, ""소켓(Socket)""이라는 용어가 아주 낯설 recipes4dev.tistory.com https://tyrionlife.tistory.com/781 [네트워크] 소켓 프로그래밍이란(Socket Programming) 1. 소켓 프로그래밍 개요 □ 소켓의 의미 사전적으로 '구멍', '연결', '콘센트' 등의 의미 프로그램이 네트워크에서 데이터를 송수신할 수 있도록, '네트워크 환경에 연결할 수 있게 만들어진 연결 tyrionlife.tistory.com https://velog.io/@guswns3371/WebSocket-and-Socket.IO WebSocket and Socket.IO webSocket 이전에는 실시간 통신을 위해 http request에 트릭을 사용하여 실시간인 것처럼 작동하게 하는 기술을 사용함http 프로토콜 자체가 클라이언트에서 서버로의 단방향 통신을 위해 만들어진 velog.io",NON_TECH
105,"앞서 읽으면 좋은 글 - @NotNull 어노테이션 예외처리 핸들링 summary lombok에서 지원하는 @NonNull 어노테이션을 통해 엔티티의 필드를 검증하던 중, javax.validation.constraints의 @NotNull 어노테이션도 엔티티에 붙여 사용할 수 있음을 알게 되었다. @NotNull과 @Column(nullable = false)의 차이 엔티티 필드의 null을 검증하기 위해서 대표적으로 사용되는 어노테이션이 @Column(nullable = false)이다. @NotNull과 어떤 차이가 있을까? 어떤걸 사용해야할까? 우선 @Column(nullable = false)을 사용할 때와 마찬가지로 @NotNull 역시 테이블 생성시 NOT NULL DDL이 입력된다. 이는 Hibernate가 @NotNull 어노테이션 역시 해석할 수 있기 때문이다. 만일 Hibernate가 해석하길 원하지 않는 경우 application.properties에 아래 옵션을 지정해주면 된다. # application.properties
spring.jpa.properties.hibernate.validator.apply_to_ddl=false 본격적인 차이로, @Column(nullable = false)는 JPA가 만든 엔티티의 필드 값이 null로 채워진 상태에서도 정상적으로 수행되다가 데이터베이스 쪽으로 SQL 쿼리가 도착한 순간에 테이블 컬럼의 NOT NULL 옵션에 의해 예외가 발생된다. 그러나 @NotNull 어노테이션은 데이터베이스 쪽으로 SQL 쿼리가 보내지기 전에, 정확히는 JPA가 만든 엔티티의 필드 값이 null로 채워지는 순간에 예외가 던져진다. 즉 @NotNull 어노테이션이 보다 더 빠른 단계에서 같은 예외를 검출하므로, 더 안전하다고 볼 수 있겠다. 단, JPA(Hibernate)를 이용한 Entity화 시킨게 아닌 객체라면 @NotNull 어노테이션을 해석하지 못한다. 이는 어노테이션 자체는 주석일뿐, 해석하는 주체에 따라 기능이 달라지는 어노테이션의 특성을 생각해보면 곧장 이해하기 수월하다. 그렇다면 @NotEmpty와 @NotBlank는? Hibernate 공식 문서에 따르면 아쉽게도 Hibernate가 별도의 DDL을 지원해주지 않는다. @NotBlank Checks that the annotated ... (생략) Hibernate metadata impact None @NotEmpty Checks whether the annotated element is not null nor empty Hibernate metadata impact None @NotNull Checks that the annotated value is not null Hibernate metadata impact Column(s) are not nullable 고로 @NotBlank와 @NotEmpty가 어울리는 상황에도 다른 방법을 이용해야 할 것 같다. References Hibernate @NotNull vs @Column(nullable = false) [JPA] nullable=false와 @NotNull 비교, Hibernate Validation - 긴 이별을 위한 짧은 편지 Hibernate Validator 7.0.1.Final - Jakarta Bean Validation Reference Implementation: Reference Guide https://hyeon9mak.github.io/not-null-vs-column-nullable-false/ @NotNull vs @Column(nullable = false) 앞서 읽으면 좋은 글 - @NotNull 어노테이션 예외처리 핸들링 hyeon9mak.github.io",BE
106,"summary lombok에서 지원하는 @NonNull 어노테이션을 통해 엔티티의 필드를 검증하던 중, @NonNull 어노테이션이 필드에 Null 값이 주입될 경우 NullPointerException이 던져지는 것을 발견했다. 프로젝트의 ControllerAdivce 구조상 RuntimeException을 한꺼번에 처리하고 있었기 때문에, RumtimeException을 상속한 NullPointerException 대신 custom exception이나 별도의 exception이 던져지길 원했다. (꼭 @NonNull이 아니어도 충분히 다른 이유에서 NullPointerException이 던져질 수도 있었다.) @RestControllerAdvice
public class BabbleAdvice {

@ExceptionHandler(Exception.class)
public ResponseEntity<ExceptionDto> unexpectedException(Exception e) {
return ResponseEntity.badRequest().body(new ExceptionDto(""unexpected exception""));
}

@ExceptionHandler(RuntimeException.class) // 여기에 걸려버린다!
public ResponseEntity<ExceptionDto> unexpectedRuntimeException(RuntimeException e) {
return ResponseEntity.badRequest().body(new ExceptionDto(""unexpected runtime exception""));
}

@ExceptionHandler(BabbleException.class)
public ResponseEntity<ExceptionDto> babbleException(BabbleException e) {
return ResponseEntity.status(e.status())
.body(new ExceptionDto(e.getMessage()));
}

@MessageExceptionHandler
public ResponseEntity<ExceptionDto> handleException(BabbleException e) {
return ResponseEntity.status(e.status())
.body(new ExceptionDto(e.getMessage()));
} 엔티티의 생성자에 검증 메서드를 직접 작성할까 고민하던 중, javax.validation.constraints의 @NotNull 어노테이션을 엔티티에도 붙여 사용할 수 있음을 알게 되었다. @NotNull 어노테이션은 MethodArgumentNotValidException와 같은 예외를 던지므로, 별도의 예외 핸들링 메서드를 구현해 문제를 해결하고자 했다. MethodArgumentNotValidException - DTO 예외 // Request DTO
@Getter
@AllArgsConstructor
@NoArgsConstructor
public class UserRequest {

@NotNull(message = ""[Request] 유저 이름은 Null 일 수 없습니다."")
private String name;
} // Controller
@PostMapping
public ResponseEntity<UserResponse> create(@Valid @RequestBody UserRequest userRequest) {
return ResponseEntity.ok(userService.save(userRequest));
} MethodArgumentNotValidException 예외는 주로 DTO 필드에 붙은 @NotNull 어노테이션과 컨트롤러 파라미터 앞에 붙은 @Valid 어노테이션을 통해 던져진다. @NotNull 어노테이션을 DTO 필드에 붙여두어도, 해당 DTO를 받아내는 메서드 파라미터 위치에 @Valid 어노테이션을 추가하지 않으면 검증이 동작하지 않는다. 우선 아래와 같은 핸들링 메서드를 작성해서 ""[Request] 유저 이름은 Null 일 수 없습니다."" 메세지가 출력되는지 테스트 해보았다. @ExceptionHandler(MethodArgumentNotValidException.class)
public ResponseEntity<ExceptionDto> methodArgumentValidException(MethodArgumentNotValidException exception) {
return ResponseEntity.badRequest().body(new ExceptionDto(exception.getMessage()));
} ""message"": ""Validation failed for argument [0] in public ...(생략)... : [Field error in object 'userRequest' on field 'nickname': rejected value [null]; ...(생략)... default message [nickname]]; default message [[Request] 유저 이름은 Null 일 수 없습니다.]] "" 도저히 알아 볼 수 없는 형태의 긴 문장이 출력된다. 수 많은 데이터들 중에서 ""[Request] 유저 이름은 Null 일 수 없습니다."" 메세지만 보고 싶은 경우 아래와 같은 파싱 작업이 필요하다. @ExceptionHandler(MethodArgumentNotValidException.class)
public ResponseEntity<List<ExceptionDto>> methodArgumentValidException(MethodArgumentNotValidException e) {
return ResponseEntity.badRequest().body(extractErrorMessages(e));
}

private List<ExceptionDto> extractErrorMessages(MethodArgumentNotValidException e) {
return e.getBindingResult()
.getAllErrors()
.stream()
.map(DefaultMessageSourceResolvable::getDefaultMessage)
.map(ExceptionDto::new)
.collect(Collectors.toList());
} {
""message"": ""[Request] 유저 이름은 Null 일 수 없습니다.""
} extractErrorMessages 파싱 메서드를 통해 보고 싶은 메세지만 예쁘게 뽑아낸 것을 볼 수 있다. 자세히 살펴보면 반환 값이 ExceptionDto에서 List<ExceptionDto>로 변화한 것을 알 수 있는데, 이는 @Valid 어노테이션을 통해 하나의 @NotNull 검증 예외만 잡아 내는 것이 아니라, 해당 DTO의 필드에 붙은 모든 검증 어노테이션의 예외를 한꺼번에 전달하는 용도로 파악된다. ConstraintViolationException - 엔티티 예외 엔티티 역시 같은 MethodArgumentNotValidException을 던질것이라 예상했지만, 별개로 ConstraintViolationException을 던지고 있었다. @Getter
@NoArgsConstructor(access = AccessLevel.PROTECTED)
@Entity
public class User {

@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@NotNull(message = ""[Entity] 유저 이름은 Null 일 수 없습니다."")
private String name;

...
} 엔티티의 필드에 붙은 @NotNull 어노테이션은 @Valid 어노테이션 없이 동작한다. JPA(Hibernate)가 @NotNull 어노테이션을 읽어 동작하기 때문이다. @ExceptionHandler(ConstraintViolationException.class)
public ResponseEntity<List<ExceptionDto>> constraintViolationException(ConstraintViolationException e) {
return ResponseEntity.badRequest().body(extractErrorMessages(e));
} ""message"": ""Validation failed ...(생략)... interpolatedMessage='[Entity] 유저 이름은 Null 일 수 없습니다.', ...(생략)... [Entity] 유저 이름은 Null 일 수 없습니다.'}
]"" 이번에도 역시 알아보기 힘든 문장이 출력된다. 파싱을 진행하자. @ExceptionHandler(ConstraintViolationException.class)
public ResponseEntity<List<ExceptionDto>> constraintViolationException(ConstraintViolationException e) {
return ResponseEntity.badRequest().body(extractErrorMessages(e));
}

private List<ExceptionDto> extractErrorMessages(ConstraintViolationException e) {
return e.getConstraintViolations()
.stream()
.map(ConstraintViolation::getMessage)
.map(ExceptionDto::new)
.collect(Collectors.toList());
} {
""message"": ""[Entity] 유저 이름은 Null 일 수 없습니다.""
} 엔티티의 @NotNull 역시 깔끔하게 에러 메세지를 얻어낼 수 있게 되었다! 엔티티의 @NotNull 어노테이션에 대해서는 @NotNull vs @Column(nullable = false) 글을 참고하자. 최종적으로 RumtimeException 핸들링 메서드에 포함되지 않는 별도의 검증 어노테이션으로 @NotNull을 사용할 수 있게 되었다. @RestControllerAdvice
public class BabbleAdvice {

@ExceptionHandler(Exception.class)
public ResponseEntity<ExceptionDto> unexpectedException(Exception e) {
return ResponseEntity.badRequest().body(new ExceptionDto(""unexpected exception""));
}

@ExceptionHandler(RuntimeException.class)
public ResponseEntity<ExceptionDto> unexpectedRuntimeException(RuntimeException e) {
return ResponseEntity.badRequest().body(new ExceptionDto(""unexpected runtime exception""));
}

@ExceptionHandler(BabbleException.class)
public ResponseEntity<ExceptionDto> babbleException(BabbleException e) {
return ResponseEntity.status(e.status())
.body(new ExceptionDto(e.getMessage()));
}

@MessageExceptionHandler
public ResponseEntity<ExceptionDto> handleException(BabbleException e) {
return ResponseEntity.status(e.status())
.body(new ExceptionDto(e.getMessage()));
}

@ExceptionHandler(MethodArgumentNotValidException.class)
public ResponseEntity<List<ExceptionDto>> methodArgumentValidException(MethodArgumentNotValidException e) {
return ResponseEntity.badRequest().body(extractErrorMessages(e));
}

private List<ExceptionDto> extractErrorMessages(MethodArgumentNotValidException e) {
return e.getBindingResult()
.getAllErrors()
.stream()
.map(DefaultMessageSourceResolvable::getDefaultMessage)
.map(ExceptionDto::new)
.collect(Collectors.toList());
}

@ExceptionHandler(ConstraintViolationException.class)
public ResponseEntity<List<ExceptionDto>> constraintViolationException(ConstraintViolationException e) {
return ResponseEntity.badRequest().body(extractErrorMessages(exception));
}

private List<ExceptionDto> extractErrorMessages(ConstraintViolationException e) {
return e.getConstraintViolations().stream()
.map(ConstraintViolation::getMessage)
.map(ExceptionDto::new)
.collect(Collectors.toList());
} References ParameterValidationException (Spring Shell Documentation ... https://hyeon9mak.github.io/not-null-annotation-exception-handling/ @NotNull 어노테이션 예외처리 핸들링 📓 summary lombok에서 지원하는 @NonNull 어노테이션을 통해 엔티티의 필드를 검증하던 중, @NonNull 어노테이션이 필드에 Null 값이 주입될 경우 NullPointerException이 던져지는 것을 발견했다. 프로젝트의 hyeon9mak.github.io",BE
110,"summary 스프링부트 서버 빌드와 소나큐브 서버 빌드를 하나의 workflow.yml 파일로 관리하던 중, 소나큐브 서버 빌드시 동작하는 jacoco가 스프링부트 서버 빌드에서 진행되는 테스트코드 결과물(부산물)을 필요로 한다는 걸 깨닫고 스프링부트 서버 빌드가 소나큐브 서버 빌드보다 먼저 실행되도록 needs 키워드를 통해 의존성을 부여했다. ...(생략)
jobs:
deploy-build:
runs-on: deploy

steps:
- uses: actions/checkout@v2
- name: Set up JDK 8
uses: actions/setup-java@v2
with:
java-version: '8'
distribution: 'adopt'

- name: gradlew 권한 변경
working-directory: ./back/babble
run: chmod +x gradlew

- name: 빌드 진행
...(생략)

sonarqube-build:
runs-on: deploy
needs: deploy-build # <- needs 키워드

steps:
- uses: actions/checkout@v2
- name: Set up JDK 11
uses: actions/setup-java@v2
with:
java-version: '11'
distribution: 'adopt'

- name: gradlew 권한 변경
working-directory: ./back/babble
run: chmod +x gradlew

- name: 소나큐브 빌드 진행
...(생략) 그렇게 PR을 작성했는데, 포츈이 gradlew 파일의 권한을 매 번 바꾸는 것에 대해 의문을 표했다. 작업 단위인 job 간에는 서로 독립된 환경을 이룰 것이라고 생각하고 있었는데, 포츈의 의견도 충분히 일리가 있었다. 때문에 직접 테스트를 진행해보게 되었다. 테스트 1번 - 파일 권한 변화가 영구적으로 유지되는가? 우선 파일 권한의 변화가 영구적으로 유지되는지 테스트가 필요했다. 이를 위해서 파일 권한을 변경하는 action 동작을 총 2회 수행시켰다. 테스트 1번 - 1차 실행 jobs:
build:

runs-on: ubuntu-latest

steps:
- uses: actions/checkout@v2
- name: Set up JDK 11
uses: actions/setup-java@v2
with:
java-version: '11'
distribution: 'adopt'

- name: graldew 파일 권한 확인
run: ls -l gradlew
- name: graldew 파일 권한 수정
run: chmod 777 gradlew
- name: graldew 파일 권한 확인
run: ls -l gradlew 1차 실행을 통해 파일 권한의 기본 값이 -rwxr-xr-x(755) 인 것을 알 수 있다. 1차 실행에서 파일의 권한을 -rwxrwxrwx(777)로 변경해두었다. 2차 실행시 기대 값은 -rwxrwxrwx(777)에서 -rw-r--r--(644)로 변경되는 것이다. 테스트 1번 - 2차 실행 jobs:
build:

runs-on: ubuntu-latest

steps:
- uses: actions/checkout@v2
- name: Set up JDK 11
uses: actions/setup-java@v2
with:
java-version: '11'
distribution: 'adopt'

- name: graldew 파일 권한 확인
run: ls -l gradlew
- name: graldew 파일 권한 수정
run: chmod 644 gradlew
- name: graldew 파일 권한 확인
run: ls -l gradlew 파일 권한이 -rwxrwxrwx(777)로 유지되지 않고 기본 값 -rwxr-xr-x(755)로 돌아와 있는 것을 볼 수 있었다. 테스트 1번 - 결론 파일 권한은 영구적으로 적용되지 않는다. 테스트 2번 - job 끼리는 서로 같은 환경에서 동작하는가? 테스트 1번을 통해 매 action 마다 환경이 초기화 된다는 것을 확인할 수 있었다. 그렇다면 하나의 action에서 job 단위로 구분하는 경우에도 초기화가 될까? 아니면 서로 같은 환경에서 데이터를 공유할까? 이번엔 2개의 job을 담아 action 동작을 총 1회 수행시켰다. jobs:
first-build:
runs-on: ubuntu-latest

steps:
- uses: actions/checkout@v2
- name: Set up JDK 11
uses: actions/setup-java@v2
with:
java-version: '11'
distribution: 'adopt'

- name: graldew 파일 권한 확인1
run: ls -l gradlew

- name: graldew 파일 권한 수정1
run: chmod 777 gradlew

- name: graldew 파일 권한 확인1
run: ls -l gradlew

second-build:
runs-on: ubuntu-latest
needs: first-build

steps:
- uses: actions/checkout@v2
- name: Set up JDK 11
uses: actions/setup-java@v2
with:
java-version: '11'
distribution: 'adopt'

- name: graldew 파일 권한 확인2
run: ls -l gradlew 테스트 2번 - 실행 first-build를 second-build의 needs로 의존성을 부여해서 순서를 명확히 강제하고 의존관계를 갖게 했음에도 first-build에서 변경시킨 파일권한 -rwxrwxrwx(777)이 유지 되지 않고 -rwxr-xr-x(755)로 초기화 된 걸 확인 가능했다. 테스트 2번 - 결론 job 간에도 파일 권한은 공유되지 않는다. 별개의 환경에서 동작한다. References 워크플로우 동작 사이 의존성 부여 - 2021-babble PR https://hyeon9mak.github.io/independent-between-github-actions-jobs/ Github-actions 작업간 독립성 🐹 summary 스프링부트 서버 빌드와 소나큐브 서버 빌드를 하나의 workflow.yml 파일로 관리하던 중, 소나큐브 서버 빌드시 동작하는 jacoco가 스프링부트 서버 빌드에서 진행되는 테스트코드 결과물( hyeon9mak.github.io",NON_TECH
114,"이 글에서는 현재 최신버전인 9.0버전이 아니라, LTS버전인 8.9버전을 사용했다! 소나큐브는 프로그래밍 언어에서 버그, 코드 악취, 보안 취약점을 발견할 목적으로 정적 코드 분석으로 자동 리뷰를 수행하기 위한 지속적인 코드 품질 검사용 오픈 소스 플랫폼이다. 중복 코드, 코딩 표준, 유닛 테스트, 코드 커버리지, 코드 복잡도, 주석, 버그 및 보안 취약점의 보고서를 제공한다. Summary 백엔드 정적 분석 리포트를 공유한다. CloudWatch logs 대시보드를 구성한다. 정적 분석 리포트를 공유한다는 요구사항이 등장했다. '정적 분석 리포트가 뭘까...' 궁금해하고 있을 때 나봄이 질문을 던져줬고, 구구께서 ""소나큐브를 적용해보면 된다."" 라는 답변을 주셨다. 그리하여 소나큐브 트러블 슈팅 대단원이 시작되었다. 소나큐브 동작구조 소나큐브 공식문서에 따르면 소나큐브는 크게 스캐너, 서버, DB로 구성되어 있다고 한다. 각각은 많은 리소스를 필요로 하기 때문에 별도의 호스트로 구성하길 추천하고 있으며, 그러는 와중에서도 서버와 DB간 네트워크 거리는 최대한 가깝게 구성할 것을 요구하고 있다. 최초 나는 설명을 읽고 '소나큐브를 사용하려면 3개의 프로그램을 설치해야하네?' 라는 생각을 했다. 그러나 실습을 진행하면서 오해임을 알게 되었는데, 실질적으로 소나큐브 측에서 제공하는 설치 프로그램은 서버뿐이며, 스캐너는 서버에서 제공한 token 값을 통해 빌드를 수행하는 '쉘 명령어'에 불과하고, DB는 그냥 일반적인 DBMS 프로그램을 요구하는 것이었다. 고로 스캐너는 이미 사용중이던 CI/CD 호스트에 포함시키면 되고, 소나큐브 서버와 DB만 구성하면 되었다. 서버와 DB마저도 하나의 호스트에 올려버렸는데, 포츈이 로컬에 직접 소나큐브를 설치하고 테스트 해본 결과 우리 팀 프로젝트의 규모 정도에선 굉장히 적은 리소스를 필요로 했기 때문에 (소나큐브 공식문서에서는 엔터프라이즈 급 프로젝트에 대해 리소스가 많이 필요함을 이야기한 듯 하다.) 하나의 호스트에 도커를 통해 서버와 DB를 분리하여 구성했다. 대략 위와 같은 구성이었던 것... 소나큐브 설치 - 서버, DB 소나큐브 공식문서에 따르면 소나큐브 서버를 설치하는 방법에는 압축(ZIP)파일을 해제하는 방법과 도커 이미지를 도커 허브로부터 받아와 사용하는 방법이 있었다. 우리 팀의 경우 도커를 이용할 계획이었기 때문에 압축 파일을 해제하는 방법은 간단한 로컬 테스트 정도로 지나쳤다. 아래는 소나큐브 공식문서에서 제공하는 docker-compose.yml 파일을 우리 팀 입맛대로 바꾼 내용이다. 주석을 통해 상세한 내용을 확인해보자. # yml 문법 3.7버전을 사용하겠다.
version: ""3.7""

# docker-compose.yml 파일을 통해 수행되길 원하는 작업들.
services:
# sonarqube 변수명으로 수행될 작업.
sonarqube:
# 컨테이너 이름을 지정하고자 할 때 사용.
# 사용하지 않을 시 도커가 임의의 이름을 지정한다.
container_name: [컨테이너 이름]
# 컨테이너에 올릴 이미지. 로컬 도커에 없을 경우 자동으로 도커 허브에서 pull.
# 공식 문서에서는 sonarqube:community 로 입력되어 있다.
# sonarqube:lts-community 로 바꿔서 lts 버전을 사용한다.
image: sonarqube:lts-community
# 하단의 db 변수명으로 수행될 작업과 의존관계를 맺는다.
depends_on:
- db
# DB와 연결을 위한 설정.
# 5432는 postgreSQL이 사용하는 포트번호.
environment:
SONAR_JDBC_URL: jdbc:postgresql://db:5432/[DB 테이블명]
SONAR_JDBC_USERNAME: [DB 아이디]
SONAR_JDBC_PASSWORD: [DB 패스워드]
# 로컬 디렉토리와 컨테이너의 디렉토리를 매핑해서 데이터에 영속성을 부여한다.
volumes:
- ./sonarqube/data:/opt/sonarqube/data
- ./sonarqube/extensions:/opt/sonarqube/extensions
- ./sonarqube/logs:/opt/sonarqube/logs
# 로컬 네트워크 요청 포트번호와 컨테이너가 사용하는 포트번호를 매핑한다.
ports:
- ""9000:9000""

# db 변수명으로 수행될 작업.
db:
# 컨테이너 이름을 지정하고자 할 때 사용.
# 사용하지 않을 시 도커가 임의의 이름을 지정한다.
container_name: [컨테이너 이름]
# 컨테이너에 올릴 이미지. 로컬 도커에 없을 경우 자동으로 도커 허브에서 pull.
image: postgres:12
# DB 생성에 필요한 설정.
environment:
POSTGRES_USER: [DB 아이디]
POSTGRES_PASSWORD: [DB 패스워드]
# 로컬 디렉토리와 컨테이너의 디렉토리를 매핑해서 데이터에 영속성을 부여한다.
volumes:
- ./postgresql:/var/lib/postgresql
- ./postgresql/data:/var/lib/postgresql/data 최초 9000포트 접근이 불가능했다. 보안그룹에 9000포트가 열려있지 않았기 때문이었다.8000:9000으로 매핑 후 사용해도 됐지만, 더 깔끔하게 매핑 하고 싶어 CU께 상황설명을 설명 드리니 포트를 개방해주셨다. 항상 감사합니다 CU! 🙇‍♂️ 위와 같이 docker-compose.yml 파일을 작성 후 곧장 도커 컨테이너를 올리면 에러를 마주친다. [1]: max virtual memory areas vm.max_map_count [65530] is too low,
increase to at least [262144] vm.max_map_count 값이 최소 262144는 되어야 소나큐브 서버가 동작할 수 있다. 이를 위한 설정을 조금 해주어야 한다. vm.max_map_count는 환경 변수. /proc/sys/vm/max_map_count 파일을 가리킨다. 이 파일은 Linux kernel의 메모리 맵 크기를 조절 할 수 있다. 메모리 매핑 작업이 많이 일어나는 프로세스일수록 가상 메모리에서의 주소 공간인 가상 주소 공간을 많이 필요로 하는데, Linux는 하나의 애플리케이션이 너무 많은 메모리를 매핑하는 것을 방지하기 위해 vm.max_map_count 값을 65530으로 제한하고 있는 것이다. # /etc/sysctl.conf 파일의 최하단
vm.max_map_count=262144 이 설정을 통해 vm.max_map_count 값이 영구적으로 증가하긴 하나, 시스템이 완전히 재부팅 된 후부터 적용된다. 재부팅을 하지 않고 곧바로 적용되길 원할 경우 아래 1회성 명령을 입력해주면 된다. $ sudo sysctl -w vm.max_map_count=262144 /etc/sysctl.conf 파일에 설정을 추가하지 않고 sysctl -w vm.max_map_count=262144 명령만 사용할 경우 재부팅 되면 설정이 날아간다. 설정 완료 후 도커에 올리면 잘 동작할 것이다. 소나큐브 설치 - 스캐너 소나큐브 스캐너 준비를 위해선 우선 소나큐브 서버 웹 콘솔에 접근해야한다. 소나큐브 서버와 데이터베이스를 구동시킨 상태에서 웹 브라우저를 통해 9000번 포트로 접근해보자. 곧바로 로그인을 요구할 것이다. 기본 계정명/비밀번호는 admin/admin이다. 로그인에 성공하면 곧바로 비밀번호 변경을 요구하니, 비밀번호를 변경해준다. 화면 1시 방향의 Add Project - Manually 메뉴로 접근한다. (community 버전을 사용 중이기 때문에 manually 외엔 선택이 불가능하다.) 프로젝트 생성을 시작하면 Project key 입력을 요구한다. 소나큐브의 Project key는 단순하게 프로젝트를 식별하는 고유한 이름이라고 생각하면 되겠다. 그 후엔 token을 발급 받는다. token의 명칭을 정하고 Generate 버튼을 눌러 token을 발급 받으면 된다. 발급된 token을 확인하고 Continue 버튼을 누른다. 빌드 옵션에 따라 선택을 진행한다. 우리 팀의 경우 Gradle을 사용 중이므로, Gradle 탭을 선택했다. Gradle탭을 선택하면 위 그림과 같이 build.gradle에 추가할 의존성 코드와 소나큐브 스캐너 동작을 위한 스크립트가 제공된다. 소나큐브 스캐너 동작을 위한 스크립트를 자세히 살펴보면 아래와 같다. ./gradlew sonarqube # build.gradle에 추가한 의존성을 통해 sonarqube 빌드 진행
-Dsonar.projectKey=[키이이이이] # 앞서 설정한 projectKey
-Dsonar.host.url=http://[유알엘엘엘엘엘]:9000 # 소나큐브 서버 URL
-Dsonar.login=[토크으으으으으은] # 앞서 발급 받은 token 앞서 소나큐브 서버 웹 콘솔을 통해 진행한 일련의 과정들은 모두 이 스크립트를 만들어내기 위함이었으며, 소나큐브 스캐너는 고작 이 스크립트를 통해 동작을 수행하게 된다. 프로젝트의 build.gradle 파일에 의존성을 추가하고, CI/CD 툴에 스크립트를 추가하자. 우리 팀의 경우 Github actions를 사용중이기 때문에, workflow 파일에 스크립트를 추가했다. name: Java CI with Gradle
...(생략)

jobs:
sonarqube-build:
runs-on: deploy

steps:
- uses: actions/checkout@v2
- name: Set up JDK 11
uses: actions/setup-java@v2
with:
java-version: '11'
distribution: 'adopt'

- name: gradlew 권한 변경
working-directory: ./back/babble
run: chmod +x gradlew

- name: 소나큐브 빌드 진행
working-directory: ./back/babble
run: ./gradlew sonarqube -Dsonar.projectKey=babble-sonarqube -Dsonar.host.url=${{ secrets.SONARQUBE_SERVER_URL }} -Dsonar.login=${{ secrets.SONARQUBE_TOKEN }}

build:
runs-on: deploy

steps:
- uses: actions/checkout@v2
- name: Set up JDK 8
uses: actions/setup-java@v2
with:
java-version: '8'
distribution: 'adopt'

- name: gradlew 권한 변경
working-directory: ./back/babble
run: chmod +x gradlew

- name: 빌드진행
...(생략) URL과 token 값은 Github secrets를 통해 숨겼다. 눈 여겨 볼 점은 jobs를 sonarqube-build(소나큐브 빌드)와 build(스프링 부트 프로젝트 빌드)로 구분하여 2가지 작업이 병렬적으로 진행되게 만들었다는 점과 각각의 빌드가 사용하는 자바 버전을 다르게 설정했다는 것이다. 우리 팀 프로젝트의 경우 Java8 버전을 사용한다. 반면 소나큐브의 경우 Java11 버전을 이용해야 한다. 서로 다른 버전의 JVM을 이용하기 때문에 작업을 나누어 진행하도록 설정했다. 설정이 완료되었다면 빌드를 진행해보자. projectKey, URL, token 소나큐브 서버 웹 콘솔을 통해 projectKey와 login(token)을 만들어내면서 'projectKey와 token을 조합해서 소나큐브 프로젝트를 식별하는구나!' 하고 생각할 수 있다. 그러나 실제 식별에 사용되는 조합은 URL과 token이다. projectKey는 그저 프로젝트를 구별시키는 이름일 뿐이다. 소나큐브 스캐너는 URL과 token을 조합하여 명령의 유효성을 판단한 다음, projectKey의 이름으로 프로젝트를 생성하여 소나큐브 서버로 분석 결과를 전달한다. 이 때 우선권을 가지는 것은 소나큐브 서버가 아니라 소나큐브 스캐너다. 표현이 모호한가? 예시를 보자. ./gradlew sonarqube 
-Dsonar.projectKey=babble_AAA 
-Dsonar.host.url=http://111.111.1.1:9000 
-Dsonar.login=abcd1234

./gradlew sonarqube 
-Dsonar.projectKey=babble_BBB 
-Dsonar.host.url=http://111.111.1.1:9000 
-Dsonar.login=abcd1234 URL과 token이 완전히 일치하는, projectKey 값만 다른 2개의 빌드 스크립트다. babble_AAA 스크립트는 소나큐브 서버 웹 콘솔을 통해 만들어진 스크립트이며(즉, 소나큐브 서버에서 존재를 알고 있음), babble_BBB 스크립트는 소나큐브 서버 웹 콘솔을 거치지 않고 곧장 작성한 스크립트다(소나큐브 서버에서 존재를 모르고 있음). 이 2가지 빌드 스크립트를 각각 동작시키면 어떤 결과가 일어날까? 우선 소나큐브 서버에서 존재를 인식하고 있던 babble_AAA는 이미 존재하는 프로젝트의 분석 결과가 업데이트 되는 형태를 가진다. 그리고 소나큐브 서버에서 존재를 인식하지 못했던 babble_BBB는 소나큐브 서버에 프로젝트가 자동으로 등록되며 분석 결과가 업데이트 되는 형태를 가진다. 즉, 소나큐브 서버는 전달 받은 분석결과들을 projectKey 이름으로 분리하고 정돈해서 보여줄 뿐 프로젝트를 직접 생성하는 등의 우선권을 가지고 있지 않다. 우선권을 가지는 것은 소나큐브 서버가 아니라 스캐너다. 후기 역시 공식문서가 정답이다. 공식 문서에서 제공하는 docker-compose 파일이 너무 잘 되어 있었다. 초반에 공식문서에 나온 성능 이야기를 보고 겁을 먹은 나머지 시간을 너무 낭비했다. 일단 들이박아보자. 별거 아니다. 인프라는 특히 더 그런거 같다. References SonarQube Documentation | SonarQube Docs Side effects when increasing vm.max_map_count https://hyeon9mak.github.io/install-sonarqube/ 소나큐브 설치하기 이 글에서는 현재 최신버전인 9.0버전이 아니라, LTS버전인 8.9버전을 사용했다! hyeon9mak.github.io",BE
126,"첫 번째 이슈 - Summary babble 프로젝트를 진행하면서, 상용 서버에서 릴리즈 테스트를 진행하지 말고 별도의 테스팅 서버를 만들어서 릴리즈 테스트를 진행하고자 했다. 상용 서버와 온전히 같은 환경을 갖추기 위해 EC2 인스턴스를 추가로 3~4개 개설하면서 테스팅 서버를 개설할까 고민해봤지만, 도커를 사용해서 각 계층을 독립시키면 EC2 인스턴스 1개만으로도 충분히 테스트가 가능할 것 같았다. (추후 인스턴스를 제거하기도 편하고) 결국 위 그림과 같은 형태로 도커 위에 3개의 컨테이너를 올렸다. web-server는 nginx로 구성하여 외부(public IP)로 부터 전달되는 요청을 바로 옆 WAS 컨테이너로 전달한다. 요청을 전달 받은 WAS는 로직을 수행하고, 영속성이 부여된 데이터가 필요하다면 바로 옆 Database 컨테이너를 찌른다. 이 때 컨테이너끼리 통신 시킬 방법이 필요했는데, 상용 서버와 최대한 비슷한 환경 구성을 위해 도커에서 제공하는 방법을 사용하지 않고 IP로만 요청을 주고 받아야 했다. 그러기 위해서 1차적으로 알아야 할 것은 도커 컨테이너의 IP 주소다. $ sudo docker inspect [컨테이너 ID] | grep IPAddress ""SecondaryIPAddresses"": null,
""IPAddress"": ""172.17.0.4"",
""IPAddress"": ""172.17.0.4"", inspect 명령을 통해 컨테이너 내부에서 사용되는 IP 주소를 알아낼 수 있다. 도커 컨테이너의 기본 IP 주소 범위는 172.17.0.* 이다. 결국 가장 마지막 번호를 알아내기 위함이다. 이렇게 알아낸 IP주소로 Web-server - WAS - DB 컨테이너를 서로 연결했고, 성공적으로 테스트가 가능했다. 첫 번째 이슈 - 원인 파악 그러나 며칠 후 CI/CD를 통해 WAS가 재배포 되면서 문제가 발생했다. WAS가 빌드 된 후 컨테이너로 도커에 오르자마자 종료되는 것이었다. 처음에는 단순히 WAS 빌드가 잘못 되었다고 판단했으나, WAS를 로컬에 띄워보니 아무런 문제가 없었다. 결국 한참을 헤매고 나서야 원인을 찾을 수 있었는데, 바로 데이터베이스의 유저계정을 생성할 때 할당했던 IP주소가 문제였다. CREATE USER [유저 이름]@'[IP주소]' IDENTIFIED BY '[비밀번호]'; 데이터베이스를 구축할 때 보안을 위해 [IP주소]에 정확하게 WAS 컨테이너의 IP주소를 기입했는데, WAS가 재빌드 된 후 기존 컨테이너를 버리고 새로운 컨테이너에 다시 올라오면서 IP주소가 변경된 것이다. 그러거나 말거나 데이터베이스는 기존 IP주소만 허용하고 있으니, WAS가 실행되는 단계에서 에러를 발생시키고, 컨테이너가 바로 다운 되었던 것이다. 첫 번째 이슈 - 해결 방법 현재는 데이터베이스에서 모든 IP 주소에서 들어오는 요청을 허용하는 것으로 해결하였으나, 도커 컨테이너를 올릴 때 원하는 IP를 직접 기입하는 방법으로도 해결 할 수 있다. # docker run 명령에 함께할 수 있는 옵션들

--add-host : 컨테이너 내부의 /etc/hosts에 호스트 이름과 IP 주소를 설정해준다. (없으면 임의의 ip adress 할당)

--dns : DNS 서버의 IP 주소를 설정해준다.

--expose : 포트 번호 할당

--mac-address : 컨테이너 MAC 주소 설정

--net : 컨테이너 네트워크 설정 (bridge, none, container:<name or d> , host)

-h, --hostname : 컨테이너 호스트 이름 설정

-P, --publish-all : 임의의 포트를 컨테이너에 할당

-p [호스트포트]:[컨테이너포트] : 호스트와 컨테이너 포트를 매핑시켜준다.

--link : 다른 컨테이너에서 접근시 이름을 설정 두 번째 이슈 - summary 어느 날 팀 프로젝트 테스트용 EC2 인스턴스의 WAS 프로그램이 비정상적으로 종료되어 있음을 확인했다. 종료된 시점은 Babble 팀 프로젝트에서 Spring 프로필 파일(yml)을 변경한 후였는데, 기존 사용중이던 application-local.yml 프로필에선 같은 서브넷 내에 있는 DB 도커 컨테이너를 조회하고 있었고, 새로이 바뀐 application-local.yml 프로필에선 H2 인메모리 DB를 조회하고 있었다. 이 때문에 새롭게 추가한 application-dev 프로필을 사용하도록 설정을 교체했는데, 테스트용 EC2 인스턴스 내부에서 DB 도커 컨테이너를 조작할 경우 DB 도커 컨테이너의 IP가 변경될 가능성이 다분했다. 도커 컨테이너의 IP가 변경될 때마다 application-dev.yml 파일에서 IP를 변경하는 것은 공수가 크므로, application-dev.yml 파일이 아닌 테스트용 EC2 인스턴스 내부에서 스크립트 파일로 데이터베이스 IP를 관리하고자 했다. # application-dev.yml

spring:
datasource:
# url: jdbc:mariadb://{IP}:3306/babble
driver-class-name: org.mariadb.jdbc.Driver
username: babble # EC2 내부 Dockerfile

FROM openjdk:8
COPY babble-0.0.1-SNAPSHOT.jar app.jar
ENTRYPOINT [""java"", ""-jar"", ""-Dspring.profiles.active=dev"",""-Dspring.datasource.url=jdbc:mariadb://{DB 컨테이너 IP}:3306/babble"" ...(기타설정)] EC2 내부 Dockerfile에 DB 컨테이너 IP를 기입하기 위해 DB 컨테이너 IP를 조회했다. docker inspect {DB 컨테이너} | grep IPAddress 명령을 통해 IP를 조회했을 때 172.21.0.2 IP를 확인할 수 있었다. '도커 컨테이너 IP 서브넷은 172.17.0.* 가 아니었나? 뭔가 달라졌나...' 하고 대수롭지 않게 넘기고 172.21.0.2 IP를 Dockerfile에 등록했다. 그러나 계속해서 Spring 애플리케이션 서버 빌드가 실패했다. 두 번째 이슈 - 해결 방법 이어서 루트가 바톤을 넘겨받아 트러블 슈팅을 진행했다. 도커를 사용하면 docker0 라는 네트워크 인터페이스가 생성되고, 도커에서는 이 네트워크 인터페이스에 bridge라는 네트워크 드라이버를 이용해서 접근한다. 각각의 컨테이너는 여러 개의 네트워크(서브넷)에 속할 수 있고, 각 서브넷에 맞는 IP를 따로 할당 받는다. 이 때 우리는 bridge 네트워크의 IP 주소를 확인했어야했는데, db_default라는 네트워크의 IP주소를 확인하고 있었다. ""brdige"": {
...(생략)
""IPAddress"": ""172.17.0.2"",
...(생략)
},
""db_default"": {
...(생략)
""IpAddress"": ""172.21.0.2"",
...(생략)
} 두 번째 이슈 - 느낀 점 얼마전에 스프링 프로필(yml)파일을 리팩토링하면서 재미본 부분이 많다보니 생각이 거기에만 갇혀있었던거 같다. 도커 네트워크를 얄팍하게 알고 있어서 '도커 IP 범위는 172.17.0.* 아닌가? 조금 다를수도 있나보다' 하고 넘어가버렸다. 얄팍하게 아는게 이렇게 위험하구나 느낀다. 모의면접 때 CU께서 ""도커 네트워크를 공부해보세요."" 하셨었는데, 그 때 도커 네트워크를 공부했다면 조금 더 수월하게 해결하지 않았을까 아쉬움이 생긴다. 두 번째 이슈 - 남는 의문 루트 덕분에 문제를 해결하고 ""docker inspect {DB 컨테이너} | grep IPAddress 명령은 단편적인 정보만 제공하기 때문에 docker inspect {DB 컨테이너}로 전체를 확인해야하는구나."" 라고 결론을 지었다. 그런데 테스트용 EC2 인스턴스에 다시 접속해서 docker inspect {DB 컨테이너} | grep IPAddress 명령을 사용하니 이전에 사용할 때와 달리 더 많은 정보를 포함하고 있었다. 분명히 bridge 네트워크의 IP 주소를 포함하지 않고 있었는데, 이번에는 포함되어 있었다. 루트가 별도로 설정을 만진게 있나 물었으나, 루트는 테스트를 위해서 커스텀 네트워크(서브넷)를 생성한거 뿐이라고 말했다. 즉, 커스텀 네트워크를 생성하기 전엔 bridge 네트워크의 IP 주소가 함께 보이지 않았으나, 커스텀 네트워크를 생성하고 나니 bridge 네트워크와 커스텀 네트워크의 IP 주소가 함께 포함되어 출력되고 있었다... 원인 파악을 위해선 도커 네트워크에 대한 공부가 더 필요할 것 같다. References docker run 과 docker 네트워크 설정, 컨테이너 라이프사이클 https://hyeon9mak.github.io/docker-container-ip-address/ 도커 컨테이너 IP 주소 트러블 슈팅 🐋 Summary babble 프로젝트를 진행하면서, 상용 서버에서 릴리즈 테스트를 진행하지 말고 별도의 테스팅 서버를 만들어서 릴리즈 테스트를 진행하고자 했다. hyeon9mak.github.io",BE
131,"첫 번째 이슈 - Summary babble 프로젝트를 진행하면서, 상용 서버에서 릴리즈 테스트를 진행하지 말고 별도의 테스팅 서버를 만들어서 릴리즈 테스트를 진행하고자 했다. 상용 서버와 온전히 같은 환경을 갖추기 위해 EC2 인스턴스를 추가로 3~4개 개설하면서 테스팅 서버를 개설할까 고민해봤지만, 도커를 사용해서 각 계층을 독립시키면 EC2 인스턴스 1개만으로도 충분히 테스트가 가능할 것 같았다. (추후 인스턴스를 제거하기도 편하고) 결국 위 그림과 같은 형태로 도커 위에 3개의 컨테이너를 올렸다. web-server는 nginx로 구성하여 외부(public IP)로 부터 전달되는 요청을 바로 옆 WAS 컨테이너로 전달한다. 요청을 전달 받은 WAS는 로직을 수행하고, 영속성이 부여된 데이터가 필요하다면 바로 옆 Database 컨테이너를 찌른다. 이 때 컨테이너끼리 통신 시킬 방법이 필요했는데, 상용 서버와 최대한 비슷한 환경 구성을 위해 도커에서 제공하는 방법을 사용하지 않고 IP로만 요청을 주고 받아야 했다. 그러기 위해서 1차적으로 알아야 할 것은 도커 컨테이너의 IP 주소다. $ sudo docker inspect [컨테이너 ID] | grep IPAddress ""SecondaryIPAddresses"": null,
""IPAddress"": ""172.17.0.4"",
""IPAddress"": ""172.17.0.4"", inspect 명령을 통해 컨테이너 내부에서 사용되는 IP 주소를 알아낼 수 있다. 도커 컨테이너의 기본 IP 주소 범위는 172.17.0.* 이다. 결국 가장 마지막 번호를 알아내기 위함이다. 이렇게 알아낸 IP주소로 Web-server - WAS - DB 컨테이너를 서로 연결했고, 성공적으로 테스트가 가능했다. 첫 번째 이슈 - 원인 파악 그러나 며칠 후 CI/CD를 통해 WAS가 재배포 되면서 문제가 발생했다. WAS가 빌드 된 후 컨테이너로 도커에 오르자마자 종료되는 것이었다. 처음에는 단순히 WAS 빌드가 잘못 되었다고 판단했으나, WAS를 로컬에 띄워보니 아무런 문제가 없었다. 결국 한참을 헤매고 나서야 원인을 찾을 수 있었는데, 바로 데이터베이스의 유저계정을 생성할 때 할당했던 IP주소가 문제였다. CREATE USER [유저 이름]@'[IP주소]' IDENTIFIED BY '[비밀번호]'; 데이터베이스를 구축할 때 보안을 위해 [IP주소]에 정확하게 WAS 컨테이너의 IP주소를 기입했는데, WAS가 재빌드 된 후 기존 컨테이너를 버리고 새로운 컨테이너에 다시 올라오면서 IP주소가 변경된 것이다. 그러거나 말거나 데이터베이스는 기존 IP주소만 허용하고 있으니, WAS가 실행되는 단계에서 에러를 발생시키고, 컨테이너가 바로 다운 되었던 것이다. 첫 번째 이슈 - 해결 방법 현재는 데이터베이스에서 모든 IP 주소에서 들어오는 요청을 허용하는 것으로 해결하였으나, 도커 컨테이너를 올릴 때 원하는 IP를 직접 기입하는 방법으로도 해결 할 수 있다. # docker run 명령에 함께할 수 있는 옵션들

--add-host : 컨테이너 내부의 /etc/hosts에 호스트 이름과 IP 주소를 설정해준다. (없으면 임의의 ip adress 할당)

--dns : DNS 서버의 IP 주소를 설정해준다.

--expose : 포트 번호 할당

--mac-address : 컨테이너 MAC 주소 설정

--net : 컨테이너 네트워크 설정 (bridge, none, container:<name or d> , host)

-h, --hostname : 컨테이너 호스트 이름 설정

-P, --publish-all : 임의의 포트를 컨테이너에 할당

-p [호스트포트]:[컨테이너포트] : 호스트와 컨테이너 포트를 매핑시켜준다.

--link : 다른 컨테이너에서 접근시 이름을 설정 두 번째 이슈 - summary 어느 날 팀 프로젝트 테스트용 EC2 인스턴스의 WAS 프로그램이 비정상적으로 종료되어 있음을 확인했다. 종료된 시점은 Babble 팀 프로젝트에서 Spring 프로필 파일(yml)을 변경한 후였는데, 기존 사용중이던 application-local.yml 프로필에선 같은 서브넷 내에 있는 DB 도커 컨테이너를 조회하고 있었고, 새로이 바뀐 application-local.yml 프로필에선 H2 인메모리 DB를 조회하고 있었다. 이 때문에 새롭게 추가한 application-dev 프로필을 사용하도록 설정을 교체했는데, 테스트용 EC2 인스턴스 내부에서 DB 도커 컨테이너를 조작할 경우 DB 도커 컨테이너의 IP가 변경될 가능성이 다분했다. 도커 컨테이너의 IP가 변경될 때마다 application-dev.yml 파일에서 IP를 변경하는 것은 공수가 크므로, application-dev.yml 파일이 아닌 테스트용 EC2 인스턴스 내부에서 스크립트 파일로 데이터베이스 IP를 관리하고자 했다. # application-dev.yml

spring:
datasource:
# url: jdbc:mariadb://{IP}:3306/babble
driver-class-name: org.mariadb.jdbc.Driver
username: babble # EC2 내부 Dockerfile

FROM openjdk:8
COPY babble-0.0.1-SNAPSHOT.jar app.jar
ENTRYPOINT [""java"", ""-jar"", ""-Dspring.profiles.active=dev"",""-Dspring.datasource.url=jdbc:mariadb://{DB 컨테이너 IP}:3306/babble"" ...(기타설정)] EC2 내부 Dockerfile에 DB 컨테이너 IP를 기입하기 위해 DB 컨테이너 IP를 조회했다. docker inspect {DB 컨테이너} | grep IPAddress 명령을 통해 IP를 조회했을 때 172.21.0.2 IP를 확인할 수 있었다. '도커 컨테이너 IP 서브넷은 172.17.0.* 가 아니었나? 뭔가 달라졌나...' 하고 대수롭지 않게 넘기고 172.21.0.2 IP를 Dockerfile에 등록했다. 그러나 계속해서 Spring 애플리케이션 서버 빌드가 실패했다. 두 번째 이슈 - 해결 방법 이어서 루트가 바톤을 넘겨받아 트러블 슈팅을 진행했다. 도커를 사용하면 docker0 라는 네트워크 인터페이스가 생성되고, 도커에서는 이 네트워크 인터페이스에 bridge라는 네트워크 드라이버를 이용해서 접근한다. 각각의 컨테이너는 여러 개의 네트워크(서브넷)에 속할 수 있고, 각 서브넷에 맞는 IP를 따로 할당 받는다. 이 때 우리는 bridge 네트워크의 IP 주소를 확인했어야했는데, db_default라는 네트워크의 IP주소를 확인하고 있었다. ""brdige"": {
...(생략)
""IPAddress"": ""172.17.0.2"",
...(생략)
},
""db_default"": {
...(생략)
""IpAddress"": ""172.21.0.2"",
...(생략)
} 두 번째 이슈 - 느낀 점 얼마전에 스프링 프로필(yml)파일을 리팩토링하면서 재미본 부분이 많다보니 생각이 거기에만 갇혀있었던거 같다. 도커 네트워크를 얄팍하게 알고 있어서 '도커 IP 범위는 172.17.0.* 아닌가? 조금 다를수도 있나보다' 하고 넘어가버렸다. 얄팍하게 아는게 이렇게 위험하구나 느낀다. 모의면접 때 CU께서 ""도커 네트워크를 공부해보세요."" 하셨었는데, 그 때 도커 네트워크를 공부했다면 조금 더 수월하게 해결하지 않았을까 아쉬움이 생긴다. 두 번째 이슈 - 남는 의문 루트 덕분에 문제를 해결하고 ""docker inspect {DB 컨테이너} | grep IPAddress 명령은 단편적인 정보만 제공하기 때문에 docker inspect {DB 컨테이너}로 전체를 확인해야하는구나."" 라고 결론을 지었다. 그런데 테스트용 EC2 인스턴스에 다시 접속해서 docker inspect {DB 컨테이너} | grep IPAddress 명령을 사용하니 이전에 사용할 때와 달리 더 많은 정보를 포함하고 있었다. 분명히 bridge 네트워크의 IP 주소를 포함하지 않고 있었는데, 이번에는 포함되어 있었다. 루트가 별도로 설정을 만진게 있나 물었으나, 루트는 테스트를 위해서 커스텀 네트워크(서브넷)를 생성한거 뿐이라고 말했다. 즉, 커스텀 네트워크를 생성하기 전엔 bridge 네트워크의 IP 주소가 함께 보이지 않았으나, 커스텀 네트워크를 생성하고 나니 bridge 네트워크와 커스텀 네트워크의 IP 주소가 함께 포함되어 출력되고 있었다... 원인 파악을 위해선 도커 네트워크에 대한 공부가 더 필요할 것 같다. References docker run 과 docker 네트워크 설정, 컨테이너 라이프사이클 https://hyeon9mak.github.io/docker-container-ip-address/ 도커 컨테이너 IP 주소 트러블 슈팅 🐋 Summary babble 프로젝트를 진행하면서, 상용 서버에서 릴리즈 테스트를 진행하지 말고 별도의 테스팅 서버를 만들어서 릴리즈 테스트를 진행하고자 했다. hyeon9mak.github.io",BE
136,"Spring WebSocket qucik Start-1 스프링을 이용해 웹소켓 서버를 구현해봅니다. Goal 웹소켓에 대한 개념 설명보다는 구현 자체에 집중해봅니다. 공식문서 가이드를 매우 참고하여, 한개의 방에서 입장, 퇴장 메시지를 전송하는 app을 구현해봅니다. 환경 IntelliJ 2021.2 java 8 gradle 7.1.1 초기설정 https://start.spring.io/ 에 들어갑니다. 위와 같이 선택한뒤 내려받습니다. 의존성 추가 implementation 'org.webjars:webjars-locator-core'
implementation 'org.webjars:sockjs-client:1.0.2'
implementation 'org.webjars:stomp-websocket:2.3.3'
implementation 'org.webjars:bootstrap:3.3.7'
implementation 'org.webjars:jquery:3.1.1-1' build.gradle 에 의존성들을 추가해줍니다. plugins {
id 'org.springframework.boot' version '2.5.3'
id 'io.spring.dependency-management' version '1.0.11.RELEASE'
id 'java'
}

group = 'com.example'
version = '0.0.1-SNAPSHOT'
sourceCompatibility = '1.8'

repositories {
mavenCentral()
}

dependencies {
implementation 'org.springframework.boot:spring-boot-starter-websocket'

implementation 'org.webjars:webjars-locator-core'
implementation 'org.webjars:sockjs-client:1.0.2'
implementation 'org.webjars:stomp-websocket:2.3.3'
implementation 'org.webjars:bootstrap:3.3.7'
implementation 'org.webjars:jquery:3.1.1-1'

testImplementation 'org.springframework.boot:spring-boot-starter-test'
}

test {
useJUnitPlatform()
} build.gradle 파일의 형태는 최종적으로 위와 같은 형태가 됩니다. 클라이언트와 서버가 입장, 퇴장 메시지를 주고받을 DTO를 만듭니다. public class EnterRequest {

private String userName;

public EnterRequest() {
}

public EnterRequest(final String userName) {
this.userName = userName;
}

public String getUserName() {
return userName;
}
}
public class EnterResponse {

private String userName;

public EnterResponse() {
}

public EnterResponse of(final EnterRequest enterRequest) {
return new EnterResponse(enterRequest.getUserName());
}

public EnterResponse(final String userName) {
this.userName = userName;
}

public String getUserName() {
return userName;
}
} 누가 입장했는지를 표시해주기 위한, 간단한 메시지 DTO를 만듭니다. 화면에 그려줄 클라이언트 코드 var stompClient = null;

function setConnected(connected) {
$(""#connect"").prop(""disabled"", connected);
$(""#disconnect"").prop(""disabled"", !connected);
if (connected) {
$(""#conversation"").show();
}
else {
$(""#conversation"").hide();
}
$(""#join"").html("""");
}

function connect() {
var socket = new SockJS('/ws');
stompClient = Stomp.over(socket);
stompClient.connect({}, function (frame) {
setConnected(true);
console.log('Connected: ' + frame);
stompClient.subscribe('/subscribe/join', function (join) {
showJoinMessage(JSON.parse(join.body).userName + ""님이 입장하셨습니다."");
});
});
}

function disconnect() {
if (stompClient !== null) {
stompClient.disconnect();
}
setConnected(false);
console.log(""Disconnected"");
}

function sendName() {
stompClient.send(""/app/join"", {}, JSON.stringify({'userName': $(""#name"").val()}));
}

function showJoinMessage(message) {
$(""#join"").append(""<tr><td>"" + message + ""</td></tr>"");
}

$(function () {
$(""form"").on('submit', function (e) {
e.preventDefault();
});
$( ""#connect"" ).click(function() { connect(); });
$( ""#disconnect"" ).click(function() { disconnect(); });
$( ""#send"" ).click(function() { sendName(); });
}); <!DOCTYPE html>
<html>
<head>
<title>Hello WebSocket</title>
<link href=""/webjars/bootstrap/css/bootstrap.min.css"" rel=""stylesheet"">
<link href=""/main.css"" rel=""stylesheet"">
<script src=""/webjars/jquery/jquery.min.js""></script>
<script src=""/webjars/sockjs-client/sockjs.min.js""></script>
<script src=""/webjars/stomp-websocket/stomp.min.js""></script>
<script src=""/app.js"" charset=""UTF-8""></script>
</head>
<body>
<noscript><h2 style=""color: #ff0000"">Seems your browser doesn't support Javascript! Websocket relies on Javascript being
enabled. Please enable
Javascript and reload this page!</h2></noscript>
<div id=""main-content"" class=""container"">
<div class=""row"">
<div class=""col-md-6"">
<form class=""form-inline"">
<div class=""form-group"">
<label for=""connect"">WebSocket connection:</label>
<button id=""connect"" class=""btn btn-default"" type=""submit"">Connect</button>
<button id=""disconnect"" class=""btn btn-default"" type=""submit"" disabled=""disabled"">Disconnect
</button>
</div>
</form>
</div>
<div class=""col-md-6"">
<form class=""form-inline"">
<div class=""form-group"">
<label for=""name"">What is your name?</label>
<input type=""text"" id=""name"" class=""form-control"" placeholder=""Your name here..."">
</div>
<button id=""send"" class=""btn btn-default"" type=""submit"">Send</button>
</form>
</div>
</div>
<div class=""row"">
<div class=""col-md-12"">
<table id=""conversation"" class=""table table-striped"">
<thead>
<tr>
<th>Chatting Room</th>
</tr>
</thead>
<tbody id=""join"">
</tbody>
</table>
</div>
</div>
</div>
</body>
</html> body {
background-color: #f5f5f5;
}

#main-content {
max-width: 940px;
padding: 2em 3em;
margin: 0 auto 20px;
background-color: #fff;
border: 1px solid #e5e5e5;
-webkit-border-radius: 5px;
-moz-border-radius: 5px;
border-radius: 5px;
} 공식문서의 코드와 거의 동일합니다. Server Code Controller 메시지를 받고, 전송하는 역할을 수행 import com.example.simplewebsocket.dto.JoinRequest;
import com.example.simplewebsocket.dto.JoinResponse;
import org.springframework.messaging.handler.annotation.MessageMapping;
import org.springframework.messaging.handler.annotation.SendTo;
import org.springframework.stereotype.Controller;

@Controller
public class JoinMessageController {

@MessageMapping(""/join"")
@SendTo(""/subscribe/join"")
public JoinResponse joinMessage(final JoinRequest joinRequest) {
return JoinResponse.of(joinRequest);
}
} @MessageMapping(""/join"") @GetMapping 처럼 /join 이라는 url를 해당 메소드로 매핑시킵니다. @SendTo(""subscribe/join"") subscribe/join 을 구독하고 있는 모든 클라이언트에게 리턴되는 값을 브로드 캐스팅 합니다. // Client
stompClient.subscribe('/subscribe/join', function (join) {
showJoinMessage(JSON.parse(join.body).userName + ""님이 입장하셨습니다."");
}); 위와 같이 subscribe/join 를 구독하고 있는 클라이언트들에게 메시지를 전부 보낼 수 있게 됩니다. Config 스프링단에서 WebSocket 을 활성화 하는 작업을 수행해 보겠습니다. import org.springframework.context.annotation.Configuration;
import org.springframework.messaging.simp.config.MessageBrokerRegistry;
import org.springframework.web.socket.config.annotation.EnableWebSocketMessageBroker;
import org.springframework.web.socket.config.annotation.StompEndpointRegistry;
import org.springframework.web.socket.config.annotation.WebSocketMessageBrokerConfigurer;

@Configuration
@EnableWebSocketMessageBroker
public class WebSocketConfig implements WebSocketMessageBrokerConfigurer {

@Override
public void registerStompEndpoints(final StompEndpointRegistry registry) {
registry.addEndpoint(""/ws"").withSockJS();
}

@Override
public void configureMessageBroker(final MessageBrokerRegistry config) {
config.enableSimpleBroker(""/subscribe"");
config.setApplicationDestinationPrefixes(""/app"");
}
} // client
function connect() {
var socket = new SockJS('/ws');
...
} // server
public void registerStompEndpoints(final StompEndpointRegistry registry) {
registry.addEndpoint(""/ws"").withSockJS();
} 엔드포인트가 어딘지를 설정하여, 최초 커넥션이 어디쪽으로 이루어져야 하는지를 설정해줍니다. 클라이언트 입장에서, /ws로 접근할시, 웹소켓 커넥션을 만들게 됩니다. 뒤의 withSockJS() 은 fallback 옵션입니다. 웹소켓을 사용하지 못하는 경우에 대체 전송방법을 찾아 전송을 수행할 수 있도록 해줍니다. @Override
public void configureMessageBroker(final MessageBrokerRegistry config) {
config.enableSimpleBroker(""/subscribe"");
config.setApplicationDestinationPrefixes(""/app"");
} 코드 설명에 앞서, 브로커라는 존재에 대해서 간단히 설명하고 지나가겠습니다. 브로커는 말 그대로 중개자의 역할을 수행한다고 생각하시면 편합니다. A가 1번방에 연결
B가 1번방에 연결
C가 1번방에 연결

이 상황일때
A가 1번방에 메시지를 전송한다면,

1번방을 중개하는 브로커가 그 메시지를 받아서, 1번방 구독자들에게 메시지를 전부 전송(브로드캐스팅) 하게 됩니다.
A (메시지)-> 1번방 브로커 -> 1번방 유저들 {A,B,C} enableSimpleBroker @Override
public void configureMessageBroker(final MessageBrokerRegistry config) {
config.enableSimpleBroker(""/subscribe"");
...
} enableSimpleBroker(/url) url에 해당하는 경로에 SimpleBroker룰 등록합니다. /url 접두사가 붙은 메시지가 있다면, 브로커는 해당 url을 구독한 클라이언트들에게 메시지를 전송(브로드캐스팅) 하게 됩니다. 아래의 예제처럼 /subscribe 라는 url 접두사를 구독하고 있는 클라이언트가 있다면, 브로커가 구독중인 클라이언트들을 찾아 메시지를 전달하게 해줍니다. (Sever to Client) stompClient.subscribe('/subscribe/join', function (join) {
showJoinMessage(JSON.parse(join.body).userName + ""님이 입장하셨습니다."");
}); 참고 : 스프링 문서에 따르면, 1:N 의 경우는 /topic, 1:1 의 경우에는 /queue 로 일반적으로 사용하나, 이해를 쉽기위해 /subscribe 라는 경로로 지정했습니다. The meaning of a destination is intentionally left opaque in the STOMP spec. It can be any string, and it is entirely up to STOMP servers to define the semantics and the syntax of the destinations that they support. It is very common, however, for destinations to be path-like strings where /topic/.. implies publish-subscribe (one-to-many) and /queue/ implies point-to-point (one-to-one) message exchanges. config.enableSimpleBroker(""/topic"", ""/queue""); 위처럼 여러 브로커를 등록할 수도 있습니다. setApplicationDestinationPrefixes setApplicationDestinationPrefixes(/url) /url 로 시작되는 메시지가 서버로 올경우, 메시지를 처리하는 메소드로 메시지를 라우팅하게 합니다.
// config.class
@Override
public void configureMessageBroker(final MessageBrokerRegistry config) {
...
config.setApplicationDestinationPrefixes(""/app"");
}

// controller.class
@Controller
public class JoinMessageController {
@MessageMapping(""/join"")
...
public JoinResponse joinMessage(final JoinRequest joinRequest) {
...
}
} /app/join 과 같은 형태의 url을 받게 된다면 /app 을보고 @MessageMapping 을 가지고 있는 메소드를 찾게 될것입니다. /app/** 는 @MessageMapping 어노테이션을 지닌 메소드를 찾습니다. 그 뒤의 /join 때문에, @MessageMapping(""/join"") 이 달려있는 메소드(joinMessage)로 최종적으로 메시지 전달이 되겠습니다. 결론 : @MessageMapping(""/join"") 은 app/join 를 지닌 url이 들어오게 됩니다. // client

function sendName() {
stompClient.send(""/app/join"", {}, JSON.stringify({'name': $(""#name"").val()}));
} 위와같이 클라이언트에서 보내는 메시지를 소화할 수 있게 되어집니다. 전체적인 패키지 구조는 아래와 같이 되겠습니다. 실행 서버를 구동시킨뒤 http://localhost:8080/ 에 접속합니다. connect를 눌러, 웹소켓 커넥션을 만듭니다. 다른쪽 클라이언트에서도 제대로 표시되는지 확인하기 위해서, 한개의 브라우저를 더 킨다음 똑같이 connect를 눌러, 웹소켓 커넥션을 만듭니다. 메시지를 보내보면, 다른쪽 클라이언트에서도 메시지가 잘 수신되는것을 확인할 수 있습니다. 아래쪽 클라이언트에서도 입장하는경우. 크롬 개발자 도구 F12 F12를 누르면, 어떤식으로 메시지를 주고받는지 확인이 가능합니다. Network -> WS 로 들어가면, 웹소켓이 커넥션이 한개 만들어져있는것을 볼 수 있고, 해당 커넥션을 통해 어떻게 어떤 메시지를 주고받았는지 확인할 수 도 있습니다. 다음 글은 지금처럼 한개의 방에서 입장, 퇴장 메시지만 전송하는것이 아닌, N개의 방에서 채팅을 주고받도록 하는법을 알아보겠습니다. 전체 코드 전체 코드는 해당 깃헙에서 볼 수 있습니다. Reference https://spring.io/guides/gs/messaging-stomp-websocket https://docs.spring.io/spring-framework/docs/current/reference/html/web.html#websocket https://docs.spring.io/spring-framework/docs/4.3.x/spring-framework-reference/html/websocket.html 본글 https://unluckyjung.github.io/spring/2021/08/19/Spring-Websocket-Implement-1/",BE
147,"Summary 최초 Github actions를 통해 프로젝트를 빌드하고 WAS EC2 인스턴스에 빌드된 파일을 전송하는 구조를 구상했다. 그러나 프로젝트 제약사항으로 등록되지 않은 외부 IP를 통해 AWS EC2 SSH 접근이 불가능하다가 있었고, Github actions 측에서 제공하는 서버로 빌드/배포 과정을 진행하는 방식으로는 완성된 빌드 파일을 보낼 수 있는 방법이 없었다. 코치님께서도 이 제약사항을 풀어줄 수 없다고 완강하게 말씀하셨다. 굳이 Github actions를 써야 겠다면, 몇가지 대안이 생각나긴 하네요. Github actions의 빌드 결과물을 도커로 빌드한후 도커 허브에 올린다. 서버에서는 이 이미지를 활용한다. github action를 self-hosted runner 방식으로 활용한다. 별도로 생성한 빌드서버(EC2, 혹은 로컬 서버)에서 운영서버로 ssh 연결이 가능하도록 구성한다. 어차피 WAS 프로그램도 도커 위에 구동시킬 것이라 1번 방법에도 메리트가 느껴졌으나, 얼마전 포츈이 Github actions 측에서 제공하는 서버로 빌드를 시도했을 때 알 수 없는 이유로 빌드가 수행되지 않아서(일시적으로 Github actions 측 서버가 멈추었던 것으로 추측) 2번 방법에 대한 메리트가 더 강하게 느껴졌다. 젠킨스 대비 메리트 2번 방법을 통해 EC2상에 빌드서버를 구성할 경우 젠킨스를 이용하는 것 대비 장점이 무엇일까 고민해보았다. 학습 곡선이 젠킨스에 비해 완만하다. 젠킨스의 설정, 플러그인 지옥 Github actions는 yml 파일만 잘 작성해주면 된다. EC2서버로 구성해도 익숙한 Github UI를 활용할 수 있다. 아직 프로젝트의 규모도 크지 않고 팀원들도 Github actions의 간단함과 UI를 마음에 들어했기 때문에 쉽게 결정을 내릴 수 있었다. 이번 기회에 EC2서버 Github actions를 구성하면 추후 젠킨스로 교체하기도 용이할 터였다. self-hosted actions runner 구성 위 도식화에서 빨간 박스로 강조된 부분만 다룰 것이다. (다른 부분들이 궁금할 경우 여기를 참고할 것) self-hosted 서버용 인스턴스 구성 github actions self-hosted 서버를 구성하기 위해 EC2 인스턴스를 생성하자. 이 때 인스턴스의 public IP는 활성화로 고정한다. self-hosted runner가 CI/CD 작업중 저장소의 Actions 탭으로 진행상황과 결과를 지속적으로 전달하고, 저장소 Actions 탭에서 CI/CD 작업을 직접 요청할 수도 있기 때문에 인터넷 연결을 필요로 한다. self-hosted runner 설치 및 연결 github actions를 통한 CI/CD가 진행되기 원하는 저장소에서 Settings - Actions - Runners - Add runner 메뉴로 접근한다. self-hosted runner가 동작할 운영환경을 설정하고 해당 운영환경의 터미널로 접속, actions runner 압축 파일을 다운로드 후 압축 해제해준다. 이어서 CI/CD가 진행되기 원하는 저장소 쪽에서 발급된 토큰 등록을 통해서 actions runner와 저장소간 연결을 진행한다. name of runner 설정은 Settings - Actions - Runners 메뉴에서 runner를 식별하기 위한 설정이다. 간단하게 Runner의 이름이라고 생각하면 된다. additional lables 설정은 Runner의 추가 라벨이다. 잠시 후 CI/CD 작업을 위한 yml 파일작성시 runner를 식별하기 위해 이름이 아닌 라벨을 사용하므로, 공들여서 라벨을 지정하자. 연결이 성공적으로 완료되었다면 ./run.sh 명령어 입력시 actions runner가 실행되면서 CI/CD 작업 요청을 기다리는 대기상태로 진입된 것을 확인할 수 있다. (nohup ./run.sh & 등의 명령어로 항시 runner가 동작하도록 설정하자.) CI/CD 작업 지정을 위한 yml 파일 작성 저장소에서 Actions 탭으로 접근하면 많은 yml 파일 예시(workflow)를 볼 수 있다. 저장소 프로젝트 성격에 맞는 workflow를 선택한다. name: Gradle Package

on:
release:
types: [created]

jobs:
build:

runs-on: ubuntu-latest # 이 부분에서 runner의 Label이 기입된다.
permissions:
contents: read
packages: write

steps:
- uses: actions/checkout@v2
- name: Set up JDK 11
uses: actions/setup-java@v2
... (생략) 공식 문서를 참고해서 프로젝트의 성격에 맞는 yml 내용을 작성하면 된다. 이 때 주의할 점은 runs-on 설정에 self-hosted runner에 기입했던 라벨을 기입해줘야 해당 runner를 식별하고 거기서 작업을 진행하게 된다. 그렇지 않을 경우 Github actions에서 제공하는 default runner가 작업을 수행한다. yml 파일 작성을 완료하고 커밋을 진행하면 yml 파일이 프로젝트의 /.github/workflows/*.yml 형태로 저장되게 된다. 지정한 on: 설정에 따라 자동으로 EC2에 설치된 actions runner가 작업을 받아들이고 수행하게 될 것이다. 수행되는 작업 내용은 Actions 탭에서 실시간으로 확인할 수 있다. References Github Actions의 Self-hosted runner 사용기 - Wayne's World About self-hosted runners - GitHub Docs https://hyeon9mak.github.io/github-actions-self-hosted-runner-on-ec2/ Github actions self-hosted runner를 EC2에서 동작시키기 hyeon9mak.github.io",BE
151,"summary ""영속성을 가져야할 클라이언 데이터를 도커 위에서 관리한다고?"" 라는 생각 때문에 데이터베이스만큼은 절대로 도커 위에 올려선 안되고, EC2 인스턴스 로컬에 설치해서 관리해야한다는 일종의 신념을 가지고 있었다. 그런데 얼마 전 다라쓰 팀이 디비를 도커 위에 올린 것을 보고 '왜 저런 선택을 했을까?' 생각하면서 도커 위의 데이터베이스가 어떻게 동작되는지를 찾아보았다. 도커 컨테이너에서 동작하는 데이터베이스는 클라이언트의 데이터까지 컨테이너 위에서 보관하는 것이 아니라, 볼륨을 이용해서 로컬에 클라이언트의 데이터를 저장하고, DBMS 관련 설정 값들만 컨테이너에서 관리하고 있었다. 이렇게 동작하는 경우 클라이언트 데이터의 영속성은 로컬 DB를 사용하는 것과 동일하게 지킬 수 있고, 오히려 DBMS 설정을 도커허브에 커밋하여 올리는 등 관리가 더욱 편리할 것으로 보였다. 팀원들과 논의결과 로컬 DB를 도커로 마이그레이션 하기로 결정되었는데, 기존 클라이언트 데이터를 최대한 보존 시켜서 마이그레이션을 진행하고자 했다. 백 그라운드로 동작중인 mariaDB 종료하기 mariaDB는 백 그라운드에서 mysqld 이름으로 동작하고 있다. maria, mariadb, db 등으로 검색할 것이 아니라, mysqld로 검색해야 한다. // 실패
$ pidof maria
$ pidof mariadb
$ pidof db

// 성공
$ pidof mysqld 동작 중인 mariadb(mysqld)를 종료하고 싶다면 service mysql stop 명령어를 사용한다. (kill 명령을 통해 프로세스를 강제 종료 시켜도 곧장 되살아 난다.) $ sudo service mysql stop mariaDB, mysqlDB의 데이터 저장 경로 별도의 데이터 저장경로를 지정하지 않았을 경우, default 저장경로는 /var/lib/mysql/ 이다. 해당 경로로 이동해보면 여러가지 로그파일과 schema 이름과 동일한 디렉토리를 발견할 수 있다. schema 이름과 동일한 디렉토리로 접근시 테이블들의 테이블명.frm 파일과 테이블명.ibd 파일을 발견할 수 있다. $ sudo ls -l babble/
total 700
-rw-rw---- 1 mysql mysql 61 Jul 9 05:50 db.opt
-rw-rw---- 1 mysql mysql 1715 Jul 17 03:52 game.frm
-rw-rw---- 1 mysql mysql 98304 Jul 17 03:52 game.ibd
-rw-rw---- 1 mysql mysql 1515 Jul 17 03:52 room.frm
-rw-rw---- 1 mysql mysql 114688 Jul 21 11:06 room.ibd
-rw-rw---- 1 mysql mysql 2730 Jul 17 03:52 room_tag.frm
-rw-rw---- 1 mysql mysql 131072 Jul 19 11:57 room_tag.ibd
-rw-rw---- 1 mysql mysql 2765 Jul 17 03:52 session.frm
-rw-rw---- 1 mysql mysql 131072 Jul 21 11:06 session.ibd
-rw-rw---- 1 mysql mysql 1686 Jul 17 03:52 tag.frm
-rw-rw---- 1 mysql mysql 98304 Jul 17 03:52 tag.ibd
-rw-rw---- 1 mysql mysql 2237 Jul 17 03:52 user.frm
-rw-rw---- 1 mysql mysql 114688 Jul 21 11:06 user.ibd 이 때 테이블명.frm 파일은 innoDB(maria, mysql이 사용하는 엔진)에서 테이블 구조를 다루는 파일이고, 테이블명.ibd 파일은 innoDB에서 테이블에 속하는 데이터를 다루는 파일들이다. 간단히 이야기해서 frm=테이블, ibd=데이터가 된다. mariaDB, mysqlDB DBMS 설정 저장 경로 별도의 설정이 없을 경우 default 설정 저장 경로는 /etc/mysql/conf.d다. 해당 디렉토리에 DBMS 설정 관련 파일이 존재한다. $ ll
total 16
drwxr-xr-x 2 root root 4096 Jul 9 02:30 ./
drwxr-xr-x 4 root root 4096 Jul 9 02:30 ../
-rw-r--r-- 1 root root 8 Aug 3 2016 mysql.cnf
-rw-r--r-- 1 root root 55 Aug 3 2016 mysqldump.cnf docker-compose.yml 파일 작성 # docker-compose.yml

version: '3.7' # yml 문법 버전 3.7을 사용하겠다.
services:
mariadb:
image: mariadb
restart: always
environment:
- MYSQL_ROOT_PASSWORD=[비밀번호]
volumes:
# [로컬 디렉토리]:[도커 컨테이너 내부 디렉토리] 매핑
- ./docker-volume/mariadb/mysql:/var/lib/mysql
- ./docker-volume/mariadb/conf.d:/etc/mysql/conf.d
ports:
- '3306:3306' #포트번호 매핑
environment:
TZ: Asia/Seoul # 타임존 기존 로컬 데이터베이스에서 사용중이던 2개의 디렉토리를 복제(원본 파일 안전을 위해)한 후 도커 컨테이너 내부 디렉토리에 매핑시켜줌을 명시한다. 이렇게 하면 기존 데이터를 모두 살린 채로 데이터베이스를 도커로 마이그레이션 할 수 있다. ./docker-volume/mariadb/mysql와 ./docker-volume/mariadb/conf.d의 경우 내가 별도로 만든 디렉토리 경로들이다. 입맛대로 디렉토리 경로를 지정하자. docker-compose.yml 파일이 존재하는 위치에서 도커 컨테이너를 올려보자. $ sudo docker-compose up -d --build
Creating [DB 컨테이너 이름] ... done # HTTP GET 요청 찔러본 결과
{
""roomId"": 3,
""createdDate"": ""2021-07-17T04:25:14.568"",
""game"": {
""id"": 3,
""name"": ""Apex Legend""
},
""tags"": [
""2시간"",
""솔로랭크"",
""실버""
]
} 한글 인코딩 설정도 함께 포함되어 있기 때문에 한글이 깨지지 않고 정상적으로 잘 반환됨을 확인할 수 있다! 그러나 실제 테이블에 값이 등록 되는지 확인하기 위해서 DBMS 툴로 접근 후 테이블을 조회 했을 때, 한글 인코딩이 전부 깨진 것을 볼 수 있었다. $ sudo docker exec -it [DB 컨테이너 이름] bash
$ root@8d577f8c631d:/# mysql -u root -p
Enter password: [설정해둔 패스워드]

MariaDB [(none)]> use babble;
MariaDB [babble]> select * from user;
+----+------+---------+
| id | name | room_id |
+----+------+---------+
| 1 | ?? | NULL |
| 2 | ??? | NULL |
| 3 | ??? | NULL |
| 4 | ?? | NULL |
| 5 | ??? | NULL |
| 6 | ?? | NULL |
+----+------+---------+

MariaDB [babble]> show variables like 'char%';
+--------------------------+----------------------------+
| Variable_name | Value |
+--------------------------+----------------------------+
| character_set_client | latin1 |
| character_set_connection | latin1 |
| character_set_database | utf8mb3 |
| character_set_filesystem | binary |
| character_set_results | latin1 |
| character_set_server | utf8mb4 |
| character_set_system | utf8mb3 |
| character_sets_dir | /usr/share/mysql/charsets/ |
+--------------------------+----------------------------+ DBMS 한글 인코딩 설정 web-server - was - db 를 거치는 HTTP GET 요청에는 정상적으로 한글 결과를 받을 수 있었으나, DBMS 툴로 직접 테이블을 조회한 경우에만 한글이 깨졌다. 즉, DBMS 자체도 한글 인코딩을 설정해주어야 한다. 도커 볼륨 매핑을 위해 복제했던 로컬 디렉토리 conf.d의 mysql.cnf 파일에 한글 인코딩 설정을 추가한다. # docker-volume/mariadb/conf.d/mysql.cnf

[mysqld]
skip-character-set-client-handshake
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci

[client]
default-character-set = utf8

[mysql]
default-character-set = utf8

[mysqldump]
default-character-set = utf8 이후 도커 컨테이너를 내린 뒤 다시 올려보자. $ sudo docker stop [DB 컨테이너 이름]
$ sudo docker rm [DB 컨테이너 이름]

$ sudo docker-compose up -d --build
Creating [DB 컨테이너 이름] ... done 도커의 데이터베이스에 다시 접근해서 테이블을 확인하자. MariaDB [babble]> select * from user;
+----+-----------+---------+
| id | name | room_id |
+----+-----------+---------+
| 1 | 루트 | NULL |
| 2 | 와일더 | NULL |
| 3 | 현구막 | NULL |
| 4 | 포츈 | NULL |
| 5 | 그루밍 | NULL |
| 6 | 피터 | NULL |
+----+-----------+---------+

MariaDB [babble]> show variables like 'char%';
+--------------------------+----------------------------+
| Variable_name | Value |
+--------------------------+----------------------------+
| character_set_client | utf8mb4 |
| character_set_connection | utf8mb4 |
| character_set_database | utf8mb3 |
| character_set_filesystem | binary |
| character_set_results | utf8mb4 |
| character_set_server | utf8mb4 |
| character_set_system | utf8mb3 |
| character_sets_dir | /usr/share/mysql/charsets/ |
+--------------------------+----------------------------+ 한글이 정상적으로 잘 출력됨을 확인할 수 있다. 결론 사실 데이터베이스 이전은 이번 기회에 처음 해본 것이었는데, 데이터가 잘 이전된 것을 보고 깜짝 놀랐다. (당연히 실패할거라 생각했는데!) 테이블(frm)과 데이터(ibd) 파일들을 통해 데이터의 영속성이 지켜진다면, 도커 위의 DBMS도 믿고 사용할 수 있을 것 같다. 도커는 가상환경에서 운영체제 설정까지 모두 신경써야하는 기존 방식에서 벗어나고자 등장했고, 영속성 관련 문제를 볼륨 개념을 지원함으로서 해결하고 있다. 조금 더 도커를 믿고 사용해보자. 도커가 지원하는 여러가지 편의를 포기하기엔 너무 아깝다. References [MySQL || MariaDB] InnoDB 총 정리 - .java의 개발일기 - Tistory InnoDB에 대하여 (MyISAM과의 차이점) - 삵 (sarc.io) [Docker] mariadb 및 utf-8 설정 - 정리 중... - 티스토리 https://hyeon9mak.github.io/migrate-local-database-to-docker/ 로컬 DB 도커로 마이그레이션 하기 🐳 summary “영속성을 가져야할 클라이언 데이터를 도커 위에서 관리한다고?” 라는 생각 때문에 데이터베이스만큼은 절대로 도커 위에 올려선 안되고, EC2 인스턴스 로컬에 설치해서 관리해야 hyeon9mak.github.io",BE
152,"Nginx는 버전 1.3부터 WebSocket을 지원하며, WebSocket의 로드 밸런싱 을 수행 할 수 있다. HTTP에서 WebSocket으로 연결 전환시 HTTP의 Upgrade 및 Connection 헤더를 사용한다. WebSocket을 지원할 때 리버스 프록시 서버가 직면하는 몇 가지 문제가 있다. 하나는 WebSocket이 hop-by-hop 프로토콜이므로 프록시 서버가 클라이언트의 Upgrade 요청을 가로챌 때 적절한 헤더를 포함하여 WAS 서버에 업그레이드 요청을 보내야 한다는 것이다. 또한 HTTP의 단기 연결과 달리 WebSocket은 오래 지속되기 때문에, 리버스 프록시는 연결을 닫지 않고 열린 상태로 유지하는 것을 허용해야 한다. Nginx는 클라이언트와 WAS 간 터널(소켓)을 설정할 수 있도록 WebSocket을 지원한다. NGINX가 클라이언트에서 WAS로 업그레이드 요청을 보내려면 Upgrade 및 Connection 헤더를 명시적으로 설정해야 한다. # Web-socket 관련 설정들

# 1. HTTP/1.1 버전에서 지원하는 프로토콜 전환 메커니즘을 사용한다
proxy_http_version 1.1;

# 2. hop-by-hop 헤더를 사용한다
proxy_set_header Upgrade $http_upgrade;
proxy_set_header Connection ""upgrade"";
# 3. 받는 대상 서버(WAS)
proxy_set_header Host $host; 실제로 적용한 모습 기존 구성해둔 nginx.conf 파일의 location 설정 부분에 웹 소켓 연결 요청을 위한 몇 가지 헤더값 설정만 진행해주면 된다. # nginx.conf

events {}

http {
upstream app {
server [WAS 인스턴스 IP]:[WAS 프로그램 포트번호];
}

# Redirect all traffic to HTTPS
# HTTP 요청을 HTTPS 로 강제전환
server {
listen 80;
return 308 https://$host$request_uri;
}

# HTTPS 요청 처리
server {
listen 443 ssl;
ssl_certificate /etc/letsencrypt/live/[도메인주소]/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/[도메인주소]/privkey.pem;

# Disable SSL
ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

# 통신과정에서 사용할 암호화 알고리즘
ssl_prefer_server_ciphers on;
ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5;

# Enable HSTS
# client의 browser에게 http로 어떠한 것도 load 하지 말라고 규제합니다.
# 이를 통해 http에서 https로 redirect 되는 request를 minimize 할 수 있습니다.
add_header Strict-Transport-Security ""max-age=31536000"" always;

# SSL sessions
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;

location / {
proxy_pass http://app;
proxy_http_version 1.1;
proxy_set_header Upgrade $http_upgrade;
proxy_set_header Connection ""upgrade"";
proxy_set_header Host $host;
}
}
} References Using NGINX as a WebSocket Proxy Nginx를 이용한 WebSocket reverse proxy - Joinc https://hyeon9mak.github.io/nginx-web-socket-proxy-configuration/ NGINX 웹 소켓 프록시 설정 Nginx는 버전 1.3부터 WebSocket을 지원하며, WebSocket의 로드 밸런싱 을 수행 할 수 있다. hyeon9mak.github.io",BE
153,"스프링에서 URL 내부의 데이터를 파싱하기 Goal 문자열 형태의 URL 에서 원하는 이용해 데이터를 파싱해봅니다. 직접 정규식을 짜지 않고, 스프링에서 제공하는 api를 이용해 데이터를 파싱해봅니다. AntPathMatcher 를 사용해봅니다. 개요 url에서 원하는 정보를 뽑아야 하는 상황이 발생 web 개발을 하다보면, url에서 원하는 데이터를 파싱해야되는 경우가 잦게 발생한다. 이 문제 사황을 해결하기위해 스프링에서는 많은 기능을 제공해주고 있다. @PathVariable의 경우가 그중 하나의 예시로 볼 수 있겠다. @RestController
@RequestMapping(""/uri-pattern"")
public class UriPatternController {

@GetMapping(""/users/{id}"")
public ResponseEntity<User> pathVariable(@PathVariable Long id) {
User user = new User(id, ""이름"", ""email"");
return ResponseEntity.ok().body(user);
}
} // url : /uri-pattern/uers/77

@GetMapping(""/users/{id}"")
pathVariable(@PathVariable Long id) 이번에 컨트롤러 단이 아닌 인터셉터 단에서, 요청온 URL에서 원하는 정보만 파싱해야 되는 상황이 발생했다. ""/topic/rooms/1/chat""

// 아래와 같이 roomId에만 관심을 가지는 상황
""/topic/rooms/{roomId}/chat"" 처음에 시도해본 방법은 roomdId를 파싱하는 정규식을 작성하는것이었다. 하지만 정규식을 작성하는데 걸리는 시간이 아까웠고, 루트의 이야기를 듣고 스프링 자체적으로 제공하는 파싱 방법이 있을꺼 같아서 찾아보게 되었다. AntPathMatcher private Long getRoomId(final String url) {
String actualUrl = ""/topic/rooms/{roomId}/chat"";
AntPathMatcher pathMatcher = new AntPathMatcher();

Map<String, String> variables = pathMatcher.extractUriTemplateVariables(actualUrl, url);
return Long.valueOf(variables.get(""roomId""));
} AntPathMatcher 를 이용하면 쉽게 파싱할 수 있었다. 추출하는 정규식 코드를 작성할 필요가 없다. 원하는 형태의 actualUrl을 만들어두고, 들어온 url을 맞춰서 넣어주면된다. 주의할점 실제로 들어오는 url이 actualUrl 형태와 같게 해야하므로, 앞에서 전처리나 분기처리가 필요하다. 인자를 넣는 순서에 주의해야한다. (actualUrl, url) 순서이다. 반대로 넣지 않게 주의한다. @PathVariable Long id 과 다르게, 문자열이 숫자형태라고 해서 자동으로 숫자타입으로 캐스팅되지 않으므로 String으로 매핑 해야한다. Map<String, String> variables // ok
Map<String, Long> variables // No 코드
import org.springframework.util.AntPathMatcher;

import java.util.Map;

public class UrlPhaser {
public static void main(String[] args) {
String url = ""/topic/rooms/1/chat"";
Long roomId = getRoomId(url);
System.out.println(roomId);
}

private static Long getRoomId(final String url) {
String actualUrl = ""/topic/rooms/{roomId}/chat"";
AntPathMatcher pathMatcher = new AntPathMatcher();

Map<String, String> variables = pathMatcher.extractUriTemplateVariables(actualUrl, url);
return Long.valueOf(variables.get(""roomId""));
}
} // result

1 public class UrlPhaser {

public static void main(String[] args) {
String url = ""/topic/rooms/1/chat/fortune"";
phaser(url);
}
private static void phaser(final String url) {
String roomId = ""roomId"";
String userName = ""userName"";

String actualUrl = String.format(""/topic/rooms/{%s}/chat/{%s}"", roomId, userName);
AntPathMatcher pathMatcher = new AntPathMatcher();

Map<String, String> variables = pathMatcher.extractUriTemplateVariables(actualUrl, url);
System.out.println(variables.get(roomId));
System.out.println(variables.get(userName));
}
} // result

1
fortune 다음과 같이 여러정보를 파싱할 수 도 있다. Reference https://stackoverflow.com/questions/26347868/spring-utility-for-parsing-uris https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/util/AntPathMatcher.html https://github.com/spring-projects/spring-framework/blob/main/spring-web/src/test/java/org/springframework/web/util/pattern/PathPatternTests.java",BE
154,"Summary 클라이언트A가 서버와 소켓 연결을 끊을 경우(Disconnect) 서버에 해당 소식을 전달하고, 소식을 접한 서버가 나머지 클라이언트들에게 클라이언트A의 연결이 끊겼음을(퇴장했음을) 알리는 형태를 구성하고 싶었다. 사용자가 Disconnect 버튼을 직접 눌러서 서버와 소켓 연결을 끊을 경우엔 연결이 끊기기 직전 서버 쪽으로 알림 요청을 먼저 보내는 방법이 채택 가능하겠으나, 사용자들은 브라우저의 탭을 닫는 등 (개발자 기준)비정상적인 방법을 많이 이용한다. 그래서 우리는 Disconnect 버튼과 관계없이, 클라이언트와 연결이 끊기면 서버에서 이를 감지해내는 방법을 찾아야 했다. @EventListener + SessionDisconnectEvent @Configuration
public class SomeConfigClass {

@EventListener
public void onDisconnectEvent(SessionDisconnectEvent event) {
...
}
} @EventListener 어노테이션과 SessionDisconnectEvent 파라미터를 조합한 메서드를 준비하면 웹 소켓 연결이 종료될 경우 SessionDisconnectEvent 파라미터에 연결종료 관련 정보가 담겨 핸들링이 가능했다. 수 차례 테스트를 반복해보면서 소켓 연결이 끊길 경우 어떤 값들이 넘어오는지 조사했다. 그 중 유일하게 sessionId가 어떤 클라이언트인지를 식별 할 수 있었다. sessionId는 최초 서버와 클라이언트가 소켓으로 연결이 되는 시점에 발급되는 식별자다. sessionId를 통해 연결 종료시 클라이언트를 식별 할 수 있게 되었지만, 클라이언트의 데이터(데이터베이스에 저장 될)와 매핑되어 있진 않았기 때문에 연결 시점에 클라이언트 데이터와 sessionId를 함께 알아내지 못하면 말짱 도루묵이었다. 최초 연결 때 - 클라이언트 데이터를 포함한 연결요청 앞서 SessionDisconnectEvent를 이용했기 떄문에, 반대로 SessionConnectEvent를 사용하면 될 것이라고 간단하게 생각했다. 웹소켓은 소켓 연결을 위해 최초 HTTP 프로토콜을 이용해 서버에 프로토콜 전환 요청을 보낸다. 이걸 SessionConnectEvent로 캐치해서 핸들링하고자 했다. stompClient = Stomp.over(socket);
stompClient.connect({}, function (frame) {
frame.headers[""clientData""] = ""현구막은 대단해!"";
} @Configuration
public class SomeConfigClass {

@EventListener
public void connectEvent(SessionConnectEvent event){
...
}
} SessionDisconnectEvent와 동일한 형태로 연결되는 순간을 잡아낼 수 있었으나, 말 그대로 연결에 관련된 Session정보만 얻을 수 있었지, 클라이언트의 데이터를 함께 포함시킬 수 없었다. javascript쪽 코드에서 stompClient.connect는 헤더에 클라이언트의 데이터를 포함시킨 후 연결이 진행되는 것이 아닌, 이미 연결이 진행된 후 추가적으로 헤더에 데이터를 기입하는 방식으로 동작했다. 헤더 외에도 여러가지 방법을 시도해보았으나, SessionConnectEvent로는 만족스러운 결과를 얻을 수 없었다. 최초 연결 때 - ChannelInterceptor를 써보자 @Configuration
public class StompHandler implements ChannelInterceptor {

@Override
public Message<?> preSend(Message<?> message, MessageChannel channel) {
StompHeaderAccessor accessor = StompHeaderAccessor.wrap(message);
if (StompCommand.SUBSCRIBE == accessor.getCommand()) {
 ...
}

return message;
}
} 고심을 하던 중 ChannelInterceptor 를 발견했다. Spring Integration. 스프링 기반 어플리케이션 내에 메시징 기반 서비스를 제공하고 선언적 어댑터를 사용해 외부 시스템과의 통합을 쉽게 해주는 프레임워크다. 리모팅, 메시징, 스케줄링과 같이 스프링이 제공하는 기능들을 추상화하고 있다. 그것 중 하나가 ChannelInterceptor. ChannelInterceptor.preSend() 메서드를 구현해주니 클라이언트에서 보낸 정보를 중간에 가로챌 수 있었다. 현재 요청이 CONNECT 때 온 것인지, SUBSCRIBE 때 온 것인지 등등 SessionConnectEvent를 사용하는 것보다 훨씬 많은 정보를 얻어낼 수 있었다. SUBSCRIBE 시점에는 구독주소에 대한 정보도 추가적으로 얻을 수 있었다! 그러나 여전히 클라이언트의 데이터를 포함시키진 못했다. 최초 연결 말고, 그 직후에 매핑하기 최초 연결시 클라이언트 데이터를 포함시키는 것을 포기하고, 연결 직후 세션정보와 클라이언트 데이터를 1회 전송하는 방식으로 관점을 변경했다. stompClient = Stomp.over(socket);
stompClient.connect({}, function (frame) {

subscription = stompClient.subscribe('/topic/', function (chat) {
...
});

const sessionId = // 무언가의 방법
const clientData = $(""#clientData"").val();
stompClient.send('/ws', {}, JSON.stringify({sessionId, clientData}));
}); 일반적인 메시지를 서버에 송신하는 것처럼, 연결이 완료 된 직후 send()를 통해서 클라이언트 데이터와 세션 정보를 담아서 보낸다면 서버에서 2가지 정보를 동시에 받아 매핑이 가능했다. 매핑이 완료될 경우 소켓 연결이 강제로 종료 되었을 때도 sessionId를 통해 클라이언트의 데이터를 찾아 후처리가 가능하다! 남은 문제는 '클라이언트 측에서 sessionId를 어떻게 찾아내는가?' 다. 클라이언트에서 발급된 sessionId가 무엇인지 확인하기 var socket = new SockJS('/connection');
stompClient = Stomp.over(socket);

stompClient.connect({}, function(frame) {
console.log(socket._transport.url);
// ws://localhost:8080/connection/039/{sessionId}/websocket
}); stackoverflow 에서 위 코드와 같은 해법을 찾을 수 있었다. socket._transport.url 을 통해 긴 주소를 받을 수 있었고, 거기서 {sessionId}에 해당하는 영역에 sessionId가 포함되어 있는 것이다. sessionId 파싱작업이 추가로 필요해졌지만, 이 외에 클라이언트 단에서 sessionId를 찾아낼 방법을 강구하기 어려웠다. stompClient = Stomp.over(socket);
stompClient.connect({}, function (frame) {

subscription = stompClient.subscribe('/topic/', function (chat) {
...
});

const sessionId = 파싱_메서드(socket._transport.url);
const clientData = $(""#clientData"").val();
stompClient.send('/ws', {}, JSON.stringify({sessionId, clientData}));
}); 결국 이와 같은 형태로 클라이언트 데이터와 sessionId를 동시에 보낼 수 있게 되었다! 결론 @EventListener + SessionDisconnectEvent ChannelInterceptor socket._transport.url 위 기술들을 사용해서 아래와 같은 흐름이 그려진다. 연결 클라이언트와 서버 간 연결을 진행한다. 연결에 성공하면, 클라이언트는 session Id 와 데이터를 서버에게 일반적인 메세지를 보내듯 send() 로 전송한다. 서버는 sessionId와 클라이언트 데이터를 매핑해서 DB에 저장한다. 연결 종료 연결이 끊어질 경우 서버가 이를 감지하고, sessionId를 잡아낸다. 서버는 sessionId를 통해 후처리를 진행한다. References 소켓통신할 경우 session 관리 문의 - OKKY [Java] WebSocket의 Session 사용 방법(Broadcast)과 웹 채팅 소스 예제 - 명월 일지 Spring Boot + STOMP + JWT Socket 인증하기 - velog How to get session id on the client side? (WebSocket) Spring Integration 이란? - Rednics Blog https://hyeon9mak.github.io/web-socket-disconnection-detection/ web-socket 연결 끊김 감지하기 🐧 Summary 클라이언트A가 서버와 소켓 연결을 끊을 경우(Disconnect) 서버에 해당 소식을 전달하고, 소식을 접한 서버가 나머지 클라이언트들에게 클라이언트A의 연결이 끊겼음을(퇴장했음을) 알리 hyeon9mak.github.io",BE
155,"AWS EC2 인스턴스 4개를 이용해 서비스 인프라를 구축하려 할 때, 루트가 ""WAS랑 DB한테 public IP를 할당할 필요가 있어?"" 라는 이야기를 꺼냈다. 어차피 클라이언트와 통신을 주고 받아야하는 Web Server(Reverse Proxy)와 우리가 직접 접속해야 할 Bastion은 public IP가 필수로 필요하지만, 그 외에 같은 IP 대역을 사용하고 있어서 private IP로 통신이 가능한 WAS와 DB한테는 public IP를 할당할 필요가 없다는 말이었다. 좋은 의견인 것 같아 결국 WAS, DataBase 인스턴스의 public IP를 할당하지 않았고(private IP만 남긴 상황), Bastion을 통해 ssh 연결을 성공했다. 그런데 몇 가지 설정을 진행하던 중에 sudo apt update 명령이 동작하지 않았다. 처음엔 permission 관련 문제인 줄 알았는데, 여러가지 시도 끝에 외부 internet과 연결이 되지 않았음을 알 수 있었다. 결국 CU한테 DM으로 질문을 드렸고, ""탄력적 IP를 부여하면 해결이 될 것이다."" 라는 답변을 받을 수 있었다. 결국 WAS와 DataBase 쪽 인스턴스에 탄력적 IP를 부여해서 문제를 해결 할 수 있었는데, 이 과정에서 유추 가능했던 것이 'public IP가 없으면 internet을 통해 외부로 요청이 불가능 하다.' 였다. private IP는 internet network에 연결 할 수 없나? 인터넷 영역에서는 모두가 알고 있는 IP로 통신해야 하는데, 이 때 필요한게 Public IP(공인 IP) 이다. AWS VPC 환경에서 EC2 인스턴스가 인터넷 통신이 가능하도록 만들려면, 인스턴스의 private IP와 연결된 public IP가 있어야 한다. 사용자의 인스턴스는 기본적으로 VPC 및 서브넷 내부 private IP 주소들만 인식한다. 인터넷 게이트웨이가 EC2 인스턴스 IP를 대신해 논리적인 NAT(Network Address Translation)을 제공할 때 인터넷 게이트웨이의 IP주소 + EC2인스턴스의 privateIP를 결합한 무언가를 만드는게 아닌, EC2인스턴스와 1:1로 매칭되는 IP를 제공한다. 때문에 인터넷 상에서 회신 주소 필드를 채우기 위해선 유일한 IP주소값인 public IP 주소가 필요하다. AWS에서 제공하는 VPC용 NAT 디바이스를 사용하면 public IP를 할당하지 않은 인스턴스도 인터넷 연결이 가능하다. 단 이 경우에는 외부 인터넷에서 EC2 인스턴스로 연결 요청을 보내는 인바운드는 거절되며, EC2 인스턴스에서 외부로 요청을 보내는 아웃바운드만 가능하다. 주로 보안 이슈를 해결하면서 소프트웨어 펌웨어 업데이트 등을 챙기려 할 때 좋은 방식이다. WAS, DataBase 인스턴스에게는 public IP가 존재하지 않았기 때문에 sudo apt update 명령을 통해 apt 측에 ""최신 버전 apt 파일들을 내려주세요!"" 라는 요청을 전송했지만, apt 측에서 답변을 받을 대상이 누군지 찾아내지 못해서 업데이트가 진행되지 못했던 것이다. Elastic IP AWS Elastic(탄력적) IP는 본래 우리가 사용한 용도로 탄생한 것이 아니다. EC2 인스턴스들의 IP는 고정적이지 않다. IPv4 주소체계에서 IP가 워낙 부족한 상황이라, 새로운 EC2 인스턴스가 생성될 때마다 IP주소를 부여하기 어렵기 때문에 EC2 인스턴스가 동작하는 동안에만 IP를 부여하고 멈추면 IP를 반납시키는 형태의 유동 IP를 사용하고 있다. 이런 상황에서 다른 네트워크와 주기적인 연결이 필요한 EC2 인스턴스에게 편의를 제공하고자 고정적인 public IP를 제공해주는 것이 바로 Elastic IP다. AWS의 Elastic IP는 EC2 인스턴스에게 쉽게 부여할 수 있고, 쉽게 제거할 수도 있다. 우리가 WAS, DataBase 인스턴스에 Elastic IP를 부여해서 internet 연결에 성공한 이유도 Elastic IP를 통해 IP 대역이 서로 다른 internet network들에게 public IP를 공개함으로서 요청과 답변을 주고 받을 수 있는 길을 텃기 때문이다. internet network에 연결이 필요한 작업을 모두 끝낸 후 Elastic IP를 다시 회수하면 IP 대역이 다른 네트워크에서 접근할 방법이 없는, 오로지 같은 대역폭 내에서 private IP로만 접근할 수 있는 고립된 인스턴스로 만들 수 있다. https://hyeon9mak.github.io/private-ip-can-not-connect-to-internet/ private IP 만으론 인터넷 연결이 불가능한가? hyeon9mak.github.io",BE
177,"Elastic Stack
ELK: Elasticsearch, Logstash, Kibana
Elasticsearch: 검색 및 분석엔진
Logstash: 서버 사이드 데이터 처리 파이프라인, 여러 소스의 데이터를 수집하여 변환한 뒤 elasticsearch 에 전송
Kibana: Elasticsearch 의 내용을 차트와 그래프를 이용해 데이터를 시각화
Beats: 경량의 단일 목적 데이터 수집기 제품군
Elastic Stack: EKL + Beat
우리 서비스 ELK Stack 구성
ELK 모식도

ElasticSeach, Logstash, Kibana는 docker-elk 레포지토리를 참고하여 설정
각각의 GM 서버에는 로그 정보를 Logstash에 전달하는 filebeat를 설치
FileBeat
로그가 있는 서버에 설치
Beats 의 제품군에는 여러가지가 존재하는데 그 중 로그파일 전용이 Filebeat
filebeat.inputs:
- type: log
# 도커 컨터이너로부터 쌓이는 로그의 경로 설정
paths:
- '/var/lib/docker/containers/*/*.log'
# docker 의 로그는 json 형태로 쌓이는데 json.message_key 는 해당 key 에 해당하는 정보를 위로 올려준다.
json.keys_under_root: true
json.overwrite_keys: true
json.add_error_key: true
json.expand_keys: true
json.message_key: log
# 로그를 한줄한줄이 아닌 원하는 패턴의 단위로 묶어서 보내줄 수 있다.
multiline.type: pattern
multiline.pattern: '^Epoch'
multiline.negate: true
multiline.match: after
# default 는 5초로 셋팅되어 있다. 묶어주는 로그 단위가 크면 중간에 의도한대로 나오지 않고 짤렸기 때문에 늘려주었다.
multiline.timeout: 30
output.logstash:
hosts: [${로그스태시의 호스트 IP:PORT}]
Logstash 설정
filebeat 에서 로그를 받아 filter 로 분석하기
input {
beats {
 port => Logstash에서 열어준 포트번호(default:5044)
}
}

filter {
# backspace 제거: 정형화된 로그가 아니다 보니 부분이 문제가 되었는데 없애줌
mutate {
 gsub => [""log"",""[]"", """"]
}
# grok 을 사용하여 원하는 방식대로 로그를 filter
# grok 문법을 사용하며, log 필드에 들어온 값을 정규표현식과 비슷하게 파싱하여 json 형태로 보내준다.
# %{해당패턴:필드로_묶을_이름:자료형} -> 필드로 묶을 이름을 지정하지 않으면 해당패턴 명으로 묶인다.
grok {
 match => {""log"" => [""Epoch %{NUMBER:currentEpoch:int}/%{NUMBER:totalEpoch:int}
%{GREEDYDATA}- loss: %{NUMBER:loss:float} - accuracy: %{NUMBER:accuracy:float}""] }
}
# jobId를 태그로 관리: [] 방식으로 json 경로의 key 접근가능
mutate {
 rename => {""[attrs][tag]"" => ""jobId""}
}
}

output {
elasticsearch {
 hosts => 엘라스틱 포트
 # .. 각종 설정정보 넣어줌 ..
 # index 는 job 으로 클러스터링
 index => ""job""
}
}
Kibana
kibana 는 elastic 에서 API 를 통해 봐야하는 로그정보들을 쉽게 시각화 해줌 (개발하며 로그 시각화 및 모니터링이 가능)
우리 서비스에서 kibana 를 사용하여 로그를 시각화 하지는 않았고, 프론트엔드 팀원들이 시각화를 진행
ElasticSearch
SpringBoot 를 사용한다면 의존성 추가로 쉽게 적용이 가능하다.
gradle 설정에서 implementation 'org.springframework.boot:spring-boot-starter-data-elasticsearch'를 추가
Java 에서 Repository 처럼 쉽게 사용이 가능하다.
Config 설정을 해준다.
@Configuration
public class ElasticClientConfig extends AbstractElasticsearchConfiguration {
@Override
@Bean
public RestHighLevelClient elasticsearchClient() {

final ClientConfiguration clientConfiguration = ClientConfiguration.builder()
 .connectedTo(host)
 .withBasicAuth(userName, password)
 .build();

return RestClients.create(clientConfiguration).rest();
}
}
ElasticsearchRepository 를 상속받아 Repository 처럼 사용가능하다.
public interface LogRepository extends ElasticsearchRepository<Log, String> {
List<Log> findByJobIdOrderByTime(Long jobId);
}
docker-compose.yml 설정
elk docker 설치 참고 레포짓토리의 설정 파일을 우리 환경에 맞게 수정을 진행했다.

version: '3.2'

services:
elasticsearch:
build:
context: elasticsearch/
args:
ELK_VERSION: $ELK_VERSION
volumes:
- type: bind
source: ./elasticsearch/config/elasticsearch.yml
target: /usr/share/elasticsearch/config/elasticsearch.yml
read_only: true
- type: volume
source: elasticsearch
target: /usr/share/elasticsearch/data
ports:
# EC2 인바운드 규칙으로 9000번 포트로 수정
- ""9000:9200""
- ""9300:9300""
environment:
# OOM 에러를 해결하기 위해 메모리 크기 수정
ES_JAVA_OPTS: ""-Xmx1g -Xms1g""
ELASTIC_PASSWORD: changeme
discovery.type: single-node
networks:
- elk

logstash:
build:
context: logstash/
args:
ELK_VERSION: $ELK_VERSION
volumes:
- type: bind
source: ./logstash/config/logstash.yml
target: /usr/share/logstash/config/logstash.yml
read_only: true
- type: bind
source: ./logstash/pipeline
target: /usr/share/logstash/pipeline
read_only: true
ports:
# EC2 인바운드 규칙으로 8000번 포트로 수정
- ""8000:5044""
- ""5000:5000/tcp""
- ""5000:5000/udp""
- ""9600:9600""
environment:
# OOM 에러를 해결하기 위해 메모리 크기 수정
LS_JAVA_OPTS: ""-Xmx1g -Xms1g""
networks:
- elk
depends_on:
- elasticsearch

kibana:
build:
context: kibana/
args:
ELK_VERSION: $ELK_VERSION
volumes:
- type: bind
source: ./kibana/config/kibana.yml
target: /usr/share/kibana/config/kibana.yml
read_only: true
ports:
# EC2 인바운드 규칙으로 8080번 포트로 수정
- ""8080:5601""
networks:
- elk
depends_on:
- elasticsearch

networks:
elk:
driver: bridge

volumes:
elasticsearch:
참고자료
what is elk
elk docker 설치
filebeat multiline
spring-data docs elasticsearch
Pages 38
Find a page…
Home
2021 07 16 Fri 기업과의 소통
2021.06.30 | 기업 미팅 | 질문 리스트
2021.06.30 | 시작 회의 | 규칙과 문화
2021.07.01 | 기획 회의 | 기업 미팅
2021.07.02 | 기획 회의 | 기획안 제출
2차 데모데이 준비
3차 데모데이 준비
5주차 (2021.07.26~08.01) 공통희의
5주차 (2021.07.26~08.01) 백엔드 회의
5주차 (2021.07.26~08.01) 프론트 회의
[2주차 데이터 관련 회의]
[2주차 백엔드] 210705~210711
[2주차 프론트엔드] 210705~210711
[2주차] 210705~210711
Clone this wiki locally
https://github.com/woowacourse-teams/2021-gpu-is-mine.wiki.git
Footer",BE
178,"DB Migration
DB Migration의 필요를 모를 수 있다. 솔직히 나는 몰랐다. 배포 후 데이터를 관리해본 경험이 없었고, 유지 보수 중 스키마 구조가 바뀌는 상황에 어떻게 대처하는지 생각해본 적 없었다.



사실 flyway를 검색하면 사용 방법이 아주 자세히 잘 나와있다. 그런데도 내가 이 글을 쓰는 이유는 배포 후 DB를 관리하고 유지 보수 해본 경험이 전혀 없는 학생들에게 '당신에게 곧 이런 문제 사항이 생길 것이고, flyway라는 툴은 이걸 이렇게 풀어준다.'를 소개하고 싶었다.



나중에 그런 경험을 만났을 때, 'DB Migration 또는 Flyway tool 라는 키워드가 있었던 거 같은데~' 정도의 생각이 들면, 나는 성공이다. 키워드 수준이라도 문제 해결의 방향을 알고 모르고의 차이는 크니까.



당신에게 곧 이런 문제 사항이 생길 것이고, 그걸 해결할 수 있는 flyway라는 툴이 있더라
상황을 하나 보여주려고 한다. SampleEntity를 예제로 사용할 것인데, 배포가 이미 완료된 더 복잡하고 이미 쌓인 정보도 많은 데이터라고 상상해주길 바란다.

@Entity
public class SampleEntity {
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;
private String name;
}


이런 필드로 SampleEntity가 배포되고 데이터가 쌓였던 상황인데, 신기술을 개발하는 도중에 int age라는 필드를 추가하게 되었다.



@Entity
public class SampleEntity {
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;
private String name;
private Integer age;
}


자, 이런 상황에서 당신은 어떻게 이 SampleEntity 변화를 처리할 것인가. 잠시 고민해봤으면 좋겠다.



나는 배포 서버 DB에 직접 들어가서 테이블을 직접 수정해왔다. 그건 쉬운 작업이 아니다. 솔직히 엄청 귀찮다. 배포 후 기능을 추가하면서 스키마가 변경되는 일이 많았고, 매번 일일히 작업하는 게 번거로울 뿐 아니라 실수하기도 딱 좋다.



ALTER TABLE sample_entity ADD COLUMN age integer default 0


로컬에서 DB 변경 사항을 추가하는 것으로 배포 이후에는 알아서 관리될 수 있다면 얼마나 좋을까. 그냥 코드를 관리하는 것처럼 DB 변경 사항을 관리할 순 없을까?



Flyway라는 툴을 사용하면 그걸 해소할 수 있다. Flyway를 사용하고 해결하는 과정을 천천히 보여주려고 한다.



직접 적용 과정을 보여드리겠습니다.
예외 확인하기 : Caused by: java.sql.SQLSyntaxErrorException: Unknown column 'age' in 'field list'



우선 sampleEntity 데이터를 미리 3개 넣어두었다. 이 상황에서 int age 필드가 추가하고 DB 스키마를 수정하지 않으면 SQL 에러가 뜨는 것을 확인한다.





자 이제 flyway를 적용할 것이다.



궁극적인 적용 목표는 다음과 같다. (환경은 gradle, spring boot, mySql 를 기준으로 한다.)



1. DB에 접속해서 table을 직접 건들지 않고, 나이를 포함한 신규 데이터를 저장한다.

2. 동시에 이전 나이가 포함되지 않은 데이터는 나이 필드를 0으로 초기화한다.

3. git으로 관리할 것이기 때문에, 해당 작업을 파일로 관리할 수 있어야 한다.



1. 의존성 추가 : build.gradle



dependencies {
implementation('org.flywaydb:flyway-core:6.4.2')
}


2. 어플리케이션 설정 추가 : application.properties



#data source 설정 / 본인 환경에 맞게 수정해주세요.
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.datasource.url=jdbc:mysql://localhost:13306/flyway
spring.datasource.username=ecsimsw
spring.datasource.password=1234

#flyway 설정
spring.jpa.properties.hibernate.jdbc.lob.non_contextual_creation=true
spring.flyway.enabled=true
spring.jpa.generate-ddl=false

#Spring boot 2 이상의 경우 아래 설정 추가
spring.flyway.baselineOnMigrate = true


3. init 파일 추가 : resources/db/migration/V1__init.sql



flyway를 등록하려는 시점의 db 스키마 구조를 입력해야 한다. 이 파일을 기준으로 flyway가 DB 버전 관리를 하게 된다. 지우면 안 된다.

resources/db/migration/V1__init.sql, 파일 경로와 파일명을 똑같이 해야 한다.

# 이전 table를 지우고
drop table if exists sample_entity;

# 초기 스케마를 정의하고
create table sample_entity(
id bigint auto_increment,
name varchar(255),
primary key (id)
);

# history를 추가한다.
INSERT into sample_entity (name) values ('ecsimsw');
INSERT into sample_entity (name) values ('코기');
INSERT into sample_entity (name) values ('김진환');


4. 스키마 구조 변경 사항 파일 추가 : resources/db/migration/V2__add_age.sql



이렇게 flyway를 적용한 시점(아직 age 사용 x)에서 작업을 하다가, 스키마 구조가 변경되는 상황(age 필요)이 생겼다고 하자. 이때 변경 사항을 파일에 저장하는 것이다. 이때 파일명은 flyway의 규칙을 따라야 한다. 어렵다면 일단 'V {숫자}__{설명}. sql' 정도만 알고 넘어가자.




https://flywaydb.org/documentation/concepts/migrations.html


다시 예제로 돌아와 'resources/db/migration/V2__add_age.sql'을 작성할 것이다. 파일에 스키마 변경 사항을 작성해주는 것이다. 마찬가지로 파일 위치와 파일명에 유의하길 바란다.



ALTER TABLE sample_entity ADD COLUMN age integer default 0


이렇게 하고 다시 age를 포함한 데이터를 추가하면 이제는 에러 없이 잘 들어감을 확인할 수 있다. 이제는 개발 도중 스키마가 변경되면 직접 배포 서버의 DB를 들어가 스키마를 변경하지 않고, 변경 사항을 코드로 관리할 수 있게 된 것이다.



5. flyway_schema_history 확인하기



마지막으로 DB에 저장된 flyway_schema_history를 확인한다. V1__init.sql과 V2__add_age.sql이 히스토리로 저장된 것을 확인할 수 있다.






추가 : 초기 설정 도중 아래의 에러를 만난다면 : Detected failed migration to version 1 (init)
org.springframework.beans.factory.BeanCreationException:
Error creating bean with name 'flywayInitializer' defined in class path resource ...
Invocation of init method failed;
nested exception is java.lang.reflect.InvocationTargetException

Caused by: java.lang.reflect.InvocationTargetException: null

Caused by: org.flywaydb.core.api.FlywayException: Validate failed:
Detected failed migration to version 1 (init)


설정 중 잘못된 flyway history 파일이 생겼을 수 있다. 내 경우에는 DB에 직접 접속해서 flyway_schema_history를 직접 제거하고 다시 실행시키니 해결되었다.",BE
179,"프로젝트 clone 하기
git clone 프로젝트.git => 서브모듈의 내부는 empty
git clone --recurse-submodules 프로젝트.git => 서브모듈까지 모두 clone (서브모듈이 private repo일 경우 권한 필요-> github action의 ACCESS_TOKEN이 이때 필요)
이미 clone 된 프로젝트의 경우 git submodule update --init = git submodule init + git submodule update
서브모듈 수정하고 반영하기
각 프로젝트 git은 그 서브모듈의 특정 시점을 기록하여 두고, 위의 방식으로 clone시 그 시점의 submodule을 가지고 옴(아래 사진을 봤을 때 @ 9049b4c는 프로젝트가 가르키고 있는 서브모듈의 특정 커밋의 해시값) submodule 그림

그렇기 때문에, 서브모듈을 업데이트 했을 때 이를 반영하기 위해서는, 프로젝트도 가르키고 있는 서브 모듈의 커밋값을 수정해줘야함.(사실 이 부분은 git에서 파일 수정처럼 진행되고 프로젝트에서 commit으로 진행됨. 아래에서 순서를 작성해보겠음)

(서브 모듈 폴더 내부에서) 서브 모듈을 수정하고, 커밋 한다. (다른 사람이 수정한 부분이더라도, 서브 모듈 폴더 내부에서 pull을 땡겨서 작업할 수있다.)
(서브 모듈 밖 프로젝트 폴더에서) git status를 쳐보면, 서브모듈명(new commit) 가 나온다. 이를 git add, commit, push하면 반영된다.
한가지 추가 하겠습니다. git clone 해온 상태에서는 HEAD detached at 30447ac와 같이 특정 커밋값을 가르키고 있습니다. 즉 특정 branch를 트랙킹하지 않고 있습니다. 만약 최신의 브랜치의 커밋으로 update 하기 위해서는 git checkout main을 진행하고 나서 진행해야 합니다.

서브 모듈 내부에서 작업을 하고, 이를 또 밖에서 한번더 진행한다 정도가 된다.

배포에 적용하기
위에서 설명했듯이, 기존의 clone으로는 서브모듈의 내부값을 못가지고 온다. 우리가 사용하는 github action, jenkins는 이러한 경우를 편리하게 할 수 있는 방법들을 제공한다.

Github Action
간단하게 checkout의 option을 추가해주면 된다. submodule의 깊이가 1인 경우면 submodules: true로 충분하고, submodule안에 submodule이 있는 구조의 경우 submodules: recursive옵션을 넣어주면 된다. 아래 token private repo의 권한이 있는 사용자의 토큰을 넣어주어야 한다.(현재 Wannte의 토큰이다.) secrets.ACCESS_TOKEN 같은 경우에는 github 레포 자체적으로 settings->secret 에 별도로 넣어두고 공개하지 않고 사용할 수 있다.

- uses: actions/checkout@v2
with:
submodules: true
token: ${{ secrets.ACCESS_TOKEN }}
Jenkins
젠킨스는 사용하면서 매번 느끼지만, 있을만한가? 하는 기능들은 다 이미 적용되어 있다. 아래의 설정을 진행해주면 된다.(Source Code Management-> Add Behavior)

jenkins-submodue

2번째 옵션으로 들어가 있는 부분을 추가적으로 설명하면 Update tracking submodules to tip of branch 이 옵션을 설정하게 되면, clone을 해올시 submodule의 가장 최신 커밋을 가지고 온다라는 것으로 보인다. git clone --recurse-submodules --remote-submodules와 같은 기능을 하는 것으로 보여서 궁금한 분은 검색해보면 좋을 것 같습니다.

참고 자료: 근본 Git Docs",NON_TECH
183,"배경지식
메세지 브로커
이벤트 브로커로 역할 불가
메세지 기반 미들웨어 아키텍처
PRODUCER, CONSUMER
메세지를 받아서 처리하고 나면 삭제되는 구조
래디스, 레빗엠큐
이벤트 브로커
메세지 브로커로 역할 가능
서비스에서 나오는 이벤트를 이벤트 큐에 저장함
딱 한번 일어난 이벤트를 저장하여 → 단일진실 공급원으로 사용
장애가 일어난 시간부터 다시 처리가능
많은 실시간 데이터를 처리가능
장부를 딱 하나만 보관하고 인덱스를 통해 개별 엑세스를 관리
업무상 필요한 시간동안 보관 가능
카프카
큐와 소비자의 연관관계
OneToOne
하나의 큐에 하나의 애플리케이션이 있음
OneToMany
하나의 큐에 여러개의 애플리케이션이 있음
Redis vs RabbitMQ vs Kafka
비교1
브로커 스케일 데이터 영속성 Consumer 연관
Redis millions msg/sec 지원하지 않음(인메모리 저장),
데이터 덤프하여 저장할 수는 있는 듯 OneToOne, OneToMany 둘다 가능
RabbitMQ 50K/sec 지원 OneToOne, OneToMany 둘다 가능
Kafka millions msg/sec 지원 OneToMany만 가능
비교2
Redis RabbitMQ Kafka
특징 - 인메모리로 동작, 캐시처럼 사용 - 복잡한 라우팅에 사용 - 대용량 데이터처리 및 대규모 분산처리에 사용
장점 - 빠르다
- 스프링과 사용시, 자바 8버전부터 지원 - UI로 지원
- consumer가 메세지를 받았음을 확신가능 (수신 확인이 되지 않은 경우 다시 보냄) - Consumer가 메세지 선택이 가능
- 메세지 다시 읽기 가능
(이벤트가 유지되기 때문)
단점 - 자료 소실 가능성
- UI 툴을 사용하려면 추가 다른 프로그램 설치 필요
- 설정 단계에서 GUI 지원하지 않음
- 수신자가 메시지를 받았음을 확신할 수 없음 - 스프링과 사용시, 자바 11버전 이상부터 지원 - 기술 튜토리얼이나 자료가 접근 난이도가 있음
소비방식 - 생산자가 push하여 소비자에게 보내는 방식 - 소비자가 pull 방식이 가능
기타 - 메세지 브로커 - 이벤트 브로커
- pub/sub 기반
- 파일시스템으로 관리
- partition과 offset으로 데이터를 선택
결론: RabbitMQ
셋다 원하는 큐 기능은 구현이 가능했으나 JobQueue를 구현하는데 있어 가장 목적과 맞는 것이 RabbitMQ라고 판단
MessageQueue를 통해 어떻게 나눠 처리할 것인지를 구현하는데(라우팅) 목적이 있는 미들웨어라고 판단
튜토리얼이나 기술 접근난이도가 높지 않아보였고, 설정단계에서도 UI로 지원하는 부분도 크다고 판단
Redis는 빠르지만 캐시처럼 동작하는 듯 했고, 우리에겐 속도가 크게 중요하지 않다고 생각
Kafka는 대규모 분산처리에 목적이 있는 프로그램이라고 판단, 메세지큐도 구현가능하지만 원래 목적이 대규모 분산처리 시스템을 위한 것이라 아키텍처의 목적과 맞지 않는다고 판단
참고
redis, rabbitMQ, kafka
이벤트브로커와 메세지큐 설명글(영어)
rabbitMQ와 redis
스프링 공식문서 rabbitMQ 튜토리얼
이벤트브로커, 메세지브로커 차이 설명 동영상
rabbitMQ 공식문서 getstarted 튜토리얼 설명",BE
184,"우리 팀의 로깅 전략
HTTP 요청, 응답 -> Console
Info 레벨 이상의 정보, DB access 정보 -> logs/log.log
서버 에러 정보 -> logs/err_log.log

로깅 설정 방법
logback 설정 파일
logback 설정 파일로 로그를 출력하는 위치, 출력할 로그 레벨, 출력 패턴을 지정한다. 스프링 부트의 기본 logback 설정 파일 위치는 'resource/logback-spring.xml' 이다.


logback 설정 파일의 Appender
발생한 로깅을 지정한 출력 레벨, 패턴, 위치에 따라 출력한다. 프로젝트에서 사용한 Appender 구현체는 ConsoleAppender, RollingFileAppender 두 가지인데, 각각 Console과 File에 출력한다.


ex,ConsoleAppender 예시

<appender name=""CONSOLE"" class=""ch.qos.logback.core.ConsoleAppender"">
<encoder class=""ch.qos.logback.classic.encoder.PatternLayoutEncoder"">
<pattern>%-5level %d{yy-MM-dd HH:mm:ss} [%logger{0}:%line] - %msg%n</pattern>
</encoder>
</appender>

<!-- 출력 예시 : INFO 21-07-08 20:47:52 [DocumentationPluginsBootstrapper:160] - Context refreshed -->

ex, RollingFileAppender 예시

<appender name=""FILE"" class=""ch.qos.logback.core.rolling.RollingFileAppender"">
<filter class=""ch.qos.logback.classic.filter.LevelFilter"">
<level>error</level>
<onMatch>ACCEPT</onMatch>
<onMismatch>DENY</onMismatch>
</filter>
<file>logs/error.log</file>
<encoder class=""ch.qos.logback.classic.encoder.PatternLayoutEncoder"">
<pattern>${LOG_PATTERN}</pattern>
</encoder>

<!-- Rolling 정책 -->

<rollingPolicy
class=""ch.qos.logback.core.rolling.TimeBasedRollingPolicy""> <!-- .gz,.zip 등을 넣으면 자동 일자별 로그파일 압축 -->
<fileNamePattern>logs/error.%d{yyyy-MM-dd}_%i.log</fileNamePattern>
<timeBasedFileNamingAndTriggeringPolicy
 class=""ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP""> <!-- 파일당 최고 용량 kb, mb, gb -->
<maxFileSize>10MB</maxFileSize>
</timeBasedFileNamingAndTriggeringPolicy> <!-- 일자별 로그파일 최대 보관주기(~일), 해당 설정일 이상된 파일은 자동으로 제거-->
<maxHistory>60</maxHistory>
</rollingPolicy>
</appender>

logging 레벨과 출력 정도
로깅의 정도를 설정하고, Application에서 받은 로그 중 Appender가 어떤 레벨까지 처리할 것인지를 정한다.


예를 들면 아래 예시에서 logging 레벨을 info로 하는 경우, info, error 모두 로깅되고, logging 레벨을 error로 하는 경우 error만 로깅된다.

public class TestApplication {
static final Logger logger = LoggerFactory.getLogger(TestApplication.class);

public void test() {
logger.error(""error"");
logger.info(""info"");
}

// 레벨 순서 : TRACE < DEBUG < INFO < WARN < ERROR
}

ex, Logging level 지정 예시 (application.properties)

logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE
logging.level.root=error

ex, Logger level 지정 예시 (logback-spring.xml)

<!-- ERROR Level 이상의 로깅 정보를 CONSOLE 이라는 이름을 가진 Appender로 처리한다. -->
<root level=""ERROR"">
<appender-ref ref=""CONSOLE""/>
</root>

단, logback 설정 파일에서 지정한 로깅 레벨보다, application 설정 파일로 지정한 레벨이 우선 순위가 높은 것으로 생각된다.


참고 자료
https://bcho.tistory.com/759",BE
209,"Elasticsearch - 1편 02 Nov 2021 on 조잘조잘 엘라스틱서치와 키바나를 이용해 검색 시스템을 구현해보자 검색엔진과 검색시스템 대부분의 서비스에 빠지지 않는 검색 기능. 이 기능을 부르는 용어로는 검색엔진, 검색 시스템, 검색 서비스 등이 대표적이다. 엘라스틱서치에 알아보기 전에 이 용어들의 정의를 먼저 살펴보자. 검색엔진(search engine)은 웹에서 정보를 수집해 검색 결과를 제공하는 프로그램이다. 검색 시스템(search system)은 대용량 데이터 기반으로 신뢰성 있는 검색 결과를 제공하기 위해 검색엔진을 기반으로 구축된 시스템을 뜻한다. 검색 서비스(search service)는 검색 시스템을 활용해 검색 결과를 서비스로 제공하는 것을 뜻한다. 검색 서비스 > 검색 시스템 > 검색엔진
엘라스틱서치는 엄밀히 말하면 검색엔진이며 검색 서비스를 제공하기 위해 엘라스틱서치를 이용해 검색 시스템을 구축한다고 생각하면 될 거 같다. 엘라스틱서치란? Elastic 홈페이지에서는 Elasticsearch를 Elastic Stack의 심장이라고 소개하고 있는 만큼 Elasticsearch는 전체 스택의 중심이며 가장 중요한 역할을 하고 있다. 기본적으로 모든 데이터를 색인하여 저장하고 검색, 집계 등을 수행하며 결과를 클라이언트 또는 다른 프로그램으로 전달하여 동작하게 한다. 특징 오픈소스 (open source) 자바 기반인 루씬으로 만들어진 오픈 소스 프로젝트 준 실시간 분석 (Near-Real Time) 전문(full text) 검색 엔진 루씬은 기본적으로 역파일 색인(inverted file index)라는 구조로 데이터를 저장 이렇게 가공된 텍스트를 검색하는 것을 전문(full text) 검색이라고 한다. RESTFul API 모든 데이터 조회, 입력, 삭제 등을 http 프로토콜을 이용한 Rest API로 처리 기본 용어 정리 [동사] 색인 (indexing) : 데이터가 검색될 수 있는 구조로 변경하기 위해 원본 문서를 검색어 토큰들으로 변환하여 저장하는 일련의 과정이다. [명사] 인덱스 (index, indices) : 색인 과정을 거친 결과물, 또는 색인된 데이터가 저장되는 저장소. 또한 Elasticsearch에서 도큐먼트들의 논리적인 집합을 표현하는 단위이기도 하다. 검색 (search) : 인덱스에 들어있는 검색어 토큰들을 포함하고 있는 문서를 찾아가는 과정. 질의 (query) : 사용자가 원하는 문서를 찾거나 집계 결과를 출력하기 위해 검색 시 입력하는 검색어 또는 검색 조건. RDBMS와 차이점 왜 검색을 기존 RDBMS로 하지 않고 검색엔진을 이용할까? 그것은 RDBMS의 한계와 연관이 깊다. 데이터베이스는 데이터를 통합 관리하는 데이터의 집합이고 저장 방식에 따라 관계형 또는 계층형으로 나뉜다. SQL문을 이용해 원하는 정보의 검색이 가능하지만 텍스트 매칭을 통한 단순한 검색만 가능하고 여러 단어로 변형, 동의어나 유의어를 활용한 검색이 불가능하다. 이와 다르게 검색엔진은 데이터베이스에서 불가능한 비정형 데이터를 색인하고 검색할 수 있다. 형태소 분석을 통해 자연어의 처리도 가능하고 역색인 구조를 바탕으로 빠른 검색 속도도 보장한다. RDBMS Elasticsearch 정형 데이터 비정형 데이터 텍스트 매칭을 통해 단순 검색 전문 검색을 통해 정확한 검색 유의어, 동의어 불가능 유의어, 동의어 가능 엘라스틱서치 설치 Elasticsearch는 https://www.elastic.co/downloads/elasticsearch 에서 설치 가능하다. EC2에서 엘라스틱서치를 사용하므로 리눅스 버전을 다운받았다. Elasticsearch 실행을 위해서는 자바1.8 이상의 버전이 설치되어 있어야 하며 JAVA_HOME 환경변수가 잡혀있어야 한다. 각 버전 별로 필요한 자바 버전은 ttps://www.elastic.co/support/matrix#matrix_jvm 페이지에서 확인이 가능하다. Elasticsearch 7.0 버전부터는 기본 배포판에 open-jdk 가 포함되어 있어 따로 Java를 설치 해 주지 않아도 된다. 대신 운영체제에 맞게 배포판을 받아야 한다. 엘라스틱서치 환경 설정 Elasticsearch는 각 노드들 별로 실행될 설정들을 각각 적용함으로써 노드들의 역할을 나누거나 클러스터의 속성을 결정하게 된다. Elasticsearch 의 실행 환경을 설정하는 방법은 홈 디렉토리의 config 경로 아래 있는 파일들을 변경하거나 시작 명령으로 설정하는 방법이 있는데 나는 config 아래에 있는 파일을 변경하는 방법을 이용했다. jvm.options - Java 힙메모리 및 환경변수 elasticsearch.yml - Elasticsearch 옵션 log4j2.properties - 로그 관련 옵션 vi config/jvm.options -Xms1g
-Xmx1g
7.0 기준으로 1gb의 힙메모리가 기본으로 설정. 원하는 값으로 변경 가능 node-1 ec2 기준(각 노드별로 각각 설정 적용해야 함) vi config/elasticsearch.yml cluster.name: ""es-cluster-1""
node.name: ""node-1""
network.host: [""_local_"", ""_site_""]
discovery.seed_hosts: [""elastic-1"", ""elastic-2"", ""elastic-3""]
cluster.initial_master_nodes: [""node-1"", ""node-2"", ""node-3""]
http.port: 9000
transport.port: 8888

xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.keystore.path: certs/es-cluster.p12
xpack.security.transport.ssl.truststore.path: certs/es-cluster.p12
_local_ : 로컬 IP _site_ : private IP discovery.seed_hosts : 클러스터 구성을 위해 바인딩 할 원격 노드들의 IP cluster.initial_master_nodes : 클러스터가 최초 실행 될 때 해당 노드들 중에서 마스터 노드 선출 http.port : http 통신을 할 port transport.port : 노드들 간 tcp 통신을 할 port xpack : 보안 관련 설정들. 추후 설명하겠습니다. 각 노드 인스턴스마다 /etc/hosts에 노드들의 private IP 등록 192.168.1.212 elastic-1
192.168.1.224 elastic-2
192.168.1.206 elastic-3
호스트 별칭 설정한다고 생각하면 된다. 이 설정을 통해 이름으로 IP 설정 가능. /etc/security/limits.conf 에 아래 내용 추가 elasticsearch - nofile 65535
max file descriptors를 기존 4096에서 65535로 늘려줘야한다. 설정을 변경하지 않으면 실행 시 에러가 발생. virtual memory 설정 /etc/sysctl.conf에 아래 내용 추가 vm.max_map_count=262144
기존 map count는 너무 적어 262144로 변경한다. 보안 설정 config/elasticsearch.yml 에 추가 xpack.security.enabled: true
이를 통해 보안 기능 활성화가 된다. 이 설정만 추가하고 엘라스틱서치를 실행하면 에러가 발생한다. 보안 기능만 활성화 하고 어떤 보안을 하는지 없기 때문에 에러가 뜨는 거라 하나하나 설정해보자. xpack.security.transport.ssl.enabled: true
이 설정을 해주고 공개키랑 대칭키를 입력을 해줘야 한다. 먼저 키를 만들어주자. bin/elasticsearch-certutil ca 를 입력하면 아래와 같은 화면이 뜬다. 엔터를 누르고 비밀번호를 설정해주자. 그러면 공개키가 생성된다. 그 다음 config 아래 certs 라는 폴더를 만들어주자. mkdir config/certs
그리고 ./bin/elasticsearch-certutil cert 
--ca elastic-stack-ca.p12 
--dns elastic-1,elastic-2,elastic-3 
--ip 192.168.1.212,192.168.1.224,192.168.1.206 
--out config/certs/es-cluster.p12

//dns, ip들 사이에 띄어쓰기 넣으면 오류납니다.
비밀번호 동일하게 설정했습니다. ./bin/elasticsearch-keystore create
./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password
./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password
한 줄씩 입력하면서 위 이미지와 같이 비밀번호 설정하면 된다. 설정을 마치면 아까 생성한 config/certs에 es-cluster.p12란 이름으로 인증서가 생성되었다. 이렇게 생성된 인증서를 다른 노드에도 추가해줘야 합니다. 1번 노드에 생성한 인증서를 로컬로 복사 로컬에 있는 인증서를 2,3번 노드에 복사 config/elasticsearch.yml에 인증서 추가
xpack.security.transport.ssl.keystore.path: certs/es-cluster.p12
xpack.security.transport.ssl.truststore.path: certs/es-cluster.p12
config/elasticsearch.yml 최종 설정 cluster.name: ""es-cluster-1""
node.name: ""node-1""
network.host: [""_local_"", ""_site_""]
discovery.seed_hosts: [""elastic-1"", ""elastic-2"", ""elastic-3""]
cluster.initial_master_nodes: [""node-1"", ""node-2"", ""node-3""]
http.port: 9000
transport.port: 8888

xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.keystore.path: certs/es-cluster.p12
xpack.security.transport.ssl.truststore.path: certs/es-cluster.p12
이로써 기본 환경설정은 끝이 났다. 엘라스틱서치를 실행시켜보자. start.sh 작성 bin/elasticsearch -d -p es.pid
-p 옵션으로 Process ID 저장 stop.sh 작성 kill `cat es.pid`
chmod 755 ""*.sh"" 를 통해 둘 다 권한을 준다.
저장된 Process ID를 통해 간단하게 kill 시킨다. 각 노드별로 start.sh를 실행시키면 엘라스틱서치가 작동된다. 이제 키바나를 설치해서 시각화 작업을 하자. 키바나 나는 node-3 인스턴스에 키바나를 설치했다. config/kibana.yml에 설정 기존 port는 5601. 프로젝트 보안 상 열려있는 port 중에 5601이 없기 때문에 8080으로 할당했다. 엘라스틱서치에 유저와 비밀번호를 설정했기 때문에 이를 키바나에도 적용해야 한다. bin/kibana-keystore create
bin/kibana-keystore add elasticsearch.password
위 이미지와 같이 비밀번호를 설정했다. 키바나는 nodejs 기반이다. cat package.json에 들어가보면 node : “14.17.5” 버전이 나와있음 먼저 nvm을 설치한다. curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
그 후 nvm을 통해 동일 버전 nodejs 다운로드 pm2 설치 npm install pm2 -g
start.sh 작성 pm2 start kibana-7.14.1-linux-x86_64/src/cli/cli.js --name kibana //이름 변경
stop.sh 작성 pm2 stop [app이름]
start.sh를 실행시킨 후 설정에서 server.host에 설정한 IP:port에 접속해서 이름과 비밀번호를 치면 키바나 메인화면으로 간다. 검색(Search) 키바나에 접속해서 devtools로 가면 아래와 같은 화면을 볼 수 있다. 이제 드디어 검색을 위한 환경을 마련했다. 먼저 비교를 통해 RDBMS의 용어 개념과 연관지어보자. RDBMS Elasticsearch 데이터베이스 인덱스 파티션 샤드 테이블 타입 컬럼 필드 행 도큐먼트 인덱스 설정 RDBMS에서 database를 설정하듯 인덱스를 설정해보자. PUT drink
{
""settings"": {
""index"": {
""number_of_shards"": 3,
""number_of_replicas"": 1
}
}
샤드의 개수를 3개로 정하고 복제본이 1개인 설정을 가진 drink 인덱스가 생성되었다. 샤드 : 루씬의 단일 검색 인스턴스로 인덱스는 샤드 단위로 분리되어 각 노드에 분산 저장됨. 현재는 노드당 하나의 샤드가 들어가도록 3개로 설정했다. 분석기 설정 분석기가 있어야 우리가 원하는 형태의 색인이 가능하다. 정리, 색인할 때 특정한 규칙과 흐름에 의해 텍스트를 변경(토큰화)하는 과정을 분석(Analyze)이라고 하며, 해당 처리는 분석기(Analyzer)를 통해서 이루어진다. 프로젝트에선 한글명과 영문명 두 개의 필드를 비교하므로 각각 다른 분석기가 필요하다. 영어명 필드는 snowball 형태소 분석기를 사용하고 한글명 필드는 nori라는 토크나이저를 사용했다. 프로젝트 인덱스 세팅 PUT drink
{
""settings"": {
""index"": {
""number_of_shards"": 3,
""number_of_replicas"": 1
}
,
""analysis"": {
""analyzer"": {
""english_name_analyzer"": {
""type"": ""custom"",
""tokenizer"": ""whitespace"",
""filter"": [""lowercase"", ""stop"", ""snowball""]
}
,
""nori_without_category_analyzer"": {
""type"": ""custom"",
""tokenizer"": ""nori_mixed"",
""filter"": [""nori_filter"", ""category_stop_filter""]
}
,
""nori_with_category_analyzer"": {
""type"": ""custom"",
""tokenizer"": ""nori_mixed"",
""filter"": [""nori_filter""]
}
}
,
""tokenizer"": {
""nori_mixed"": {
""type"": ""nori_tokenizer"",
""decompound_mode"": ""mixed""
}
}
,
""filter"": {
""nori_filter"": {
""type"": ""nori_part_of_speech"",
""stoptags"": [
""IC""
]
}
,
""category_stop_filter"": {
""type"": ""stop"",
""stopwords"": [
""소주"", ""맥주"", ""와인"", ""막걸리"", ""양주"", ""칵테일""
]
}
}
}
}
,
""mappings"": {
""properties"": {
""id"": {
""type"": ""long""
},
""name"": {
""type"": ""text"",
""analyzer"": ""nori_without_category_analyzer""
},
""englishName"": {
""type"": ""text"",
""analyzer"": ""english_name_analyzer""
},
""category"": {
""type"": ""text"",
""analyzer"": ""nori_with_category_analyzer""
}
}
}
}
검색 쿼리 GET drink/_search
{
""query"": {
""bool"": {
""should"": [
{
""match_phrase"": {
""name"": {
""query"": ""금성맥주"",
""slop"": 1,
""boost"": 2
}
}
}
,
{
""match_phrase"": {
""englishName"": {
""query"": ""금성맥주"",
""slop"": 1,
""boost"": 2
}
}
}
,
{
""multi_match"" : {
""query"": ""금성맥주"",
""type"": ""best_fields"",
""fields"": [""name"", ""englishName""],
""tie_breaker"": 0.3
}
}
,
{
""match"": {
""category"": {
""query"": ""금성맥주"",
""boost"": 0.5
}
}
}
]
}
}
}
bool 복합 쿼리를 사용했다. 여러 조건들을 총합해서 검색하는 쿼리로 must, must_not, should, filter 가 있다. 자세한 설명은 아래 참고 자료 Elastic 가이드 북에 있다. 나는 원하는 조건을 맞추기 위해 위에서부터 조건에 맞으면 가중치를 주고 없으면 다음 조건으로 넘어가는 should를 사용했다. must일 시 조건에 해당하지 않으면 false를 반환해버려 A가 아니면 B라는 조건이 성립하지 않는다. 따로 or 조건을 걸어줄 수도 있지만 가중치 boost를 통해 score 조절을 해서 원하는 결과에 가까울수록 점수가 높아지도록 하고 싶었기 때문에 should를 사용했다. 다음 편에서는 이것을 자바 코드에서 어떻게 이용하는지 알아보자. 참고 자료 처음부터 배우는 엘라스틱서치 Elastic 가이드 북 Elasticsearch Gide 7.15",BE
232,"DB 성능 측정 01 Nov 2021 on 조잘조잘 리플리케이션과 인덱스를 적용한 후 성능 측정을 해보자 들어가기전… 안녕하세요 피카입니다. 이전의 글에서 리플리케이션과 인덱스를 적용하여 DB의 성능을 향상 시켰습니다. 이 글에서는 실제로 성능 향상을 수치로 나타내어 얼만큼의 성능이 증가했는지 알아보기 위해 작성한 글입니다. 테스트 환경 DB 구성은 3가지로 나뉘어 테스트합니다. 단일 DB 리플리케이션 적용 리플리케이션 + 인덱스 적용 데이터는 DB에 선호도 데이터 53만건을 넣었습니다. 테스트 도구는 k6로 이루어집니다. 테스트 시나리오는 메인페이지 조회(읽기), 선호도 수정(쓰기) 2가지 시나리오로 구성되어있습니다. k6 결과 출력 테스트를 들어가기 전 k6로 출력되는 결과가 무엇인지 확인할 필요가 있습니다. 자세한 정보는 k6 공식문서 에서 확인할 수 있습니다. 다음은 k6의 결과를 나타내는 지표 예시입니다. 간단히 하나씩 살펴보겠습니다. data_received: 수신된 데이터의 양입니다. data_sent: 전송된 데이터의 양입니다. http_req_blocked: 요청을 시작하기 전에 차단된 시간(사용 가능한 TCP 연결 슬롯 대기)입니다. http_req_connecting: 원격 호스트에 대한 TCP 연결을 설정하는 데 소요된 시간입니다. http_req_duration: 요청에 대한 총 시간입니다. (즉, 초기 DNS 조회/연결 시간 없이 원격 서버가 요청을 처리하고 응답하는 데 걸린 시간) http_req_failed: setResponseCallback 에 따른 실패한 요청 비율입니다. http_req_receiving: 원격 호스트에서 응답 데이터를 수신하는 데 소요된 시간입니다. http_req_sending: 원격 호스트에 데이터를 보내는 데 소요된 시간입니다. http_req_tls_handshaking: 원격 호스트와 TLS 세션을 핸드셰이킹하는 데 소요된 시간입니다. http_req_waiting: 원격 호스트의 응답을 기다리는 데 소요된 시간입니다. http_reqs: k6이 생성된 총 HTTP 요청 수입니다. iteration_duration: 기본/메인 함수의 전체 반복을 완료하는 데 걸린 시간입니다. iterations: 테스트의 VU가 JS 스크립트를 실행한 총 횟수입니다. vus: 현재 활성 가상 사용자 수입니다. vus_max: 최대 가상 사용자 수입니다. 여러가지 많지만 저희는 성능 테스트를 하기 때문에 http_req_duration 또는 http_req_waiting 만 볼 것입니다. 이 글에서는 http_req_waiting을 기준으로 측정하겠습니다. 단일 DB 읽기 시나리오 http_req_waiting: 17.6s 쓰기 시나리오 http_req_waiting: 356.12ms 리플리케이션 적용 읽기 시나리오 http_req_waiting: 10.08s 쓰기 시나리오 http_req_waiting: 363.88ms 인덱스 + 리플리케이션 적용 읽기 시나리오 http_req_waiting: 9.36s 쓰기 시나리오 http_req_waiting: 440.38ms 정리 리플리케이션 적용 후 읽기는 1.74배 (17.6s/10.08s), 쓰기는 0.97배(356.12ms/363.88ms)로 쓰기는 거의 변동이 없고 읽기는 매우 크게 향상된 것을 확인할 수 있습니다. 실제로 리플리케이션은 여러 대의 replica를 통해 분산시켜 읽기 성능을 향상시키는 것을 목적으로 하였으므로 소기 목적을 달성했다고 볼 수 있습니다. 인덱스 + 리플리케이션 적용 후 읽기는 1.88배(17.6s/9.36s), 쓰기는 0.8배(356.12ms/440.38ms)로 성능의 변동이 있습니다. 실제로 인덱스는 쓰기 성능을 희생시켜 읽기 능력을 향상시키기 때문에 이러한 결과가 나왔다고 볼 수 있습니다. 느낀 점 성능 개선을 했지만, 아직 데이터가 많을 경우 10초나 걸리기 때문에 성능 개선이 더 필요하다고 느껴집니다. 따라서 추후에 Optimizer 실행 계획을 분석하는 쿼리 튜닝을 적용해 볼 예정입니다.",BE
233,"리플리케이션 28 Oct 2021 on 조잘조잘 주절주절에 리플리케이션을 적용해보자 목차 리플리케이션 목차 리플리케이션 적용기 들어가기전… Mysql 설치 mysql 설정 mysql source db 설정 mysql replica db 설정 프로세스 확인 테스트 확인 Spring Boot 설정 application.yml 설정 yml 정보 객체 바인딩 Datasource 등록 Source Replica 분기처리 DataSourceConfig 구현 리플리케이션 적용기 들어가기전… 안녕하세요, 피카입니다. 기존 주절주절에는 단일 DB로 데이터를 저장하고 조회하는 방식을 처리했습니다. 해당 DB 구조는 일반적으로 간단한 프로젝트에서 자주 사용하는 구조입니다. 하지만 이러한 구조는 다음과 같은 문제가 있습니다. 하나의 DB로 쓰기와 조회를 동시에 처리하게 됩니다. DB 장애시 DB를 사용하는 모든 기능 동작이 멈추게 됩니다. 하나의 DB가 모든 부하를 감당하게 됩니다. 이러한 문제는 하나의 DB를 사용하기 때문에 생기는 문제입니다. 현재 주절주절 트래픽에서 리플리케이션을 적용할만큼의 성능 이슈가 일어나지 않습니다. 하지만 리플리케이션은 트래픽 뿐만 아닌, 단일 장애점 방지, 백업의 이유도 있습니다. 이러한 이유로 주절주절에 리플리케이션을 적용하기로 결정해였습니다. 리플리케이션은 다음과 같은 장점이 있습니다. 스케일 아웃: 여러 대의 DB를 늘려 성능 처리의 향상을 꾀할 수 있습니다. 데이터 백업: 다수의 DB에 데이터를 동기화시켜 저장해놓기 때문에 데이터 백업이 자연스럽게 됩니다. 단일 장애점 방지: 여러 대의 Replica DB를 두기 때문에 하나의 Replica DB가 고장나도 기능을 수행할 수 있습니다. 프로젝트에 적용한 구조는 하나의 Source(source Database)와 두 개의 Replica(Replica Database)로 구성되어 있습니다. 각 데이터 베이스는 MySql 8.0.27을 사용하였습니다. 기본 환경은 ubuntu 18.04 입니다. Mysql 설치 주절주절에서는 mysql 8.0버전을 사용합니다. 기존 mysql을 설치할 경우 5.7버전을 그대로 설치하므로 8.0을 설치하기 위해 다른 명령어를 입력해야합니다. //apt를 업데이트 하기 위한 데이터를 받아온다.
sudo wget https://dev.mysql.com/get/mysql-apt-config_0.8.15-1_all.deb
sudo dpkg -i mysql-apt-config_0.8.15-1_all.deb
OK를 선택해줍니다. //위에서 다운로드 받은 데이터를 기반으로 업데이트한다.
sudo apt-get update
//8.0버전을 설치한다.
sudo apt-get install mysql-server
설치를 마치면 비밀번호를 입력하는 창으로 넘어갑니다. 비밀번호를 입력합니다. mysql 설치가 완료가 됩니다. 다음 명령어를 통해 mysql에 접속합니다. mysql 실행 sudo mysql -u root -p
mysql 설정 기본적으로 리플리케이션은 source와 replica 두가지 DB로 나뉩니다. 따라서 각각 설정을 해주어야합니다. mysql source db 설정 /etc/mysql/mysql.conf.d/mysqld.cnf 파일을 열어 server-id와 log_bin 파일을 설정해줍니다. (source) server-id(1~2^32-1 범위)는 개별 db서버를 인식하는데 사용되며 기본 값은 0입니다. source db는 1, replica-1는 2, replica-2는 3…과 같은 방식으로 구별합니다. mysql은 bin_log 파일을 통해 source 진행된 상황을 replica 읽어 동기화를 합니다. 따라서 log_bin 파일의 위치를 설정해주어야합니다. 기본적으로 모든 데이터베이스를 리플리케이션하지만 binlog_do_db를 설정해주면 해당 데이터베이스만을 리플리케이션합니다. 만약 추가로 데이터 베이스를 추가하고 싶다면 binlog_do_db를 계속 추가해주면 됩니다. (ignore은 제외시킬 데이터베이스) 참고: 현재 우아한테크코스에서 제공해주는 AWS는 보안상의 이유로 몇몇 포트를 막아놓았습니다. 따라서 기본 포트를 8000번으로 바꾸어주고 mysql에서 외부접속을 허용하도록 bind-address를 0.0.0.0으로 변경해주어야합니다. 설정을 적용시키기 위해 mysql을 다시 시작합니다. (source) $ sudo service mysql restart
데이터 베이스를 만들어줍니다. 각 리플리케이션은 사용자 계정을 통해 연결되므로 사용자를 만들어 주어야합니다. (source) 참고: 'userid'@'%' 에서 '%' 는 외부에서 접근을 허용 mysql> CREATE USER 'repl'@'%.example.com' IDENTIFIED BY 'password';
해당 계정에 모든 권한을 줘도 됩니다. 만약 불안하다면 replication 권한만 줄 수 있습니다. (source) ```sql mysql> GRANT REPLICATION SLAVE ON . TO ‘repl’@’%.example.com’; // 권한 적용 mysql> flush privileges;
![](/assets/img/2021-10-04-00-23-20.png)

> 참고: replication 권한을 설정할 때 {database}.* 으로 설정하게되면 `SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT, CREATE USER` 옵션은 글로벌 옵션이기 때문에 *.* 전체 타깃으로만 사용이 가능하다.

![](/assets/img/2021-10-04-00-23-47.png)

6. source db 정보를 확인합니다. (source)

```sql
mysql> show master status;
File(mysql-bin.000001)과 Position(939)를 기억해야합니다. 이 정보는 후에 replica 설정할 때 사용합니다. 각각은 로그 파일의 이름과 파일내의 위치를 나타냅니다. 자세한 정보는 공식 문서 binary log 를 통해 확인할 수 있습니다. mysql replica db 설정 /etc/mysql/mysql.conf.d/mysqld.cnf 파일을 열어 server-id를 설정해줍니다. (replica) 설정을 적용시키기 위해 mysql을 다시 시작합니다. (replica) $ sudo service mysql restart
데이터 베이스를 만들어줍니다. (replica) Replica와 source DB를 연결합니다. (replica) mysql> change master to
-> master_host={master_db_ip},
-> master_port={master_db_port},
-> master_user={master_username},
-> master_password={master_password},
-> master_log_file={master_bin_file},
-> master_log_pos={position};
replica를 실행 해줍니다. (replica) mysql> start slave;
show slave statusG; 명령어를 치면 replica 상태와 source 연결 정보를 확인할 수 있습니다. Slave_IO_State가 Waiting for master to send event 상태가 되어야 합니다. error connecting to master 에러가 뜰 경우 참고 error connecting to master 'repl@13.125.200.56:8000' - retry-time: 60 retries: 1와 같은 에러 메시지가 뜰 수 있습니다. 이럴 경우 source 서버에 접근하지 못하는 상태입니다. 따라서 source 서버에 접근할 수 있도록 ip나 포트를 제대로 설정해주어야합니다. 프로세스 확인 각각 source와 replica mysql에 show processlistG; 명령어를 통해 프로세스로 연결되어있는 것을 확인할 수 있습니다. source replica 테스트 확인 실제 source DB에 테이블을 만들고 값을 넣게 되면 replica DB에 값이 들어가는 것을 확인할 수 있습니다. source DB replica DB source DB에 member 테이블과 (1, pika) 데이터를 넣었을 때, replica DB에도 같이 적용되는 것을 확인할 수 있습니다. Spring Boot 설정 application.yml 설정 datatsource가 여러개이기 때문에 자동 설정을 사용할 수 없다 따라서 application.yml 또는 application.properties에 사용할 DB 정보를 설정해줍니다. spring:
config:
activate:
on-profile: prod
jpa:
database: mysql
database-platform: org.hibernate.dialect.MySQL8Dialect
properties:
hibernate:
show_sql: true
format_sql: true
use_sql_comments: true
datasource:
driver-class-name: com.mysql.cj.jdbc.Driver
url: jdbc:mysql://source_ip주소:3306/jujeol?useSSL=true
username:
password:
replica1:
url: jdbc:mysql://replica1_ip주소:3306/jujeol?useSSL=true
username:
password:
replica2:
url: jdbc:mysql://replica2_ip주소:3306/jujeol?useSSL=true
username:
password:
yml 정보 객체 바인딩 CustomDataSourceProperties.java @Getter
@Setter
@ConfigurationProperties(prefix = ""spring.datasource"")
public class CustomDataSourceProperties {

private final Replica replica1 = new Replica();
private final Replica replica2 = new Replica();

private String name = ""source"";
private String url;
private String username;
private String password;

@Getter
@Setter
public static class Replica {
private String name;
private String url;
private String username;
private String password;
}
}
앞서 yml에 설정한 정보는 사용자 임의로 정의해준 형식이기 때문에 해당 정보를 java 코드로 사용할 수 있도록 ConfigurationProperties를 사용하여 바인딩해주어야합니다. 해당 어노테이션은 내부적으로 getter, setter를 사용하므로 필수적으로 getter, setter 메서드가 있어야합니다. Datasource 등록 자동으로 Datasource를 등록해주었던 이전과는 다르게 등록해줘야할 Datasource가 여러개이므로 수동으로 등록해주어야합니다. @Configuration
@EnableAutoConfiguration(exclude = {DataSourceAutoConfiguration.class}) // 자동 등록 제외
@EnableConfigurationProperties(CustomDataSourceProperties.class)
public class DatasourceConfig {

private static final String MYSQL_JDBC_DRIVER = ""com.mysql.cj.jdbc.Driver"";

private final CustomDataSourceProperties databaseProperty;

public DatasourceConfig(CustomDataSourceProperties databaseProperty) {
this.databaseProperty = databaseProperty;
}

}
@EnableAutoConfiguration(exclude = {DataSourceAutoConfiguration.class})으로 기존에 자동으로 설정하는 부분을 제외해줍니다. @EnableConfigurationProperties(CustomDataSourceProperties.class)으로 CustomDatasource를 등록해줍니다. Source Replica 분기처리 public class ReplicationRoutingDataSource extends AbstractRoutingDataSource {

private boolean replicaSelector = false;

@Override
protected Object determineCurrentLookupKey() {
boolean isReadOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly();
if (isReadOnly) {
DatabaseReplicaType replicaType = chooseNextReplica();
log.info(""Select replica DB : {}"", replicaType);
return replicaType;
} else {
log.info(""Select source DB : {}"", SOURCE);
return SOURCE;
}
}

private DatabaseReplicaType chooseNextReplica(){
replicaSelector = !replicaSelector;
return replicaSelector ? REPLICA1 : REPLICA2;
}
}
@Transactional(readOnly=true)인 경우 replica datasource로, 나머지는 source datasource로 분기 처리를 하기위한 ReplicationRoutingDataSource 클래스를 생성합니다. determineCurrentLookupKey()메서드는 현재 요청에서 사용할 datasource의 key값을 반환해줍니다. chooseNextReplica에서 boolean값을 통해 어떤 replica를 사용할지 정해집니다. 현재는 2개의 DB만 사용하고 있어 boolean 값을 사용하였지만 추후 3개 이상의 DB를 사용할 경우 다른 방법으로 구현해야할 것입니다. 이렇게 한 이유는 circular 방식으로 할 경우 thread safe하지 않아 에러가 날 수 있기 때문입니다. 실제로 돌려보게되면 50대 50으로 정확히 나오지는 않고 오차가 어느정도 있지만 감안할 정도입니다. DataSourceConfig 구현 @Configuration
@EnableAutoConfiguration(exclude = {DataSourceAutoConfiguration.class})
@EnableConfigurationProperties(CustomDataSourceProperties.class)
public class DatasourceConfig {

private static final String MYSQL_JDBC_DRIVER = ""com.mysql.cj.jdbc.Driver"";

private final CustomDataSourceProperties databaseProperty;

public DatasourceConfig(CustomDataSourceProperties databaseProperty) {
this.databaseProperty = databaseProperty;
}

@Primary
@Bean
public DataSource dataSource() {
return new LazyConnectionDataSourceProxy(routingDataSource());
}

@Bean
public DataSource routingDataSource() {
Map<Object, Object> dataSources = createDataSources(databaseProperty);

ReplicationRoutingDataSource replicationRoutingDataSource = new ReplicationRoutingDataSource();
replicationRoutingDataSource.setDefaultTargetDataSource(dataSources.get(SOURCE));
replicationRoutingDataSource.setTargetDataSources(dataSources);

return replicationRoutingDataSource;
}

private Map<Object, Object> createDataSources(CustomDataSourceProperties databaseProperty) {
Map<Object, Object> dataSourceMap = new LinkedHashMap<>();
dataSourceMap.put(SOURCE,
createDataSource(databaseProperty.getUrl(),
databaseProperty.getUsername(),
databaseProperty.getPassword())
);

dataSourceMap.put(REPLICA1,
createDataSource(databaseProperty.getReplica1().getUrl(),
databaseProperty.getReplica1().getUsername(),
databaseProperty.getReplica2().getPassword())
);

dataSourceMap.put(REPLICA2,
createDataSource(databaseProperty.getReplica2().getUrl(),
databaseProperty.getReplica2().getUsername(),
databaseProperty.getReplica2().getPassword())
);

return dataSourceMap;
}

public DataSource createDataSource(String url, String username, String password) {
return DataSourceBuilder.create()
.type(HikariDataSource.class)
.url(url)
.driverClassName(MYSQL_JDBC_DRIVER)
.username(username)
.password(password)
.build();
}

}
LazyConnectionDataSourceProxy를 사용하는 이유 일반적으로 Spring에서는 DataSource를 하나만 사용하기 때문에 시작전에 정해 놓습니다. 그러나 리플렉션을 사용할 경우 DataSource가 아직 정해지지 않았으므로 Lazy 전략을 사용하여 실제 쿼리가 실행 될 때 DataSource를 가져옵니다. 네이밍(카멜 케이스? 언더바?) DataSourceAutoConfiguration를 사용하지 않기 때문에 자동으로 설정되던 부분이 빠졌습니다. 그 중 하나가 네이밍인데 자동 설정을 했을 경우 언더바를 자동으로 붙여주지만 그렇지 않을 경우 카멜케이스로 네이밍을 하게 됩니다. 따라서 프로퍼티 설정에 spring.jpa.properties.hibernate:physical_naming_strategy: org.springframework.boot.orm.jpa.hibernate.SpringPhysicalNamingStrategy을 설정해주어야합니다.",BE
234,"Spring Boot 모니터링 적용 - 2 28 Oct 2021 on 조잘조잘 Spring Boot 어플리케이션 모니터링 적용기 1편에 이어서 모니터링 구성을 끝내려 한다. 메트릭 시각화는 완료했으므로 로그를 추가할 것이다. 모니터링을 구상하면서 가장 필요한 항목 중 하나가 로그였다. 오류가 발생할 때마다 직접 EC2 서버에 접근해 cat log 를 반복하기를 몇 주.. Grafana 에 다 통일해버리겠다는 욕심을 가지고 시작했다. step1 처음으로 시도한 방법은 actuator 에 뭔가 다른 엔드포인트를 줘서 가져오기. /actuator/logfile 로 받아오게 만들고 싶었는데, 이러려면 application.yml 설정을 손보면 된다. 이렇게 작성하면 서버/actuator/logfile 에서 로그를 확인할 수 있다. 하지만 내보낸 로그를 그라파나에서 사용할 방법이 없다.ㅎ 참고 http://forward.nhnent.com/hands-on-labs/java.spring-boot-actuator/03-configuration.html#web step2 logfile 엔드포인트는 기본적으로 JMX API 이다. step1 처럼 logging file 을 설정해주면 웹으로도 확인할 수 있지만, Jolokia 를 사용하는 방법도 있다. Jolokia 는 JMX-HTTP 브리지 역할을 하는 툴이다. Joloki 를 사용하면 JMX API 를 HTTP 프로토콜을 이용하여 요청할 수 있게 해주며, 응답을 JSON 형태로 받을 수 있다. 하지만 step1 의 결과와 다를 바가 없어보여 그만두었다. step3 로그 모니터링 방법을 찾아보던 중 발견한 것이 Grafana Loki 이다. Loki 는 Grafana 에서의 로그 출력을 위한 데이터 소스인데, 성능도 괜찮고 쿠버네티스에 특화되어 있다고 한다. Grafana 6.0 부터 기본적으로 지원한다. 처음에는 도커로 설치해서 사용하려고 했다. 맨 먼저 시도한 방법은 docker-compose 설치인데, 원하는 로그를 수집하기 위해서는 Promtail 설정을 여러 개 줘야하므로 promtail-config.yaml 을 반영하기 힘든 이 방법은 실패했다. (Promtail 은 로그를 수집해서 Loki 에 전달하는 에이전트이다.) step4 도커 컴포즈가 실패해서, 그냥 도커 설치 방법을 시도했다. wget https://raw.githubusercontent.com/grafana/loki/v2.3.0/cmd/loki/loki-local-config.yaml -O loki-config.yaml
docker run -v $(pwd):/mnt/config -p 3100:3100 grafana/loki:2.3.0 -config.file=/mnt/config/loki-config.yaml
wget https://raw.githubusercontent.com/grafana/loki/v2.3.0/clients/cmd/promtail/promtail-docker-config.yaml -O promtail-config.yaml
docker run -v $(pwd):/mnt/config -v /var/log:/var/log grafana/promtail:2.3.0 -config.file=/mnt/config/promtail-config.yaml
이 방법의 경우.. $(pwd) 이 부분이 경로에 한글이 있으면 제대로 실행되지 않는 문제가 있다. 문제를 해결한 뒤 시도했으나, Auth.docker.io on 192.168.65.1:53: no such host 이런 에러가 발생하면서 계속 Promtail 실행에 실패했다. step5 설치 및 실행 그래서 로컬로 설치하기로 했다. wget https://github.com/grafana/loki/releases/download/v2.3.0/loki-linux-amd64.zip
wget https://github.com/grafana/loki/releases/download/v2.3.0/promtail-linux-amd64.zip
먼저 실행파일을 설치한 뒤, 설정 파일도 다운로드 받는다. wget https://raw.githubusercontent.com/grafana/loki/master/cmd/loki/loki-local-config.yaml
wget https://raw.githubusercontent.com/grafana/loki/main/clients/cmd/promtail/promtail-local-config.yaml
저 config 파일들을 입맛대로 수정할 수 있어 가장 만족스러웠다. ./loki-linux-amd64 -config.file=loki-local-config.yaml 로 Loki 를 띄우고, ./promtail-linux-amd64 -config.file=promtail-local-config.yaml 로 대상 파일과 연결한다. Loki를 띄우는 과정에서 failed parsing config: loki-local-config.yaml: yaml: unmarshal 이런 식으로 시작하는 에러가 발생하는데, master 브랜치에서 받느라 생긴 문제이다. 저 자리에 버전을 명시해주면 잘 실행된다. wget https://raw.githubusercontent.com/grafana/loki/v2.3.0/cmd/loki/loki-local-config.yaml
Grafana 설정 Loki 와 Promtail 을 각각 실행한 뒤, Grafana 에 Data Source 로 Loki 를 추가해줘야 한다. Prometheus 추가했듯이 쉽게 할 수 있다. 대신 저 포트를 이후 바꾸게 되는데… Grafana 에 패널을 추가한다. Data Source 를 Loki 로 잘 선택하고, Promtail 설정에 따라 filename 또는 jobs 로 쿼리를 생성한다. server:
http_listen_port: 9080
grpc_listen_port: 0

positions:
filename: /tmp/positions.yaml

clients:
- url: http://loki:3100/loki/api/v1/push

scrape_configs:
- job_name: system
static_configs:
- targets:
- localhost
labels:
job: varlogs
__path__: /var/log/*log
초기의 promtail-local-config.yaml 파일은 이런 모습인데, clients 에 실행중인 Loki url 이 정확히 입력되어야 하고, loki/api/v1/push 는 promtail 로 수집한 로그를 Loki 에 푸쉬하는 POST 요청 api 이다. (/api/prom/push 는 deprecated 되었다.) scrape-configs 에 job 을 더 추가할 수 있다. - job_name: mylog
static_configs:
- targets:
- localhost
labels:
job: mylog
__path__: {경로}/mylog.log
이렇게 다른 경로의 로그를 Grafana 로 조회하고 싶을 때 함께 전송한다. 이 방법을 그대로 개발 서버에 적용하면서 이슈가 하나 있는데, Grafana 가 설치된 서버와 실제 로그가 쌓이는 서버가 다르다는 것. (Loki 와 Grafana, Promtail 이 모두 같은 서버에 있다면 기본 설정만으로도 Grafana 에서 바로 로그가 조회된다.) 처음에는 권한을 줘야할 것이라고 생각해서 API token 관련해서 먼저 찾아보았다. (참고1, 참고2, 참고3) server:
http_listen_port: 9080
grpc_listen_port: 0

positions:
filename: /tmp/positions.yaml

clients:
# 이 자리에 삽입
- url: <http://{name}:{token}@loki:3100/loki/api/v1/push>

scrape_configs:
- job_name: system
static_configs:
- targets:
- localhost
labels:
job: varlogs
__path__: /var/log/*log
토큰 넣는 방법을 시도해보긴 했는데, 별로 의미는 없었다. 그리고 개인 계정 토큰은 별로 의미가 없고, ...

clients:
- url: http://{user-name}:{password}@loki:3100/loki/api/v1/push

...
name-password 쌍으로 넣어야 Invalid User Name or Password 이런 에러가 뜨지 않았다. 아마 Loki 관련한 권한 자체를 제한하면 name-password 를 필수로 설정할 수 있지 않을까 싶다. 다른 서버에 실행한 Loki 로 보내는 방법은.. 어쨌든 Loki 도 서버가 실행되기 때문에, 그 위치로 요청을 보내면 된다. ...

clients: # url 을 변경한다.
- url: http://{요청 보낼 Loki 서버 url}:{요청 보낼 Loki port}/loki/api/v1/push

scrape_configs:
- job_name: mylog
static_configs:
- targets:
- {요청 보낼 Loki 서버 url}
labels:
job: mylog
__path__: {경로}/mylog.log
내가 원하는 특정한 위치를 url 로 입력하고, targets 부분도 변경했다. Loki 포트의 경우는, loki-local-config.yaml 파일을 열어서 auth_enabled: false

server:
http_listen_port: {원하는 포트, 기본 3100}
grpc_listen_port: 9096

...
http_listen_port 를 원하는 포트로 바꿔준다. 초기값은 3100 이다. 각각의 설정을 마친 뒤에는 돌고 있던 Loki 와 Promtail 을 종료하고 다시 실행한다. # Loki
./loki-linux-amd64 -config.file=loki-local-config.yaml
# Promtail
./promtail-linux-amd64 -config.file=promtail-local-config.yaml
이 밖의 이런저런 에러들 connect: connection refused : Loki 가 아직 실행 전일 때 발생한다. http://{loki url}:{port}/ready 로 요청을 보냈을 때, ready 라고 응답이 와야 한다. 로그 삽입 중 뭔가 발생하면 Promtail msg=""final error sending batch"" 이런 식의 에러를 자주 맞닥뜨리는데… promtail 설정의 url 에 틀린 부분이 있을 경우 404 에러가 발생한다. error: context exceeded 로그 이름을 제대로 입력 안해줘서 생긴 문제였다. (폴더명만 준다거나…)",BE
235,"Spring Boot 모니터링 적용 - 1 28 Oct 2021 on 조잘조잘 Spring Boot 어플리케이션 모니터링 적용기 도입 목적 이미 서버 모니터링은 Cloud Watch 를 통해 하고 있었으나, Cloud Watch Agent 가 cron 명령어로 일정 시간마다 보내기 때문에 실시간 확인이 어려웠다. 예전 스프린트 때 서버가 다운됐는데 바로 파악이 안되어 이미 필요성이 한번 제기되었으나 계속 우선순위에서 밀리다가 이번에 본격적으로 작업하게 되었다. 본 목적은 서버 메트릭을 실시간으로, 가시성 높게 모니터링하려는 것이었으나, 로그 수집도 포함하게 되었다. 로그 모니터링 과정은 2편으로 따로 서술하겠다. Spring Boot Actuator + Prometheus + Grafana Spring Boot Actuator Spring Boot은 이미 모니터링과 관리를 위한 도구가 있는데, Actuator 가 그것이다. HTTP 또는 JMX로 엔드포인트를 설정하여 이 정보를 기반으로 모니터링할 수 있다. 메트릭과 헬스체크, 오디팅Audting 수집도 자동으로 적용할 수 있다. Prometheus Prometheus는 이벤트 모니터링 및 경고에 사용되는 오픈 소스 프로그램이다. 유연한 쿼리를 제공하고, HTTP 풀 모델을 사용하여 구축 된 시계열 데이터베이스에 실시간 메트릭을 기록한다.(참고) Grafana Grafana는 시계열 데이터 시각화 오픈 소스이다. 여러 DB를 지원하고, 자유롭게 대시보드를 구성할 수 있다. 쿼리 작성도 지원하고 템플릿을 다운로드 받아 구성할 수도 있어 유용하게 사용할 수 있다. 사전설정 build.gradle 에 의존성을 추가한다. implementation 'org.springframework.boot:spring-boot-starter-actuator'
implementation 'io.micrometer:micrometer-registry-prometheus'
application.yml 설정을 변경했다. spring:
application:
name: {my-app-name}
management:
endpoints:
web:
exposure:
include: ""prometheus""
metrics:
tags:
application: ${sprring.application.name}
prometheus 에 endpoint 를 노출하겠다는 의미이다. include 부분에 beans, health 등등.. 어플리케이션 상태를 체크할 다양한 소스를 추가할 수 있다. 하지만 모든 정보를 노출해서는 안되기 때문에 Spring Security 를 사용하는 등 여러 보안 대책이 필요하다. Prometheus와 Grafana 설치 및 실행 homebrew 를 사용한 설치 및 실행 brew install prometheus
brew install grafana
brew services start prometheus
brew services start grafana
promethues.yml 위치 /opt/homebrew/etc Grafana 포트 변경 위치 /opt/homewbrew/etc/grafana.ini ubuntu 환경에서는 아래와 같이 진행한다. # Grafana 설치
wget https://dl.grafana.com/oss/release/grafana-{[버전](https://grafana.com/docs/grafana/latest/installation/debian/)}.linux-amd64.tar.gz
tar -zxvf grafana-6.7.2.linux-amd64.tar.gz

# Prometheus 설치
wget https://github.com/prometheus/prometheus/releases/download/{[버전](https://prometheus.io/download/)}/prometheus-2.17.1.linux-amd64.tar.gz
tar -xzf prometheus-2.17.1.linux-amd64.tar.gz
Prometheus 는 추가 설정이 필요하다. prometheus.yml 을 아래와 같이 수정한다. global:
scrape_interval: 15s

scrape_configs:
- job_name: ""prometheus""
static_configs:
- targets: [""localhost:9090""]

# 여기가 새로 추가한 부분
- job_name: ""springboot""
metrics_path: ""/actuator/prometheus""
static_configs:
- targets: [""{스프링 부트 서버}""]
각각 실행한다. cd grafana-6.7.2.linux-amd64
./grafana

cd prometheus-2.17.1.linux-amd64
./prometheus
Prometheus 의 기본 포트는 9090, Grafana 는 3000이다. 실행 포트 변경 Grafana 설치 경로에서 conf/default.ini 파일을 수정한다. # The http port to use
;http_port = {원하는 포트}
Prometheus 의 포트를 변경하려면 실행할 때 옵션을 준다. (참고) ./prometheus --web.listen-address=:{원하는 포트}
포트변경을 하지 않았다면 각각 localhost:9090 , localhost:3000 으로 연결해 확인할 수 있다. Prometheus 실행 화면 현재 돌아가고 있는 상태도 확인할 수 있다. 중간에 어플리케이션을 종료하면 이렇게 상태가 변한다. 메트릭 확인 원하는 메트릭을 그래프, 테이블 형태로 확인할 수 있다. 아래는 JVM 프로세스의 최근 CPU 사용량을 확인한 모습이다. 서버에 요청을 몇번 보냈더니 CPU 사용량이 확 뛰었다. Grafana 대시보드 기본 구성 Grafana 의 기본 id, pw 는 admin, admin 이다. 접속해서 바로 비밀번호를 변경한다. (설정의 Users 항목에서 추가 유저를 생성하고 권한을 부여할 수 있다.) 홈 화면에 진입한 뒤 Prometheus 를 데이터소스로 추가한다. URL 은 지금 Prometheus가 떠있는 곳, 현재는 기본값인 http://localhost:9090 로 연결했다. 이렇게 잘 추가된 것을 확인할 수 있다. 연결했으니 대시보드를 만들어야 한다. 일단 빈 패널을 생성해서 시도해보려고 한다. 먼저 사용할 데이터소스를 선택하고, PromQL(Prometheus Query Language)을 작성해서 어떤 지표를 조회할지 작성해야 하는데, 어느정도 지원한다. system_cpu_usage 메트릭을 확인할 것이고, 어떤 어플리케이션과 잡을 선정할 것인지도 고르면 된다. 그러면 알아서 쿼리를 생성해준다! Use query 를 눌러서 추가하면, 시각화된 모습을 확인할 수 있다. 대시보드에 패널 하나가 추가되었다. Template을 적용한 대시보드 Grafana 에서 다른 사람들이 구성한 템플릿을 사용할 수 있다. (https://grafana.com/grafana/dashboards/) 마음에 드는 템플릿을 json 형식으로 다운받는다. 우리 프로젝트는 이것을 사용하고 있다. 대시보드 탭에서 Import 를 눌러 다운받은 파일로 구성하면 끝 대신 구성하면서 변수가 제대로 할당이 되지 않는 문제가 있었다. jvm_classes_loaded_classes 부분이 최초에는 jvm_classes_loaded 로 되어있어 제대로 된 metric이 아니었던 것이다. 로컬에서 직접 /actuator/prometheus 에 어떤 값이 오는지 확인하고 바꿔서 넣어줬다. 이 밖에도 템플릿과 일치하지 않는 부분은 직접 수정하여 대시보드 구성을 완료했다. 종종 수정 버튼이 없는 패널이 있는데, More > Panel JSON 에서 쿼리를 수정하면 된다. 참고자료 대부분 공식문서를 통해 손쉽게 진행할 수 있다. Prometheus, Grafana 연동 - https://meetup.toast.com/posts/237",BE
253,"QueryDSL 사용기 28 Oct 2021 on 조잘조잘 QueryDSL을 사용해보자. 오픈 소스 프로젝트이자 type-safe한 쿼리를 위한 Domain Specific Language이다. Type safe란? 기존 JPQL은 String으로 쿼리 문자열을 작성하여 오타나 잘못된 부분이 있는지 확인이 어렵다. 프로그램을 실행하고 나서야 오류 확인이 가능한 것도 단점 중 하나. 게다가 데이터베이스의 스키마가 변경된다면 프로그램 안에서 그 부분과 관련된 쿼리 문자열이 모두 수정되어야 한다. JPA의 criretia도 필드이름을 String으로 넘기는 방식이라 type safe하지 않다. 또한 문법이 복잡하고. QueryDSL은 QEntity를 이용하여 도메인 객체에 수정이 일어나면 쿼리 코드에 컴파일 에러가 발생하게 된다. 컴파일 에러가 발생하는 코드를 수정하면 적어도 소프트웨어 동작 중에 쿼리가 잘못되어 에러가 발생하는 경우는 없다고 봐도 된다. 이런 것을 Type safe하다고 한다. 특징 컴파일 타임에 오류 체크 가능 동적쿼리를 Criteria API 보다 직관적으로 표현 가능 문자가 아니라 코드로 작성 가능 IDE의 도움을 받을 수 있다. JPA가 지원해주진 않아서 별도로 의존성 추가 필요 Gradle 설정 plugins{
id ""com.ewerk.gradle.plugins.querydsl"" version ""1.0.10""
}

dependencies {
implementation 'com.querydsl:querydsl-jpa'
}

//querydsl 추가 시작
def querydslDir = ""$buildDir/generated/querydsl""

querydsl {
jpa = true
querydslSourcesDir = querydslDir
}

sourceSets {
main.java.srcDir querydslDir
}
//querydsl 추가 끝
이렇게 설정해주면 된다. 다만 이 Gradle 플러그인은 1.0.10을 마지막으로 업데이트가 없다. Gradle 4.6에서 ‘Annotaion Processor’가 소개되고, 이를 반영한 Gradle 5.x가 출시됐을 때는 정상으로 작동하지 않는다. 그래서 이 부분이 따로 추가되었다. compileQuerydsl {
options.annotationProcessorPath = configurations.querydsl
}
querydsl-apt 에 있는 AnnotationProcessor 의 경로를 설정해준다. Gradle 6.x 에서는 아래 코드를 추가해주면 작동한다고 한다. configurations {
//아래를 지정하지 않으면, compile 로 걸린 JPA 의존성에 접근하지 못한다.
querydsl.extendsFrom compileClasspath
}
현재 주절주절 팀의 Gradle 설정은 이렇게 하나씩 추가된 설정이 다 들어가 있다. 최종 설정 plugins{
id ""com.ewerk.gradle.plugins.querydsl"" version ""1.0.10""
}

dependencies {
implementation 'com.querydsl:querydsl-jpa'
}

//querydsl 추가 시작
def querydslDir = ""$buildDir/generated/querydsl""

querydsl {
jpa = true
querydslSourcesDir = querydslDir
}

sourceSets {
main.java.srcDir querydslDir
}

compileQuerydsl {
options.annotationProcessorPath = configurations.querydsl
}

configurations {
//아래를 지정하지 않으면, compile 로 걸린 JPA 의존성에 접근하지 못한다.
querydsl.extendsFrom compileClasspath
}
//querydsl 추가 끝
2년 전의 Gradle 플러그인을 최신 버전에서 작동 시키려고 하나씩 설정을 추가하는 방법이다. 현재 잘 동작하고 있으니 다른 방법은 간단하게 소개만 하겠다. Annotation Processor 설정(참고) configure(querydslProjects) {
apply plugin: ""io.spring.dependency-management""

dependencies {
compile(""com.querydsl:querydsl-core"")
compile(""com.querydsl:querydsl-jpa"")

annotationProcessor(""com.querydsl:querydsl-apt:${dependencyManagement.importedProperties['querydsl.version']}:jpa"") // querydsl JPAAnnotationProcessor 사용 지정
annotationProcessor(""jakarta.persistence:jakarta.persistence-api"") // java.lang.NoClassDefFoundError(javax.annotation.Entity) 발생 대응
annotationProcessor(""jakarta.annotation:jakarta.annotation-api"") // java.lang.NoClassDefFoundError (javax.annotation.Generated) 발생 대응
}

// clean 태스크와 cleanGeneatedDir 태스크 중 취향에 따라서 선택.
/** clean 태스크 실행시 QClass 삭제 */
clean {
delete file('src/main/generated') // 인텔리제이 Annotation processor 생성물 생성위치
}

/**
* 인텔리제이 Annotation processor 에 생성되는 'src/main/generated' 디렉터리 삭제
*/
task cleanGeneatedDir(type: Delete) { // 인텔리제이 annotation processor 가 생성한 Q클래스가 clean 태스크로 삭제되는 게 불편하다면 둘 중에 하나를 선택
delete file('src/main/generated')
}
}
변경된 Gradle Annotation Processor 설정을 이용하면 별다른 설정을 하지 않아도 annotationProcessor로 선언한 라이브러리의 적절한 AnnotationProcessor를 선택하여 사용 lombok도 이와 유사하게 사용 가능함 참고 : http://honeymon.io/tech/2020/07/09/gradle-annotation-processor-with-querydsl.html 설정한 뒤에 IntelliJ 우측의 Gradle을 열어 Tasks → other → compileJava 를 실행시키면 Q클래스가 생성된다. 주절주절 팀의 경우 build/generated/querydsl 패키지에 생성되었다. 혹은 Gradle Project (View -> Tool Windows -> Gradle Project)에서 Tasks -> other -> compileJava를 실행 (기본적으로 src/main/generated에 경로 설정) Config 설정 설정값을 모아둔 패키지에 QuerydslConfiguration 생성. @Configuration
public class QuerydslConfiguration {

@PersistenceContext
private EntityManager entityManager;

@Bean
public JPAQueryFactory jpaQueryFactory() {
return new JPAQueryFactory(entityManager);
}
}
이 설정을 통해 JPAQueryFactory를 주입 받아 Querydsl을 사용 가능하게 됐다. 기본적인 사용법 현재 JPA Repository를 사용하고 있으므로 거기에 Querydsl을 추가하는 방법으로 가겠다. public interface DrinkRepository extends JpaRepository<Drink, Long> {

...

}
여기에 따로 Querydsl용 Repository를 따로 생성한다. import static com.jujeol.drink.domain.QDrink.drink; // (2)

import com.jujeol.drink.domain.Drink;
import com.querydsl.jpa.impl.JPAQueryFactory;
import java.util.List;
import org.springframework.data.jpa.repository.support.QuerydslRepositorySupport;
import org.springframework.stereotype.Repository;

@Repository
public class DrinkRepositorySupport extends QuerydslRepositorySupport {

private final JPAQueryFactory queryFactory;

public DrinkRepositorySupport(JPAQueryFactory queryFactory) {
super(Drink.class);
this.queryFactory = queryFactory;
}

public List<Drink> findByName(String name) {
return queryFactory
.selectFrom(drink) // (1)
.where(drink.name.name.eq(name)) //(3)
.fetch();
}
}
설정에서 Bean으로 등록한 JPAQueryFactory를 주입 받아 사용 (1) : 여기서 drink를 쓰면 오류가 날 수 있다. 그렇다면 옆에 Gradle에서 Tasks -> other -> compileQuerydsl를 실행. 끝나면 프로젝트에 있는 모든 Entity의 QClass 생성 (2) : 생성된 QClass import 한 것 (3) : sql의 where와 같다. 자바 코드처럼 .equals() 느낌이라고 보면 된다. 추후에 기능들에 대해 설명하겠다. 이제 JPA의 DrinkRepository처럼 DrinkRepositorySupport를 Service에서 필드로 가지고 사용하면 된다. 이 방법도 뭐 나쁘진 않지만 Repository 두 개를 선언해야 하는 것이 마음에 들지 않는다. Spring Data Jpa Custom Repository 적용 두 개로 나뉜 Repository를 하나로 합치고 싶다. 다행히 Spring Data Jpa에서는 Custom Repository를 JpaRepository 상속 클래스에서 사용할 수 있도록 기능을 지원해준다. Spring Data 공식 문서 DrinkRepository 에서 DrinkRepositoryImpl 에 있는 코드도 사용 가능. Custom이 붙은 인터페이스를 상속한 Impl이 붙은 클래스의 코드는 Custom 인터페이스를 상속한 JpaRepository에서 사용 할 수 있다. Custom 과 Impl만 공식처럼 외우고 클래스를 만들어도 된다.
DrinkRepository와 같은 패키지에 DrinkCustomRepository 인터페이스와 DrinkRepositoryImpl 클래스를 생성해준다. 그 뒤에 DrinkCustomRepository에 만들고 싶은 메서드를 추가한다. Support 클래스와 동일한 메서드로 설명하겠다. public interface DrinkCustomRepository {

List<Drink> findByName(String name);
}
그리고 DrinkCustomRepository를 상속 받는 DrinkRepositoryImpl를 구현한다. @RequiredArgsConstructor
@Transactional(readOnly = true)
public class DrinkRepositoryImpl extends DrinkCustomRepository {

private final JPAQueryFactory queryFactory;

public List<Drink> findByName(String name) {
return queryFactory
.selectFrom(drink) // (1)
.where(drink.name.name.eq(name)) //(3)
.fetch();
}
}
DrinkRepository에 DrinkCustomRepository를 상속한다. public interface DrinkRepository extends JpaRepository<Drink, Long>, DrinkCustomRepository {
...
}

이렇게 설정해두면 Service에서 DrinkRepository 만 주입 받아도 DrinkCustomRepository에 있는 메서드들 까지 모두 사용 가능하다. 상속/구현 없는 Repository → 현재 적용하진 않았지만 JpaRepository 없이 Querydsl만 이용하고 싶을 때 사용 Querydsl만으로 Repository 구성하는 방법. JPAQueryFactory만 주입 받고 Querydsl 사용한다. @RequiredArgsConstructor
@Repository
public class DrinkQueryRepository {
private final JPAQueryFactory queryFactory;

public List<Academy> findByName(String name) {
return queryFactory.selectFrom(drink)
.where(drink.name.name.eq(name))
.fetch();
}
}
Bean 등록을 위해 @Repository 선언 특정 Entity만 사용해야 한다는 제약도 없다. 별도의 상속이나 구현이 필요 없다. 이렇게 기본적인 세팅과 클래스 설정까지 끝이 났다. 다음으로는 queryFactory를 이용해서 Querydsl을 직접 사용하는 방법을 알아보겠다. Query문 작성 기본적으로 sql문과 흡사한 형태라 사용하기 쉬울 것이다. 결과 반환 fetch : 조회 대상이 여러 건일 경우. 컬렉션 반환 fetchOne : 조회 대상이 1건일 경우(1건 이상일 경우 에러). Generic에 지정한 타입으로 반환 fetchFirst : 조회 대상이 1건이든 1건 이상이든 무조건 1건만 반환. 내부에 보면 return limit(1).fetchOne() 으로 되어있음 fetchCount : 개수 조회. long 타입 반환 fetchResults : 조회한 리스트 + 전체 개수를 포함한 QueryResults 반환. count 쿼리가 추가로 실행된다. 기본적으로 static import 하나가 들어가 있으니 drink에 혼동하지 않도록 주의 바람 import static com.jujeol.drink.domain.QDrink.drink;
프로젝션(select) Drink findDrink = queryFactory.select(drink)
.from(drink)
.fetchOne();
select에 어떤 값을 찾을 것인지, from 절에는 어디서 찾을 것인지 인자로 넣으면 된다. from Drink findDrink = (Drink) queryFactory.from(drink)
.fetchOne();
프로젝션을 지정하는 select 문이 빠졌지만 위와 동일하게 from의 처음에 나오는 Entity를 사용한다. 다만 이 경우엔 Object를 반환해서 타입 캐스팅이 필요하다. selectFrom Drink findDrink = queryFactory.selectFrom(drink)
.fetchOne();
위 두가지를 합친 것. 이걸 사용하는 것이 가장 좋다. 타입 캐스팅도 필요 없다. Join join, innerJoin, leftJoin, rightJoin을 지원한다. Drink findDrink = queryFactory.selectFrom(drink)
.join(drink.viewCount, QViewCount.viewCount1)
.fetchOne();
join에 첫번째 인자로 join할 대상, 두번째 인자로는 join 할 대상의 쿼리 타입을 주면 된다. on 절은 자동으로 붙는다. 추가적인 on 절도 사용 가능 Drink findDrink = queryFactory.selectFrom(drink)
.join(drink.viewCount, QViewCount.viewCount1)
.on(drink.name.name.eq(""테라"")) // (1)
.fetchOne();
(1) : Drink Entity의 name이 Embbeded라 name.name으로 가야 원시값으로 접근 가능하다보니 생기는 현상 조건 Entity에 대해 조건을 걸 수 있다. String 테라 = ""테라"";

Drink findDrink = queryFactory.selectFrom(drink)
.where(drink.name.name.likeIgnoreCase(""%"" + 테라 + ""%""))
.fetchFirst();
(조건1 or 조건2) and (조건3 or 조건4) 이런 식으로 괄호와 and, or을 이용해서 여러 조건을 걸 수 있다. 조건문에 사용되는 메서드 더 많은 메서드를 확인하고 싶으면 여기 BooleanBuilder를 사용해서 조건을 동적으로 만들 수 있다. @Override
public Page<Drink> findByRecommendation(
RecommendationTheme recommendationTheme,
Pageable pageable
) {
QueryResults<Drink> result = queryFactory.selectFrom(drink)
.where(recommendationCondition(recommendationTheme)) // (1)
.orderBy(recommendationOrder(recommendationTheme))
.offset(pageable.getOffset())
.limit(pageable.getPageSize())
.fetchResults();
return new PageImpl<>(result.getResults(), pageable, result.getTotal());
}
(1) : private BooleanBuilder recommendationCondition(RecommendationTheme recommendationTheme) {
BooleanBuilder builder = new BooleanBuilder();
if (recommendationTheme.isPreference()) {
builder.and(drink.preferenceAvg.gt(0));
}
if (recommendationTheme.isViewCount()) {
builder.and(drink.viewCount.viewCount.gt(0));
}
if (recommendationTheme.isBest()) {
builder.and(drink.preferenceAvg.gt(0));
builder.and(drink.viewCount.viewCount.gt(0));
}
return builder;
}
builder에 자바 코드처럼 분기 처리를 통해 다양한 경우에 대해 조건을 설정해서 where절에 메서드로 주입할 수 있다. 또는 각 조건마다 BooleanExpression 메소드화시켜 where절에 and나 or로 연결해줄 수도 있다. 동일한 findByRecommendation 메서드를 변경해보겠다. @Override
public Page<Drink> findByRecommendation(
RecommendationTheme recommendationTheme,
Pageable pageable
) {
QueryResults<Drink> result = queryFactory.selectFrom(drink)
.where(isPreference(recommendationTheme),
isViewCount(recommendationTheme),
isBest(recommendationTheme))
.orderBy(recommendationOrder(recommendationTheme))
.offset(pageable.getOffset())
.limit(pageable.getPageSize())
.fetchResults();
return new PageImpl<>(result.getResults(), pageable, result.getTotal());
}
private BooleanExpression isPreference(RecommendationTheme recommendationTheme) {
return drink.preferenceAvg.gt(0);
}

private BooleanExpression isViewCount(RecommendationTheme recommendationTheme) {
return drink.viewCount.viewCount.gt(0);
}

private BooleanExpression isBest(RecommendationTheme recommendationTheme) {
return drink.preferenceAvg.gt(0).and(drink.viewCount.viewCount.gt(0));
}
그룹핑 group by 이용해서 그룹핑도 가능하다. List<String> drinkNames = queryFactory.from(drink)
.select(drink.name.name)
.groupBy(drink.name.name)
.fetch();
정렬 List<Drink> drinks = queryFactory.selectFrom(drink)
.orderBy(drink.name.asc(), drink.viewCount.viewCount.desc())
.fetch();
페이징 @Override
public Page<Drink>findBySearch(Search search, Pageable pageable) {
QueryResults<Drink> result = queryFactory.selectFrom(drink)
.where(searchCondition(search))
.offset(pageable.getOffset())
.limit(pageable.getPageSize())
.fetchResults();
return new PageImpl<>(result.getResults(), pageable, result.getTotal());
}
시작 인덱스를 정하는 offset 조회할 개수를 지정하는 limit fetchResults()를 이용해서 QueryResults를 반환 받은 후 PageImpl 에 인자로 위 코드와 같이 넘기면 외부에서 받은 Pageable 에 해당하는 페이징 처리를 할 수 있다. 정리 서브쿼리와 수정, 삭제 등 더 많은 코드가 있으므로 필요할 때마다 참고해서 사용하면 될 것 같습니다. 모두들 화이팅! 참고 자료 http://honeymon.io/tech/2020/07/09/gradle-annotation-processor-with-querydsl.html https://jojoldu.tistory.com/372#recentEntries https://sup2is.github.io/2020/10/20/what-is-jpa-query-dsl.html https://joont92.github.io/jpa/QueryDSL/ https://sas-study.tistory.com/393",BE
254,"협업필터링 아이템-기반 추천 (슬로프 원 알고리즘) 24 Oct 2021 on 조잘조잘 주절주절의 새로운 추천 방식 기존 주절주절의 추천 방식은 협업필터링을 이용한 사용자 기반(User-basd) 추천이었다. 협업필터링 관련 글 보러가기 하지만 이번에 아이템 기반(Item-based)로 바꾸게 되었다. 그리고 머하웃 라이브러리를 이용하지 않고 직접 알고리즘을 구현해 사용하고 있다. 왜 이런 선택을 했을까? 왜 아이템 기반으로 바꾸게 되었나? 먼저, 사용자 기반으로 흘러가게 되는 방식은 한 사용자가 많은 선호도를 남겼을 때 다른 사용자와의 유사도를 더욱 정확하게 측정할 수 있다. 반대로 이야기하면 해당 사용자가 적은 양의 선호도를 남긴다면 유사도를 측정하기가 힘들다는 것이다. 위 상황이라면 과연 새로운 유저와 비슷한 취향을 가진 사용자는 누구일까? 새로운 유저가 B 혹은 C 상품에 대해 선호도를 남길 때까지 애매하다. 하지만 반대로 상품 기반으로 측정한다면 A 상품을 좋아한다면 D 상품도 좋아하는 것을 볼 수 있다. 따라서 새로운 유저에게 바로 D 상품을 추천할 수 있게 된다. 주절주절은 서비스 특성상 상품의 추가는 빈번하지 않다. 유저의 추가는 빈번하다고 예상하기에 조금의 데이터로도 유저가 추천을 받을 수 있는 상황을 만드는 것이 중요하다고 판단했다. 이러한 이유때문에 주절주절은 사용자 기반 추천에서 아이템 기반 추천으로 변경하게 되었다. 왜 알고리즘을 직접 구현했나? 이전에 주절주절은 알고리즘을 위해 머하웃 라이브러리를 사용했다. 라이브러리에 대해 미숙해서인지 많은 에러를 직면했다… 먼저, 매번 커넥션을 가져가는 상황도 존재했고(프록시를 만들어 해결) 다른 라이브러리(유레카)와 의존성이 충돌하는 경우도 발생했다. 또한, 몇몇 유저에게는 추천이 잘 되지 못하는 상황도 발생했다… 마지막으로 알고리즘에 대해 자세히 모르다보니 최적화를 어떻게 진행할 지 감을 잡기가 힘들었다. 이러한 모든 이유때문에 직접 알고리즘을 구현해보는 것도 좋을 것 같다는 판단 하에 직접 만들어보게 되었다. 사용한 알고리즘은 슬로프 원 알고리즘이다. 간단하게 식을 보고 어떤 방식으로 알고리즘이 흘러가는지 살펴보자. 슬로프 원 알고리즘 편차 구하기 예측 점수 구하기 자, 이제 그림으로 슬로프 원 알고리즘을 살펴보자. 먼저, A~D 까지의 주류가 존재한다. 여기서 새로운 유저의 A의 예상 선호도를 측정해보자. 주류 A 를 기준으로 각 주류의 선호도 편차를 구하고 모든 유저의 평균을 구한다. A-B 주류 선호도 편차 평균 구하기 유저 ‘가’ 입장에서 A-B 주류 선호도 편차는 4.5 - 2.0 = 2.5 이다. 유저 ‘나’ 입장에서 A-B 주류 선호도 편차는 3.5 - 5.0 = -1.5 이다. 이후 주류 선호도 편차를 더하고 계산된 유저 총 수를 나눈다. (2.5 - 1.5) / 2 = 0.5 A-C 주류 선호도 편차 평균 구하기 유저 ‘가’ 입장에서 A-C 주류 선호도 편차는 4.5 - 5.0 = -0.5 이다. 유저 ‘나’ 입장에서 C 에 대한 평가가 없으니 패스한다. 이후 주류 선호도 편차를 더하고 계산된 유저 총 수를 나눈다. (-0.5 ) / 1 = -0.5 A-D 주류 선호도 편차 평균 구하기 유저 ‘가’ 입장에서 A-D 주류 선호도 편차는 4.5 - 4.5 = 0 이다. 유저 ‘나’ 입장에서 A-D 주류 선호도 편차는 3.5 - 2.5 = 1.0 이다. 이후 주류 선호도 편차를 더하고 계산된 유저 총 수를 나눈다. 1 / 2 = 0.5 자 이제 예상 선호도를 구해보자. (인원 수(새로운 유저의 선호도 + 편차 평균) + … ) / (평균을 낸 인원수 + … ) 식을 진행해보자. 여기서 새로운 유저는 C에 대한 선호도가 없으니 패스한다. (A-B ) + (A-D) / (A-B에 대한 평가 남긴 인원 수) + (A-D에 대한 평가 남긴 인원 수) ( 2(4.5 + 0.5) + 2(4.5 + 0.5) ) / (2 + 2) = ( 10 + 10 ) / 4 = 5 새로운 유저에게 A 주류에 대한 예상 선호도는 5.0 점이 되는 것이다. 자바 코드로 구현하기 자바 코드로 구현할 때 생각하는 구조는 다음과 같다. 먼저, 주류 선호도 편차를 구하기 위한 데이터 세팅이다. 또한 이 데이터 세팅을 하면서 나중에 편하게 계산할 수 있도록 유저 별 데이터 세팅도 같이 해준다. public class DataMatrix {

private final Map<Long, Map<Long, ItemCounter>> matrix;
private final Map<Long, Map<Long, Double>> dataByMember;

public DataMatrix(List<DataModel> dataModel) {
this.dataByMember = new HashMap<>();
this.matrix = new HashMap<>();
prepareMatrix(dataModel);
}

private void prepareMatrix(List<DataModel> dataModel) {
for (DataModel model : dataModel) {
final Long memberId = model.getMemberId();
final Map<Long, Double> itemByMember = dataByMember
.computeIfAbsent(memberId, id -> new HashMap<>());

for (Entry<Long, Double> itemPreference : itemByMember.entrySet()) {
final Long itemId = itemPreference.getKey();
final Double preference = itemPreference.getValue();
if(itemId.equals(model.getItemId())) continue;

final Map<Long, ItemCounter> primaryMap =
matrix.computeIfAbsent(model.getItemId(), id -> new HashMap<>());
final Map<Long, ItemCounter> secondaryMap =
matrix.computeIfAbsent(itemId, id -> new HashMap<>());

primaryMap.computeIfAbsent(itemId, id -> new ItemCounter()).addSum(model.getPreference() - preference);
secondaryMap.computeIfAbsent(model.getItemId(), id -> new ItemCounter()).addSum(preference - model.getPreference());
}
itemByMember.put(model.getItemId(), model.getPreference());
}
}
public Map<Long, Map<Long, ItemCounter>> getMatrix() {
return matrix;
}

public Map<Long, Double> getDataByMember(Long id) {
return dataByMember.getOrDefault(id, new HashMap<>());
}
}
이후, 추천해야 할 유저의 아이디가 온다면 위 데이터를 가지고 추천을 해준다. public class Recommender {

public List<RecommendationResponse> recommend(DataMatrix dataMatrix, Long memberId, double minPreference) {
final Map<Long, Map<Long, ItemCounter>> matrix = dataMatrix.getMatrix();
final Map<Long, Double> dataByMember = dataMatrix.getDataByMember(memberId);
final List<RecommendationResponse> recommendItems = new ArrayList<>();

for (Entry<Long, Map<Long, ItemCounter>> matrixEntry : matrix.entrySet()) {
final Long primaryItemId = matrixEntry.getKey();
final Map<Long, ItemCounter> matrixValue = matrixEntry.getValue();
if(dataByMember.containsKey(primaryItemId)) continue;

double sumValue = 0.0;
long count = 0;

for (Entry<Long, Double> itemPreference : dataByMember.entrySet()) {
final Long itemId = itemPreference.getKey();
final Double preference = itemPreference.getValue();
final ItemCounter itemCounter = matrixValue.get(itemId);
final double deviation = itemCounter.getDeviation();
sumValue += (preference + deviation) * itemCounter.getCount();
count += itemCounter.getCount();
}

final double expectedPreference = sumValue / count;
if(expectedPreference >= minPreference) {
recommendItems.add(new RecommendationResponse(primaryItemId, expectedPreference));
}
}
return recommendItems;
}
}
이것으로 추천 알고리즘 작성이 완성되었다. 주절주절의 예상 선호도 모습입니다!! 읽어주셔서 감사합니다 =]",BE
263,"Dirties Context 제거를 통한 테스트 최적화 24 Oct 2021 on 조잘조잘 Dirties Context 제거를 통한 테스트 최적화 문제상황 인수테스트가 많아지며 테스트 시간이 증가하기 시작했습니다. 주절주절이 구축한 CI/CD는 PR을 보내면 테스트 코드 실행이 포함된 빌드 테스트를 거칩니다. 또한 빌드시에도 다시 한번 운영환경에서 테스트를 거치고 배포를 수행하게 되는데, 2분 이상이 되자 CI/CD 전 과정에 걸쳐서 10분 이상의 시간을 소요하게 되었습니다. 이는 서버 인프라 테스트나 다양한 기능이 비슷한 타이밍에 주절주절 베이스 코드에 합쳐지게 되었을 때 생산성을 저하하는 것으로 이어졌기 때문에, 팀 회의를 거쳐 테스트 최적화를 진행하기로 결정하였습니다. 사전정보 SpringBoot는 SpringBoot가 run 될 때마다 새롭게 어플리케이션 컨텍스트를 막기 위해 context caching 기능을 지원합니다. 컨텍스트 캐싱 기능이 없다면 매 테스트를 실행할 때마다 스프링의 Application Context를 로딩해주어야 합니다. 테스트가 2~3개 이면 크게 상관 없겠지만, 100개, 200개가 되면 상황이 다릅니다. Spring TestContext 프레임워크는 한 번 ApplicationContext가 만들어지면 캐시에 저장합니다. 그리고 다른 테스트를 실행할 때 가능한 경우 재사용 하는데요, 여기서 가능한 경우란 같은 bean의 조합을 필요로 하고 이전 테스트에서 ApplicationContext가 오염되지 않은 경우 를 말합니다. 컨텍스트 캐싱의 조건 이 중 1번, 같은 bean의 조합은 Context Caching에서 cache key로 어떤 것을 쓰는지와 동일합니다. Spring TestContext에선 여러 configuration으로 이 key를 구성합니다. 공식 문서에 나타나 있는 설정 종류는 아래와 같습니다. locations (from @ContextConfiguration) classes (from @ContextConfiguration) contextInitializerClasses (from @ContextConfiguration) contextCustomizers (from ContextCustomizerFactory) contextLoader (from @ContextConfiguration) parent (from @ContextHierarchy) activeProfiles (from @ActiveProfiles) propertySourceLocations (from @TestPropertySource) propertySourceProperties (from @TestPropertySource) resourceBasePath (from @WebAppConfiguration) 여기에 더하여, TestStub 라이브러리인 Mockito를 사용했을 때, 특정 빈을 @MockBean으로 교체하면 contextCustomizers에 MockitoContextCustomizer가 추가 되어서 MockBean 처리한 bean의 조합이 달라질 경우 cache key가 달라지게 됩니다. 그러면 새로운 컨텍스트를 로딩하게 되므로 주의해야 합니다. 컨텍스트가 오염되었을 때(Dirties Context) 2번, 테스트에서 사용하는 컨텍스트가 오염되었다는 신호는 @DirtiesContext 어노테이션을 통해 줄 수 있습니다. 주절주절 프로젝트에서 인수테스트 시 상속받아 활용하고 있는 AcceptanceTest의 코드 입니다. @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)
@DirtiesContext(classMode = DirtiesContext.ClassMode.AFTER_EACH_TEST_METHOD)
@ExtendWith(RestDocumentationExtension.class)
@ActiveProfiles(""test"")
public class AcceptanceTest {
...
}
classMode 옵션을 주어 매 테스트 이후(after each test) 새롭게 어플리케이션 컨텍스트를 띄우게 됩니다. 상황에 맞게 사용하면 훌륭한 어노테이션이지만, 주절주절에서는 인수테스트에서 테스트 DB(h2)를 초기화 하기 위해 매 메소드마다 활용하게 되었습니다. 왜 @Transactional, @Rollback을 사용하지 않고 DirtiesContext를? 위에서 DB 롤백을 위해 DirtiesContext를 활용하였다고 말씀 드렸는데요, 스프링 테스트에서는 DB 롤백을 위해 @Transactioal 과 @Rollback을 지원하고 있습니다. 테스트 메소드에 해당 어노테이션이 있는 경우, TransactionSyncManager에서 테스트 메소드가 종료된 후 rollback 명령을 날려 테스트 동안 진행된 데이터를 롤백할 수 있습니다. 하지만 주절주절에서는 인수테스트 도구로 RestAssured를 활용하고 있었는데, RestAssured에선 Transactional 어노테이션이 동작하지 않습니다. 그 이유는 RestAssured와 MockMVC의 차이점 때문입니다. MockMVC는 슬라이스 테스트 도구입니다. 스프링 서버를 별도로 띄우지 않고도 복잡한 스프링의 설정을 다 불러들이기 위하는 것에 사용 목적이 있습니다. 그래서 서블릿 컨테이너를 새롭게 생성하지 않고, 내부적으로 DispatcherServlet을 모킹하여 사용합니다. 같은 스레드에서 코드가 수행되기 때문에 Rollback이나 Transactional 어노테이션이 동작합니다. 그와 달리 RestAssured는 스프링 서버를 별도 포트(SpringBootTest의 webEnvironment 설정에 의해)에 띄워놓고, 유저 요청(HTTP request)를 모방하여 인수테스트를 진행하게 됩니다. 테스트가 진행되는 스레드와 실제 서버가 띄워져 있는 스레드가 다르기 때문에, RestAssured를 활용하는 코드에 Transactional이나 Rollback 코드를 붙여도 서버 쪽에는 영향을 주지 못 합니다. 도식화 하면 다음과 같습니다. 이때문에 DirtiesContext를 사용하여, 어플리케이션 컨텍스트를 새로 띄워 데이터베이스를 초기화 하는 방식으로 사용하였습니다. 이는 인수테스트가 추가 될 때마다 테스트 시간이 산술급수적으로 증가하는 결과로 나타나게 됩니다. 매번 DML (Truncate)을 날려 초기화하는 방법도 있지만 삭제와 관련된 DML은 누군가의 실수로 인해 테스트용 데이터를 미리 적용해놓은 DB에 영향을 줄 수 있어 기각하였습니다. 실제로 어떤 내용을 진행하였는지 아래에 서술하겠습니다. 개선 목표 DirtiesContext를 제거하여 test시간을 n초수정 요망으로 줄인다. 방법 논의 RestAssuered를 MockMVC로 변환한다. 기존에 활용하던 RequestBuilder를 인터페이스화 하여 MockMVC 구현체를 활용할 수 있도록 수정한다. 진행 기존에는 RequestBuilder 라고 하는 인수테스트 툴을 만들어 사용하고 있었습니다. 링크 @Component
@ActiveProfiles(""test"")
public class RequestBuilder {

private final ObjectMapper objectMapper;
private final LoginService loginService;
private final QueryCounter queryCounter;

private RestDocumentationContextProvider restDocumentation;

...

// 각 메소드별 요청의 추상화
public class Function {
//Option Chaining을 통해 선택적으로 필요한 내용 구현
public Option get(String path, Object... pathParams) {
return new Option(new GetRequest(path, pathParams));
}
...
}

// 각 옵션을 활성화하거나 비활성화 할 수 있도록 추상화
public class Option {

private final RestAssuredRequest request;
private boolean logFlag;
private DocumentHelper documentHelper;
private UserHelper userHelper;
private MultipartHelper multipartHelper

// Option chaining
public Option withDocument(String identifier) {
documentHelper.createDocument(identifier);
return this;
}
...
}

// RestAssured.then()의 결과를 변환하는 역할을 하는 클래스
public class HttpResponse {
...
}

public HttpResponse build() {
// RestAssured를 의존하여 테스트를 수행함
RequestSpecification requestSpec = documentHelper.startRequest();
...
return new HttpResponse(validatableResponse.extract(), queryResult);
}
}
해당 클래스의 역할은 중복되는 Dto 변환 로직, 인증 관련 처리, Content-type 설정 등 주절주절에서 기본 설정에 가까운 코드들을 캡슐화 해둔 클래스입니다. 또한 RestDocs, logging, multipart 처리, 인증, 테스트 도중 몇번의 쿼리가 나가는지 확인하는 QueryCounte의 동작을 RequestBuilder 사용하는 코드에서 결정할 수 있도록 코드를 작성했습니다. @Test
void 카테고리_전체조회_테스트(){
...
// 활용 예시
new RequestBuilder().builder()
.get(""/categories"")
.withUser() // 인증을 활용하고 싶을 때
.withDocument(""category/show/all"") // Restdocs snipet 생성
.build()
.convertBodyToList(CategoryResponse.class); // 주절주절 공용 DTO에 맞는 형변환 제공
}
기존에 작성한 인수테스트 코드를 고치지 않고 RestAssuered에서 MockMVC로 변경하기 위하여, RequestBuilder의 내부 테스트 실행부를 인터페이스화 하여 유연하게 변경할 수 있도록 수정하였습니다. 이후 RestAssured 구현체 사용 & DirtiesContext를 제거하니 M1 Mac, 16G 메모리 팀원 기준 150s 걸리던 테스트가 15s 로 개선되었습니다. PR결과, github action으로 빌드테스트를 하는 구간에서 4분 17초에서 1분53초로 감소한 것을 확인하여 CI과정에서 걸리는 병목을 의미있게 감소시켰습니다. 결론 PR을 보내고 빌드 테스트를 하는 동안 ‘세상에서 가장 오래 기다려야하는 5분’이라는 느낌으로 대기 했는데, 2분 내외로 줄어드니 살 맛납니다. 각종 최적화 기법을 적용해 테스트 시간을 줄여보는 것이 어떨지요! 참고 https://www.youtube.com/watch?v=jdlBu2vFv58&t=463s https://bperhaps.tistory.com/entry/테스트-코드-최적화-여행기-1 https://docs.spring.io/spring-framework/docs/current/reference/html/testing.html",BE
264,"SPRING, AOP, JDBC를 활용해 쿼리 개수를 세어보자 (쿼리 스파이) 10 Oct 2021 on 조잘조잘 SPRING, AOP, JDBC를 활용해 쿼리 개수를 세어보자 (쿼리 스파이) 쿼리 개수를 세는 작업이 필요했던 이유 이번 팀 프로젝트에서는 JPA 를 사용하게 되었다. 자동으로 쿼리를 작성해 디비에 요청을 하다보니 생각치도 못한 곳에서 여러 개의 쿼리를 날리며 (n+1 문제) 서비스를 느리게 만들었다. 이 문제를 확인하기 위해 매번 날아가는 쿼리를 확인해가며 n+1 문제가 발생하는 지 확인해야했다. 이 부분을 자동으로 세어주는 ‘쿼리 스파이’를 만들어 쿼리 문제 개선의 발판을 만들었다. ‘쿼리 스파이’ 고민의 시작 처음엔 많은 고민이 있었다. p6spy 라는 쿼리 로그를 사용하는 라이브러리를 커스터마이징 해 로그를 찍을 때마다 카운트를 올려줄까? JPA 에서 사용되는 EntityManager 를 이용해볼까? 이렇게되면 너무 한 기술(p6spy 혹은 JPA)에 의존적이라 생각하게 되어 가장 기술에 덜 의존적인 부분이 어디일까 고민하다 jdbc api의 흐름인 Connection 의 PreparedStatement 부분을 이용하자라고 생각하게 되었다. ‘쿼리 스파이’ 의 흐름 먼저, JPA 라는 기술의 흐름을 알아보자. JPA 는 Hibernate 를 사용하며 Hibernate 는 jdbc api를 사용하게 된다. 그리고 필자는 이 jdbc api의 흐름을 이용할 것이다. JdbcTemplate의 흐름은 다음과 같다. DataSource 를 주입 받는다. DataSource 에서 getConnection() 메서드를 이용해 Connection 을 획득한다. Connection 의 prepareStatement() 메서드를 이용해 PreparedStatement 를 획득한다. PreparedStatement 의 executeQuery() 메서드를 이용해 쿼리를 실행한다. 즉, executeQuery() 의 실행 횟수를 알아내게 되면 쿼리 개수를 알 수 있는 것이다. 목표는 PreparedStatement 의 프록시를 만들어 executeQuery() 를 호출할 때마다 카운팅을 해주는 작업을 한다. 만들어보기 위에서 이야기했듯이 우리에게 필요한 건 PreparedStatement 의 프록시를 대신 사용하게 하는 것이다. PreparedStatement 의 프록시를 사용하려면 Connection 의 preparedStatement() 를 조작해야 하며 이는 DataSource 도 해당된다. 꽤나 복잡해 보이지만 간단하게 그림으로 살펴보자! 스프링 부트에서는 DataSource 를 빈으로 만든다면 해당 DataSource 를 이용해 위의 흐름을 진행한다. (기본 DataSource 빈은 h2 디비를 이용한 DataSource 이다.) 그렇기에 빈으로 등록된 부분을 Proxy로 만들 수 있는 스프링 AOP를 이용해 DataSource 를 해결할 것이고 다른 부분들은 다이나믹 프록시를 이용해 해결할 것이다.(일반 프록시를 만들어 해결할 수 있지만 각각의 인터페이스의 메서드가 너무 많아서 다이나믹 프록시로 구현했다.) 먼저, QueryCounter 라는 쿼리를 카운트하기 편한 객체를 만들어보자. (테스트 용도로만 사용하기에 QueryCounter 를 빈으로 등록해서 사용하게 되었다. 실제 서버에서도 사용하려면 쓰레드 세이프하게 다시 구성해야한다.) 간단한 흐름은 다음과 같다. public class QueryCounter {

private QueryResult queryResult;
private Count count;
private boolean countable;

public QueryCounter() {
this.countable = false;
}

public void startCount() {
this.countable = true;
this.count = new Count(0);
this.queryResult = new QueryResult();
}

public void upCount(String statement) {
this.count = count.upCount();
this.queryResult.save(count, statement);
}

public QueryResult endCount() {
final QueryResult result = this.queryResult;
this.countable = false;
return result;
}

public boolean isCountable() {
return countable;
}
}
다음은, PreparedStatement 의 다이나믹 프록시를 만들어 위에서 만든 쿼리카운터를 넣어주자. jdk dynamic proxy는 다음과 같이 만들 수 있다.(Connection 과 PreparedStatement 모두 인터페이스이기 때문에 jdk 로 dynamic proxy를 만들 수 있다. 만일 클래스라면 CGLib 를 알아보자.) Dynamic proxy 만드는 코드 Proxy.newProxyInstance(
target.getClass().getClassLoader(),
target.getClass().getInterfaces(),
""InvocationHandler"")
이제 InvocationHandler 부분에 들어갈 부분을 구현한다. (프록시의 모든 요청은 InvocationHandler 의 invoke(...) 로 들어오게된다.) ProxyPreparedStatementHandler public class ProxyPreparedStatementHandler implements InvocationHandler {

private final Object preparedStatement; //target
private final QueryCounter queryCounter;

public ProxyPreparedStatementHandler(Object preparedStatement,
 QueryCounter queryCounter) {
this.preparedStatement = preparedStatement;
this.queryCounter = queryCounter;
}

@Override
public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
if(method.getName().equals(""executeQuery"")) {
if(queryCounter.isCountable()) {
queryCounter.countUp();
}
}
return method.invoke(preparedStatement, args);
}
}
executeQuery 라는 메서드가 호출 될 때만 쿼리카운터에서 카운트가 되게끔 코드를 구현했다. 이제 Connection 이 prepareStatement() 메서드를 호출할 때 위의 프록시를 리턴하게끔 만들면 된다. ProxyConnectionHandler public class ProxyConnectionHandler implements InvocationHandler {

private final Object connection;
private final QueryCounter queryCounter;

public ProxyConnectionHandler(Object connection, PerformanceLoggingForm loggingForm) {
this.connection = connection;
this.queryCounter = queryCounter;
}

@Override
public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
final Object returnValue = method.invoke(connection, args);
if (method.getName().equals(""prepareStatement"")) {
return Proxy.newProxyInstance(
returnValue.getClass().getClassLoader(),
returnValue.getClass().getInterfaces(),
new ProxyPreparedStatementHandler(returnValue, queryCounter));
}
return returnValue;
}
}
이제 마지막으로 DataSource 가 getConnection 을 호출할 때 위에서 만든 Proxy Connection 을 리턴하도록 만들어보자. 이 부분은 스프링 AOP 를 이용해보자! Spring AOP 코드 @Component
@Aspect
@RequiredArgsConstructor
public class QueryCounterAop {

private final QueryCounter queryCounter;

@Around(""execution(* javax.sql.DataSource.getConnection())"")
public Object datasource(ProceedingJoinPoint proceedingJoinPoint) throws Throwable {
final Object proceed = proceedingJoinPoint.proceed();
return Proxy.newProxyInstance(
proceed.getClass().getClassLoader(),
proceed.getClass().getInterfaces(),
new ProxyConnectionHandler(proceed, queryCounter));
}
}
QueryCounter 로 쿼리 세기 이제 QueryCounter 를 주입받아 쿼리를 세어 주면 된다. @SpringBootTest
@ActiveProfiles(""test"")
public class TestQueryCounter {

@Autowired
private QueryCounter queryCounter;
@Autowired
private DrinkService drinkService;

@Test
@DisplayName(""query counter test"")
public void queryCounter() {
queryCounter.startCount();
drinkService.showAllDrinksByPage(Pageable.ofSize(8), LoginMember.anonymous());
final QueryResult queryResult = queryCounter.endCount();
System.out.println(""쿼리 개수 : "" + queryResult.queryCount());
}
}
이것을 활용해 생각보다 쿼리 개수가 많은 로직에서 n+1 을 의심해볼 수 있다. 또한 모든 로직에 쿼리 개수를 셀 수 있게 하고 5개 이상 쿼리 개수가 세어진다면 WARN level 로 로깅해 다시 한 번 확인할 수 있게 만들어 의도치 않은 쿼리를 방지할 수 있다. 다음 글은 이 기술을 활용해 편하게 AWS의 CloudWatch 에서 각 api 별 실제 트랜잭션 시간, 쿼리 개수, 쿼리 실행 시간 등 편하게 퍼포먼스를 확인할 수 있는 방법을 소개할 것이다.",BE
265,"로그 유무에 따른 성능 측정 23 Aug 2021 on 조잘조잘 Apache JMeter를 통해 성능 측정하기 JMeter JMeter 는 성능 측정을 위해 사용하는 오픈소스이다. 오픈소스 중에서는 항상 TOP7에 랭크될 정도로 많이 사용한다. 순수 자바 어플리케이션이며 정적, 동적, 웹 동적 어플리케이션 테스팅 모두 가능하다. Apache JMeter 공식 문서 에 따르면, Web - HTTP, HTTPS (Java, NodeJS, PHP, ASP.NET, …) SOAP / REST Webservices FTP Database via JDBC LDAP Message-oriented middleware (MOM) via JMS Mail - SMTP(S), POP3(S) and IMAP(S) Native commands or shell scripts TCP Java Objects 위와 같이 다양한 어플리케이션, 서버, 프로토콜에서 사용할 수 있다. 대규모 테스트에는 좀 불리하다고 한다🤔 그래도 여러 플러그인을 지원한다는 장점도 있다. (참고1, 참고2) JMeter 설치 아래 설치 방법은 Mac OS 기준이다. $ brew install jmeter
JMeter 실행 $ open /opt/homebrew/Cellar/jmeter/5.4.1/bin/jmeter

# 찾아보면 보통 아래처럼 하라는데 나는 brew 위치가 달라서 알아서 찾아서 했다.
$ open /usr/local/bin/jmeter
plugin 설치 Options → Plugin Manager Available Plugin 탭에서 3 Basic Graphs Custom Thread Groups 두 개를 설치한다. ThreadGroup 생성 및 측정 여러 개의 스레드 그룹을 만들어서 실행해 볼 수 있다. JMeter 의 실행 단위가 이 스레드 그룹이다. 하나의 스레드 그룹에 여러 개의 HTTP Request 를 만들 수 있지만, 나는 같은 요청을 여러 형태로 보존하고 싶었기 때문에 따로 만들었다. 결과를 여러 형태의 리스너로 볼 수 있다. 나는 그 중에서 요청을 하나씩 볼 수 있는 View Results Tree 와 TPS 등을 요약해서 보여주는 Summary Report , 같은 정보를 그래프로 표현하는 Graph Results 세 개만 볼 것이다. Thread Group 은 아래와 같이 스레드 수만 변경한다. Numbers of Threads (users) 가 말 그대로 스레드 수, 요청하는 사용자의 수를 나타낸다. 아래는 어느정도 시간 간격으로 할거냐, 반복할거냐 이런건데 옵션을 바꿔가며 주면 좋을 것 같다. 그리고 HTTP Request 에 다음과 같이 요청 보낼 곳 정보를 입력한다. 지금은 단순 조회 테스트라 내용이 많지 않은데, POST 요청 등에서 body 값을 넣어주면 된다. 그리고 Start 를 누르면 끝~!🕺 성능 측정 성능 측정 대상 우리 어플리케이션은 크게 두 가지 로그를 출력한다. Logback 을 사용한 로그 P6spy 를 사용한 SQL 쿼리 로그 Logback 을 사용한 로그의 경우, 개발 서버는 INFO 레벨, 운영 서버는 WARN 레벨로 출력하고 있다. 여기에 더해 LogAOP 라는 별도 설정을 통해 다양한 형태의 로그 포맷을 만들었다. 500 대 에러의 경우 StackTrace를 출력한다. 컨트롤러단부터 모든 레이어의 모든 메서드에 대한 로그를 출력한다. 이 로그란 파라미터, 메서드가 수행되는 시그니처(클래스와 메서드명), 반환값을 말한다. 어노테이션을 사용해 특정 메서드 수행에 소요되는 시간을 출력할 수 있다. 그리고 Logback 설정 파일에서는 시간, 스레드 등을 표시하며 HTTP Request&Response 설정도 별도로 해 둔 상태이다. LogAOP X, P6spy O (개발 서버) LogAOP X, P6spy O LogAOP O, P6spy X LogAOP X, P6spy X (운영 서버) 결론 콘솔 출력이 꽤 많은 시간을 잡아먹는다는 것을 알게 되었다. 성능 향상을 위해 운영 버전에는 에러 출력 이외의 모든 로그를 제외했다.",BE
266,"crontab을 사용해 AWS S3에 데이터 백업하기 23 Aug 2021 on 조잘조잘 crontab과 AWS S3를 이용한 로그 백업 자동화 과정 로그 백업하기 로그 저장 정책 우리 어플리케이션의 로그는 하루 단위로 저장된다. 날짜가 지나면 전날의 로그를 zip 파일로 저장하며, 30일이 지나면 오래된 파일부터 삭제한다. 이 설정은 Logback을 통해 완료해둔 상황이다. 로그 백업이 필요한 이유는 아래와 같다. 로그 관련 리팩토링을 거쳐 기존보다 많은 로그가 생성된다. 정식 배포를 마쳤으므로 사용자가 늘면 로그도 더 많이 생성될 것이다. 생성되는 로그가 많아질수록 서버에서 많은 메모리를 차지할 것이다. 30일간 저장하는 것에서 더 적은 일자로 바꿔 부담을 줄이고 싶다. 저장소 선정 처음에 생각한 방법은 Logstash + ElasticSearch 를 통한 전송, 저장이다. Logstash가 로그를 수집하면 다양한 형태로 출력할 수 있다. ElasticSearch에 적재해 데이터를 분석, 가공하고 Kibana로 모니터링할 수도 있고, 로그 지표를 AWS CloudWatch로 바로 출력할 수도 있다. 물론 단순 저장 목적으로 사용해도 된다. 하지만 이 방법은 저장 목적으로만 사용하기엔 생각 이상으로 리소스가 소요되고, 이미 AWS CloudWatch에서 확인하는 로그가 이미 우리 입맛대로 가공된 상황이므로 Kibana를 적용해서 관리 포인트가 늘어나면 번거로워지기만 할 수 있다고 판단했다. 고로 단순 백업 목적에 더 가까운 방법을 생각하게 되었다. crontab crontab crontab은 cron table을 말한다. cron은 유닉스 계열 운영체제에서 지원하는 시간 기반 잡 스케줄러이다. 고정된 시간, 날짜, 간격에 주기적으로 작업을 실행할 수 있게 해준다. (참고) crontab 사용 이번 작업을 수행하면서 자주 사용한 대표적인 명령어를 별도로 기재한다. crontab을 열어 편집 $ crontab -e
나는 거의 vim 으로 사용하는데, 우연찮게 다른 컴퓨터에서 작업하다가 nano로 열려 난감했었다. 아래처럼 편집기를 선택할 수 있다. # nano
$ export VISUAL=nano; crontab -e

# vim
$ export VISUAL=vim; crontab -e
그리고 아래 두 명령어는 각각의 crontab을 생성한다. 즉, sudo 로 만든 crontab은 일반 crontab과 공유되지 않는다. 내 경우에 일반 crontab 은 ubuntu 계정으로 실행되는 반면 sudo는 root 계정으로 실행되었다. 참고 에 따르면 기본 설정은 root라는데 차이가 있었다. $ crontab -e
$ sudo crontab -e
저장된 스케줄 조회 $ crontab -l
스케줄은 실행할 날짜와 작업 지정 * * * * * [뭔가 실행할 일]
다섯 개의 * 로 주기를 설정할 수 있다. 순서대로 분(0-59) 시간(0-23) 일(1-31) 월(1-12) 요일(0-7) 0, 7은 일요일, 1부터 월요일 을 의미한다. cron 실행 상태 확인 $ service cron status
crontab 스케줄링 실행중인 우리 AWS EC2 서버에서 crontab -e 를 입력했을 때 등록되어 있는 스케줄이다. 매일 밤 12시 10분에 쉘 스크립트를 실행하도록 작성했다. 미리 만들어 놓은 sh 파일을 실행하며 전송 과정을 별도의 로그로 남긴다. 로그 파일은 매번 생성하지 않고 이어서 작성한다. 처음에는 로그를 생성하지 않고 작업했는데, 실패할 때마다 원인을 몰라서 만들게 되었다. service cron status 로도 한계가 있었다. 실제로 출력된 로그이다. 매일 업로드가 성공할 때마다 기록되고 있다. 쉘 스크립트 작성 날짜 설정 쉘 스크립트를 작성하기에 앞서 한 일이 현재 날짜를 입력받도록 한 것이다. 로그 파일은 jujeol.{로그 생성 일자}.zip 형태로 되어있기 때문에 저 로그 생성 일자 부분이 시간이 지남에 따라 동적으로 변할 수 있게 만들어야 한다. 간단하게 오늘 날짜를 출력하고 싶으면 $ YYYYMMDD=`date '+%Y%m%d'`
위와 같이 작성하면 된다. 우리 로그 파일 형식에 맞게 변형을 하고 출력하면 다음과 같은 결과를 확인할 수 있다. 하지만 우리가 확인하고 싶은 날짜는 어제 이기 때문에 다른 설정이 필요하다. 참고 에 따라 아래와 같이 설정한다. 이 설정은 단순히 EC2 배쉬에서 해주는 것이 아니라, 쉘 스크립트 안에 직접 작성해야 한다. $ YESTERDAY=`TZ=aaa24 date +%Y-%m-%d`
AWS S3로 파일 전송하기 AWS CLI를 먼저 설치 한다. access key가 있다면 /aws/credentials 에 설정 해준다. 우리는 모든 권한 설정에 IAM role 을 부여하는 형식을 이용했다. $ curl ""https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"" -o ""awscliv2.zip""
$ unzip awscliv2.zip # 없다면 apt install upzip 으로 설치한다.
$ sudo ./aws/install

$ aws --version # 설치를 마치고 버전을 확인해본다.
S3 업로드에도 몇 가지 종류가 있다. aws s3 mv 파일을 말 그대로 이동시킨다. S3 버킷으로 이동시킨 뒤 파일은 내 위치에 없고, S3에만 존재하게 된다. $ sudo aws s3 mv [이동시킬 파일] s3://{S3 버킷 이름 + (폴더)}
aws s3 sync 내 위치와 S3 버킷의 상태를 똑같이 맞춘다. 폴더 단위로만 작동한다. 여러 파일을 한 번에 옮길 수 있다. S3 버킷 이름만 입력하면 최상위 폴더에 위치하게 된다. --delete 옵션을 주면 없어진 파일은 삭제한다. $ sudo aws s3 sync [싱크를 맞출 폴더] s3://{S3 버킷 이름 + (폴더)}
$ sudo aws s3 sync --delete [싱크를 맞출 폴더] s3://{S3 버킷 이름 + 폴더}
aws s3 cp 파일을 복사한다. 이번 작업에서 우리가 선택한 방법이다. $ sudo aws s3 cp [복사할 파일] s3://{S3 버킷 이름 + (폴더)}
log-backup.sh 필요한 작업을 모두 넣으면 아래와 같은 형태가 된다. # cron의 기본설정은 sh이기 때문에 bash로 실행하고 싶다면 이 부분을 추가한다.
SHELL=/bin/bash
# aws 바이너리를 제대로 읽어오지 못할 경우 추가한다.
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# 여기에 따로 써주지 않으면 안먹힌다.
YESTERDAY=`TZ=aaa24 date +%Y-%m-%d`

sudo aws s3 cp /home/ubuntu/logs/jujeol.$YESTERDAY*.zip s3://jujeol-log-bucket
bash 설정 참고 aws 바이너리 관련 참고 sh 파일을 실행할 때 계속 Permission denied 에러가 발생한다면, $ chmod +x log-backup.sh
스크립트 실행 권한을 준다. (참고) 추가사항 추가로 적용하고 싶은 부분 지금은 로그 파일이 보통 한 개 밖에 생성되지 않기 때문에 문제가 없지만, 0, 1, 2, 3, .. 여러 개로 늘어나면 이슈가 생길 것이다. 여러 개의 파일을 cp 명령어로 이동하는 방법을 찾아보거나, 일자별 폴더를 생성하여 sync 로 처리하도록 만들어야 할 것 같다. 참고자료 cron 사용법 1 cron 사용법 2 cron 사용법 3 cron 을 사용한 데이터 백업 (동영상)",BE
267,"Timezone 설정을 위한 여정 19 Aug 2021 on 조잘조잘 AWS EC2, MySQL, Spring 시간 설정하기 서버 시간을 UTC → KST로 바꿔야 한다. 목적은 아래와 같다. 우리 서비스에서 한 상품에 대한 리뷰는 인당 하루에 하나만 가능하다. 자정을 기준으로 초기화할 것이기 때문에 리뷰의 createdAt 컬럼이 제대로 표시되어야 한다. AWS EC2에 쌓이는 로그 파일을 일정 시간마다 AWS S3로 전송하여 보관할 것이다. 마찬가지로 자정을 기준으로 옮길 것이므로 정확한 확인을 위해 시간대를 설정할 필요가 있다. AWS EC2 서버 시간 동기화하기 AWS는 기본적으로 Amazon Time Sync Service를 지원한다. ubuntu ec2는 NTP(Network Time Protocol로 시간이 설정되어 있다. 💡 NTP와 Chrony NTP의 단점을 보완한 네트워크 프로토콜이 Chrony (a versatile implementation of the Network Time Protocol)이다. NTP는 정기적인 폴링이 필요하기 때문에 영구적으로 유지되는 시스템 혹은 브로드캐스트나 멀티캐스트 IP를 사용하는 환경에 적합하다. Chrony는 네트워크가 자주 중단되는 경우에 주로 사용한다. (참고1, 참고2) AWS 문서를 참고 하여 chrony를 사용해 서버 시간을 동기화할 수 있다. Timezone 설정 변경하기 위의 방법은 UTC가 기본이다. KST로 바꾸려면 추가 작업이 필요하다. $ date
현재 서버 시간을 확인할 수 있다. $ sudo cat /etc/localtime
어디에 시간이 맞춰져있는지 본다. 지금 현재 나는 UTC로 설정되어 있고, KST로 바꿀 것이다. $ ls /usr/share/zoneinfo
위 명령어로 검색하면 타임존을 확인할 수 있다. Asia/Seoul 은 저 경로 아래 Asia 에서 확인이 가능하다. 현지 시간을 참조할 때 표준 시간대 파일을 찾아볼 수 있도록 심볼 링크 설정을 해준다. $ sudo ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime
(참고1, 참고2) MySQL 설정 변경하기 select @@system_time_zone;
select @@global.time_zone;
select now();
여러 시간 확인 방법들. set @@global.time_zone = 'Asia/Seoul'
이렇게 설정할 수 있다. SpringApplication에 설정하기 위의 방법들을 다 시도해도 서비스에서는 UTC로 반영된다. (참고1, 참고2, 참고3, 참고4) 1. jar 실행할 때 옵션 주기 $ java -jar -Duser.timezone=Asia/Seoul
2. 빈 초기화 될 때 설정하기 @PostContruct 는 의존성 주입이 이루어진 후 빈을 초기화하는 메서드를 선언한다. 이 메서드는 WAS가 실행될 때 호출하므로, 빈이 생성될 때 딱 한 번만 초기화한다.",BE
287,"협업 필터링 12 Aug 2021 on 조잘조잘 주절주절에 사용된 협업 필터링 설명 목차 추천 알고리즘 들어가기에 앞서… 추천 알고리즘 종류(CB vs CF) CB(Content Based) CF(Collaborative Filtering) CB? or CF? 협업 필터링(Collaborative Filtering) 협업 필터링 동작 방식 협업 필터링 적용기 gradle 추가 mahout 흐름 DataModel 테스트 마무리 참고 자료 추천 알고리즘 들어가기에 앞서… 이번 우아한테크코스 레벨3을 진행하면서 추천 알고리즘이 필요하게 되었다. 여러 시행착오를 거친 추천 알고리즘 일대기를 적어 내려가고자 한다. 스프링 만지고 싶어 이론 말고 협업 필터링을 구현을 바로 보고싶다면 여기로 추천 알고리즘 종류(CB vs CF) 추천 알고리즘은 대표적으로 CB(Content Based), CF(Collaborative Filtering) 두 가지로 나뉜다. 이외에도 LF(Latent Factorization), Hybrid Filtering 등이 있으나 CB, CF 기반으로 파생된 것이므로 이 글에서는 다루지 않는다. CB(Content Based) 사용자 혹은 아이템에 대한 프로필 데이터를 가지고 내가 좋아했던 아이템과 비슷한 유형의 아이템을 추천하거나 나와 비슷한 유형의 사람이 좋아하는 아이템을 추천 사용자를 기준으로 하게 되면 사용자의 프로필(성별, 나이, 지역 등)을 미리 받아 이와 비슷한 사용자가 선호하는 상품을 추천해주는 방식이다. 상품을 기준으로 하게 되면 상품의 특징(주종, 도수 등)이 비슷한 상품끼리 묶어 A 상품을 사용자가 좋아하면 A와 비슷한 주류를 추천해주는 방식이다. 만약 우리가 CB를 사용한다면, 주류 상품을 카테고리 또는 도수 등으로 분리하여 사용자가 A 주류를 좋아한다고 하면 해당 주류와 비슷한 주류를 추천해주면 된다. CB의 단점 또한 존재한다. 해당 주류의 데이터를 일일히 작성해야하는 번거로움이 있고, 이 데이터가 주관적인(맛, 설명) 데이터면 객관성이 떨어진다는 단점이 있다. CF(Collaborative Filtering) 내가 남긴 평점 데이터를 가지고 나와 취향이 비슷한 사람이 선호하는 아이템을 추천 CF는 프로필 데이터 없이 사용자의 발자취를 통해 추천을 진행한다. 예를 들어, 10명의 사용자들이 각자 자신의 선호도를 입력하게 된다면, 나와 비슷한 선호도를 가진 주류를 추천해주는 방식이다. CF는 신규 사용자이거나 초기 데이터가 없으면 추천 정확도가 급격히 떨어진다는 단점이 있다. 또한 매우 어렵다. 하지만 일반적으로 CB보다 정확하다고 한다. CB? or CF? 우리 주절주절에서는 처음 타겟팅 했던 추천 알고리즘은 협업 필터링(CF)이다. 그러나 초기데이터가 없는 것이 큰 문제다. 막상 CB를 적용시키기에도 아직 주류를 분류하는 기준이 주종밖에 없기 때문에 이마저도 어려워 보인다. 그래서 대량의 데이터를 만들던 어떻게든 방법을 찾아 CF를 적용하는 방향으로 가기로 했다. 아마도…? 협업 필터링(Collaborative Filtering) 협업 필터링 동작 방식 출처 아래는 각 크루들이 맥주에 대한 선호도 평가를 내린 것이다. 라거 에일 밀 둔켈 스타우트 피카 5 4 4 3 웨지 1 0 1 4 나봄 4 4 5 3 크로플 2 1 4 3 소롱 4 4 4 2 서니 4 2 3 1 티케 2 4 1 4 사용자 기반(User Based) 사용자 기반의 협업 필터링에서 유사도는 두 사용자가 얼마나 유사한 항목을 선호하는지를 기준으로 한다. 유사도를 구하는 방법에는 코사인 유사도, 피어슨 유사도 를 사용할 수 있다. 이 글에서는 코사인 유사도만 다뤄보기로 한다! 코사인 유사도 를 이용하여 웨지와 크로플의 유사도를 구해보자. 유사도를 구할 때에는 두 사용자가 공통으로 평가한 항목에 대해서만 계산한다. 크로플과 웨지의 유사도는 0.84라고 할 수 있다. 이런식으로 모든 사용자에 대해 유사도를 구할 수 있다. 피카 웨지 나봄 크로플 소롱 서니 티케 피카 1 0.84 0.96 0.82 0.98 0.98 0.86 웨지 0.84 1 0.61 0.84 0.63 0.47 0.73 나봄 0.96 0.61 1 0.87 0.99 0.92 0.93 크로플 0.82 0.84 0.97 1.00 0.85 0.71 0.97 소롱 0.98 0.63 0.99 0.85 1.00 0.98 0.72 서니 0.98 0.47 0.92 0.71 0.98 1.00 0.69 티케 0.86 0.73 0.93 0.97 0.72 0.69 1 이때 피카의 스타우트 평가 점수를 예측해보자! 우리는 피카와 유사한 몇명의 점수를 이용하여 점수를 구할 수 있고, 전체를 대상으로 예측 점수를 사용할 수도 있다. 전체를 대상으로 하여 평가 점수를 예측해보자. 피카의 스타우트 평가 점수는 2.7점으로 예측할 수 있다. 아이템 기반(Item-Based) 아이템 기반 또한 사용자 기반과 매우 유사한 과정을 거친다. 여기에서는 아이템들 간의 유사도를 계산한다. 라거와 에일의 유사도를 구하려고 한다면, 이 둘을 모두 평가한 사용자를 기반으로 계산한다. 각각에 대한 평가는 (5, 1, 4, 4, 2), (4, 0, 4, 2, 4) 이다. 에일과 라거 유사도는 0.91로 다소 높은 유사도를 보인다. 이는 일반적으로 라거를 좋아하면 에일을 좋아하고 에일을 좋아하면 라거를 좋아한다고 말할 수 있다. 우리는 사용자를 기반 으로한 협업 필터링을 적용하고자 한다! 협업 필터링 적용기 우리는 mahout 라이브러리를 사용하여 협업 필터링을 구현하고자 하였다. 파이썬 쪽으로는 추천 시스템 관련 라이브러리가 잘 되어있는데, 우리는 자바를 사용해야했기 때문에 mahout을 이용했다. mahout 적용은 이 글 을 따라서 적용하였다. gradle 추가 dependencies {
implementation('org.apache.mahout:mahout-integration:0.13.0') {
exclude group: 'log4j', module: 'log4j'
exclude group: 'org.slf4j', module: 'slf4j-log4j12'
}
'org.apache.mahout:mahout-integration:0.13.0' 을 통해 해당 라이브러리를 추가한다. 스프링 부트의 로그와 mahout 로그가 겹치기 때문에 충돌이 일어나 exclude를 해주었다. mahout 흐름
// DataModel 읽어들이기
DataModel model = new FileDataModel(new File(""data.csv""));

// 유저 기준으로 유사성 계산
UserSimilarity similarity = new PearsonCorrelationSimilarity(model);

// 0.1 보다 큰 유사성을 가진 데이터를 사용
UserNeighborhood neighborhood = new ThresholdUserNeighborhoodl(0.1, similarity, model);

// 유저 기준 추천 모델을 만든다.
UserBasedRecommender recommender = new GenericUserBased(model, neighborhood, similarity);

// 2번 유저에게 1개의 아이템을 추천한다.
List<RecommendedItem> recommendations = recommender.recommend(2, 1);
for (RecommendedItem recoomendation : recommendations) {
System.out.println(recommendation);
}
mahout 라이브러리를 이용한 협업 필터링의 큰 흐름은 다음과 같다. DataModel을 만든다. 유저 기준(UserSimilarity) 또는 아이템 기준(ItemSimilarity)으로 유사성을 계산한다. 유사도 기준으로 데이터를 분류한다. 유저 기준(UserBasedRecommender) 또는 아이템 기준(ItemBasedRecommender)로 추천 모델을 만든다. 유저가 추천 받을 아이템 개수를 입력한다. DataModel Mahout은 다양한 방식으로 데이터 모델을 만들 수 있도록 지원하고 있다. 사진에 보이는 것처럼 File을 직접 읽어와 DataModel을 만들 수도 있고, MySQL, MongoDB, H2, JDBC 등 DB에서 읽어와 DataModel을 만들 수 도 있다. 처음에는 local에서는 H2, prod와 dev에서는 MySQL 구현체를 사용하여 데이터 모델을 만들어 주는 방식을 사용했다. 그러나 실제 서버를 돌려보며 DB쪽에 많은 과부하가 걸려 다운되는 현상이 발생하여 협업 알고리즘을 최적화 하는 방식을 찾아보기로 하였다. 실제 내부 구현체를 보니 위의 사진처럼 하나의 메서드에서 데이터가 필요할 때마다 DB Connection을 열고 닫으며 쿼리를 날리고 있었다. 우리는 데이터를 한번만 가져와 필터링하면 되기 때문에 DataSource를 직접 넣어주어 인터페이스를 통해 구현체를 직접 만들어 사용했다. (나봄 구현) 테스트 협업 필터링을 실제로 구현하여 테스트한 결과
// 선호도_등록(주류ID, 선호도, 멤버) 메소드를 통해 선호도를 등록함

private void 협업_필터링_데이터_등록() {
memberAcceptanceTool.선호도_등록(obId, 2.0, PIKA);
memberAcceptanceTool.선호도_등록(stellaId, 5.0, PIKA);
memberAcceptanceTool.선호도_등록(kgbId, 4.5, PIKA);

memberAcceptanceTool.선호도_등록(obId, 1.5, SOLONG);
memberAcceptanceTool.선호도_등록(stellaId, 4.5, SOLONG);
memberAcceptanceTool.선호도_등록(kgbId, 4.2, SOLONG);
memberAcceptanceTool.선호도_등록(tigerId, 5.0, SOLONG);
memberAcceptanceTool.선호도_등록(appleId, 4.5, SOLONG);
memberAcceptanceTool.선호도_등록(tigerRadId, 4.5, SOLONG);

memberAcceptanceTool.선호도_등록(obId, 1.0, WEDGE);
memberAcceptanceTool.선호도_등록(kgbId, 4.5, WEDGE);
memberAcceptanceTool.선호도_등록(stellaId, 5.0, WEDGE);
memberAcceptanceTool.선호도_등록(tigerId, 4.7, WEDGE);
memberAcceptanceTool.선호도_등록(appleId, 4.5, WEDGE);
memberAcceptanceTool.선호도_등록(tigerRadId, 4.5, WEDGE);

memberAcceptanceTool.선호도_등록(obId, 4.7, CROFFLE);
memberAcceptanceTool.선호도_등록(kgbId, 1.5, CROFFLE);
memberAcceptanceTool.선호도_등록(stellaId, 2.4, CROFFLE);
memberAcceptanceTool.선호도_등록(tigerId, 2.1, CROFFLE);
}
현재 피카는 ob에 2점, 스텔라에 5점, kgb에 4.5점을 주었다. 따라서 피카는 ob는 좋아하지 않고, 스텔라와 kgb는 좋아한다는 것을 알 수 있다. 이와 비슷한 취향을 가진 멤버로는 소롱과 웨지가 있고, 반대되는 취향을 가진 멤버는 크로플이 있다. 협업 필터링은 나와 비슷한 사용자가 좋아하는 아이템을 추천해주기때문에 내가 아직 선호도를 체크하지 않았지만, 소롱과 웨지가 좋아하는 tiger, apple, tigerRad 를 추천해줄 것으로 기대된다. 실제 테스트 결과 실제로 앞서 말한 주류가 제일 상위에 떠있는 것을 확인할 수 있다. 마무리 실제 주절주절에서 사용된 로직은 앞서 소개한 것보다는 조금 더 복잡해서, 대략적인 큰 흐름만 살펴보았다. 많이 간소시켜 설명한 부분도 있으니 참고해서 읽었으면 좋겠다! 끝으로 협업 필터링은 나봄의 많은 도움으로 이루어졌기 때문에 감사인사를 표한다. 주절주절 화이팅!! 참고 자료 https://blog.pabii.co.kr/recommendation-algorithm-cb-cf-lf/ https://yeomko.tistory.com/3 https://scvgoe.github.io/2017-02-01-%ED%98%91%EC%97%85-%ED%95%84%ED%84%B0%EB%A7%81-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-(Collaborative-Filtering-Recommendation-System)/ https://mahout.apache.org/ https://iamksu.tistory.com/38",BE
294,"프록시 패턴으로 인터셉터 Path와 Method 설정하기 07 Aug 2021 on 조잘조잘 프록시 패턴으로 인터셉터 Path와 Method 설정하기 안녕하세요! 이번 포스팅에서는 스프링 인터셉터에 Path와 Method를 검증할 수 있는 기능을 프록시 패턴으로 구현하며 프록시 패턴에 대해 알아볼게요! 먼저 프록시에 대해 알아볼게요! 프록시(Proxy)란 대리자라는 뜻이에요. 프록시 패턴 또한 원래 객체를 대신해 무언가를 일을 처리하는 역할이에요. 이제 우리 프로젝트에서 적용한 예제를 통해 프록시 패턴에 대해 더 자세히 알아볼게요! public class WebMvcConfig implements WebMvcConfigurer {

private final JwtTokenProvider jwtTokenProvider;

@Override
public void addInterceptors(InterceptorRegistry registry) {
registry.addInterceptor(loginInterceptor())
.addPathPatterns(""/api/**"");
}

private HandlerInterceptor loginInterceptor() {
return new LoginInterceptor();
}
}
위 코드처럼 스프링에는 인터셉터에 원하는 요청 api에만 작동할 수 있는 기능을 제공해요. 하.지.만, GET, POST, PUT, DELETE 등과 같은 메서드를 구분하는 기능은 제공하지 않기 때문에 인터셉터 안에 다음과 같이 코드가 추가돼요! public class LoginInterceptor implements HandlerInterceptor {

private final JwtTokenProvider jwtTokenProvider;

@Override
public boolean preHandle(HttpServletRequest request, HttpServletResponse response,
Object handler) {

final String method = request.getMethod();
if(method.equals(HttpMethod.POST) || method.equals(HttpMethod.PUT)){
String token = AuthorizationExtractor.extract(request);
if (jwtTokenProvider.validateToken(token)) {
return true;
}
throw new UnauthorizedUserException();
}
return true;
}
}
여기서 또 애매한 건 각 api 요청마다 적용되는 메서드가 달라요. 예를 들어 /members/me 같은 경우는 모든 메서드에 LoginInterceptor 가 적용이 되어야하는 반면 /drinks 는 GET 요청을 제외한 메서드에 LoginInterceptor 가 적용이 되어야 했죠! 이러한 상황을 프록시 패턴을 이용해 해결할 거에요. 구조를 간단하게 그림으로 표현하면 다음과 같아요! 그림을 보면 로그인 인터셉터를 PathMatcher Interceptor가 감싸고 있어요. PathMatcher Interceptor는 로그인 인터셉터에 가는 요청의 흐름을 제어하게 되죠. 즉, PathMatcher 인터셉터가 로그인 인터셉터가 어떤 api + http method 에 적용이 될지 정해주는 프록시가 되는 거죠! 그럼 천천히 PathMatcher Interceptor 를 만들어볼게요! 먼저, PathMatcher Interceptor 도 스스로 인터셉터여야 해요. public class PathMatcherInterceptor implements HandlerInterceptor {

}
이 PathMatcher Interceptor 는 로그인 인터셉터뿐만이 아니라 어떠한 인터셉터이든 요청의 흐름을 제어가 가능한 인터셉터로 만들 것이기 때문에 인터셉터 자체를 필드로 받게 만들었어요! public class PathMatcherInterceptor implements HandlerInterceptor {

private final HandlerInterceptor handlerInterceptor;

public PathMatcherInterceptor(HandlerInterceptor handlerInterceptor) {
this.handlerInterceptor = handlerInterceptor;
}

}
다음은 url pattern과 HttpMethod 를 소지하고 target path 가 포함된 path 인지 확인 책임을 가진 PathContainer 라는 객체를 만들게요! PathMatcher 는 스프링에서 제공하는 AntPathMatcher 를 사용했어요. public class PathContainer {

private final PathMatcher pathMatcher;
private final List<RequestPath> includePathPattern;
private final List<RequestPath> excludePathPattern;

public PathContainer() {
this.pathMatcher = new AntPathMatcher();
this.includePathPattern = new ArrayList<>();
this.excludePathPattern = new ArrayList<>();
}

public boolean notIncludedPath(String targetPath, String pathMethod) {
boolean excludePattern = excludePathPattern.stream()
.anyMatch(requestPath -> anyMatchPathPattern(targetPath, pathMethod, requestPath));

boolean includePattern = includePathPattern.stream()
.anyMatch(requestPath -> anyMatchPathPattern(targetPath, pathMethod, requestPath));

return excludePattern || !includePattern;
}

private boolean anyMatchPathPattern(String targetPath, String pathMethod, RequestPath requestPath) {
return pathMatcher.match(requestPath.getPathPattern(), targetPath) &&
requestPath.matchesMethod(pathMethod);
}

public void includePathPattern(String targetPath, PathMethod pathMethod) {
this.includePathPattern.add(new RequestPath(targetPath, pathMethod));
}

public void excludePathPattern(String targetPath, PathMethod pathMethod) {
this.excludePathPattern.add(new RequestPath(targetPath, pathMethod));
}
}
이제 이 객체를 Path Interceptor 에 적용만하면 끝이에요! public class PathMatcherInterceptor implements HandlerInterceptor {

private final HandlerInterceptor handlerInterceptor;
private final PathContainer pathContainer;

public PathMatcherInterceptor(HandlerInterceptor handlerInterceptor) {
this.handlerInterceptor = handlerInterceptor;
this.pathContainer = new PathContainer();
}

@Override
public boolean preHandle(HttpServletRequest request, HttpServletResponse response,
Object handler) throws Exception {

// pathContainer에 해당 요청 url과 메서드가 포함되지 않다면 바로 서비스 로직으로 요청이 간다.
if (pathContainer.notIncludedPath(request.getServletPath(), request.getMethod())) {
return true;
}

// 해당 요청 url과 메서드가 포함이 되어있다면 본 인터셉터(로그인 인터셉터)에 요청을 위임한다. (인터셉터 기능 실행)
return handlerInterceptor.preHandle(request, response, handler);
}

// 외부에서 적용 path 패턴을 추가할 때
public PathMatcherInterceptor includePathPattern(String pathPattern, PathMethod pathMethod) {
pathContainer.includePathPattern(pathPattern, pathMethod);
return this;
}

// 외부에서 미적용 path 패턴을 추가할 때
public PathMatcherInterceptor excludePathPattern(String pathPattern, PathMethod pathMethod) {
pathContainer.excludePathPattern(pathPattern, pathMethod);
return this;
}
}
이제 로그인 인터셉터와 함께 인터셉터로 등록을 해볼까요? @Configuration
@RequiredArgsConstructor
public class WebMvcConfig implements WebMvcConfigurer {

private final JwtTokenProvider jwtTokenProvider;

@Override
public void addInterceptors(InterceptorRegistry registry) {
registry.addInterceptor(loginInterceptor())
.addPathPatterns(""/api/**"");
}

private HandlerInterceptor loginInterceptor() {
final PathMatcherInterceptor interceptor =
 // PathMatcherInterceptor 에 로그인 인터셉터를 주입한다.
new PathMatcherInterceptor(new LoginInterceptor(jwtTokenProvider));

// PathMatcherInterceptor를 로그인 인터셉터인 것처럼 인터셉터로 등록한다.
return interceptor
.excludePathPattern(""/**"", PathMethod.OPTIONS)
.includePathPattern(""/members/me/**"", PathMethod.ANY)
.includePathPattern(""/drinks/*/reviews/**"", PathMethod.ANY)
.excludePathPattern(""/drinks/*/reviews"", PathMethod.GET);
}
}
위와 같은 PathMatcherInterceptor 를 만든다면 어떠한 인터셉터이든 저렇게 편하게 PathPattern 을 제어할 수 있어요! 마무리 이렇게 스프링 인터셉터에 Path와 Method를 검증할 수 있는 기능을 구현해봤어요! 프록시 패턴은 정말 많은 곳에서 사용이 가능한 유용한 패턴이에요! 객체의 정보를 늦게 가져올 수 있는 Lazy Loading 을 적용할 수도 있고 위처럼 실제 객체 사용에 접근에 대한 흐름제어도 가능하죠! (이것 말고도 다른 많은 활용법이 있어요!) 실제 객체 대신 무언가 일 처리가 필요하다면 프록시 패턴을 이용해보면 어떨까요~? =]",BE
295,"젠킨스를 실행하며 우당탕탕 도커에 입문하기 24 Jul 2021 on 조잘조잘 jujeol 프로젝트 우당탕탕 도커 알아보기 안녕하세요! 이번에 주절주절은 도커로 젠킨스를 사용하게 되었습니다. 이번 포스팅에서는 도커를 처음부터 하나하나 짚어가는 것이 아니라 도커로 젠킨스를 띄우기까지 과정을 통해 도커를 알아보기 위함입니다! (야생 스타일) 젠킨스 이미지 가져오기 저희 프로젝트는 자바 11 버전 이상에서만 가능한 프로젝트에요. 그렇기에 자바 11 버전 이상을 지원하는 젠킨스 이미지를 가져와야 했어요! 이미지와 컨테이너 도커를 이해하려면 이미지와 컨테이너를 이해해야해요! 이미지란 파일이라고 생각하면 편할거에요. 파일 시스템과 실행할 어플리케이션 설정을 하나로 합친 것이죠. 컨테이너는 이미지(파일)를 담은 박스라고 보시면 돼요! 이미지를 받아와 격리된 실행환경을 제공받을 수 있는 컨테이너에 담아 실행할 수 있답니다! 컨테이너는 모두 격리되어있기에 마치 새로운 컴퓨터로 실행한 것처럼 모두 격리되어있어요! https://www.jenkins.io/doc/administration/requirements/jenkins-on-java-11/ 젠킨스 공식 홈페이지에서 자바 11 버전으로 이미지를 가져오려면 다음과 같은 명령어를 쳐야한다고 하네요. docker pull jenkins/jenkins:jdk11 💡docker pull 이란? 혹시 깃헙을 아시나요? 깃헙처럼 도커도 이미지를 인터넷에 저장할 수 있는데 이 저장소를 도커허브라고 불러요. docker pull 이미지이름:태그 명령어를 친다면 그 저장소에서 해당 이미지를 가져오게 돼요. 자, 이제 이미지를 잘 가져왔는지 확인해볼까요? 확인하는 명령어는 docker images 로 확인할 수 있어요. (내가 가지고 있는 이미지들의 리스트를 보여주는 명령어에요) 오! 잘 가져왔네요. 젠킨스 이미지 실행하기 이제 이미지를 컨테이너로 실행해야해요. 공식사이트에는 다음 명령어가 있네요. docker run --rm -ti -p 8080:8080 -p 50000:50000 -v jenkins-home:/var/jenkins_home jenkins/jenkins:jdk11 위 명령어를 분석해볼게요. docker run 이란 이미지를 컨테이너로 만들어주고 실행해주는 명령어에요. 기본 포맷은 다음과 같아요. docker run (<옵션>) <이미지 아이디 혹은 이름> (<명령어>) (<인자>) --rm 컨테이너를 멈추면 아예 컨테이너를 종료해준다는 옵션이에요. -ti -t 는 terminal 이고 -i 는 interactive 입니다. 컨테이너 내부 터미널로 실행하고 싶을 때 사용하는 명령어에요. -p 포트를 연결해주는 명령어에요. 컨테이너는 모두 격리되어있어요. 즉, 내 컴퓨터(호스트)에서 실행이 되어도 컨테이너는 호스트와 격리된 환경을 사용하고 있는 것이죠. 호스트에서 컨테이너로 포트로 연결을 이어줄 때 사용이 됩니다. <호스트 포트>:<컨테이너 내부 포트> 를 입력하게 되면 호스트 포트에서 컨테이너 내부 포트로 연결(bind)이 되어요. 만일, 8888:8080 이라고 입력하게 되면 호스트의 8888포트가 컨테이너 내부 8080포트로 연결이 된다는 의미에요! 젠킨스는 기본적으로 8080포트(master port)와 50000포트(slave port)를 사용하기에 호스트 포트와 연결을 해주는 작업이에요. 젠킨스의 master and slave architecture에 대해 궁금하다면 https://www.edureka.co/blog/jenkins-master-and-slave-architecture-a-complete-guide/ 에 잘 나와있네요! -v 호스트와 컨테이너 간의 볼륨 설정을 위해 사용되는 명령어에요. 호스트와 컨테이너는 서로 격리되어 있지만, 컨테이너의 데이터를 호스트에 저장하고 사용하는 방식입니다. 마치 게임 세이브 하듯이요. 이렇게 저장하고 사용한다면 컨테이너를 지우고 다시 시작해도 같은 볼륨을 바라본다면 이전 컨테이너에서 저장한 데이터를 그대로 사용할 수 있게 돼요. <호스트 디렉터리>:<컨테이너 디렉터리> 위 명령어를 사용하면 굉장한 출력이 나올거에요. 잘 돌아가는 것 같은 출력! 하지만 불편한 게 하나 있어요. 터미널을 종료하면 컨테이너(젠킨스)도 같이 종료가 되죠. 그리고 콘솔모드로 실행이 되는 것이 불편해요. 실행하되 백그라운드 모드로 실행하는 옵션은 -d 에요. 그럼 지금 컨테이너는 ctrl+c로 종료하고 다시 명령어를 입력해 볼게요(현재 --rm 옵션으로 실행되기 때문에 멈추면 자동으로 컨테이너가 종료돼요.). docker run -d -p 8080:8080 -p 50000:50000 --name jenkins -v jenkins-home:/var/jenkins_home jenkins/jenkins:jdk11 --name 명령어를 통해 컨테이너에 jenkins라는 이름을 주었어요. 이제 명령어를 입력해볼게요. 이전의 현란하던 콘솔은 사라지고 이상한 해시값만 주어진 채 우린 다시 터미널을 입력할 수 있는 상태로 넘어왔어요. 컨테이너가 잘 돌아가는 지는 docker ps -a 라는 명령어를 통해 확인할 수 있어요. 💡 도커 이미지, 컨테이너 확인하기 이미지 확인하기 docker images 실행중인 컨테이너 확인하기 docker ps 모든 컨테이너 확인하기 docker ps -a 컨테이너가 잘 돌아가고 있네요. 💡컨테이너 생명주기 실행 중 : 컨테이너가 실행중인 상태를 이야기한다. 정지 : 컨테이너가 정지된 상태를 이야기한다. 파기 : 컨테이너가 종료되어 삭제된 상태를 이야기한다. 컨테이너 생성에서 삭제까지 명령어 컨테이너 생성하기 : docker create <이미지 식별번호> 컨테이너 시작하기 : docker start <컨테이너 식별번호> 컨테이너 실행하기 : docker run <이미지 식별번호> (생성과 시작을 동시에 한다.) 컨테이너 중지하기 : docker kill <컨테이너 식별번호> 혹은 docker stop <컨테이너 식별번호> 컨테이너 삭제하기 : docker rm <컨테이너 식별번호> 이제 localhost:8080 을 접속해볼게요. (host ip:port 로 접근하면 돼요!) 잘 나오네요!! 패스워드를 입력하라고 하는데 패스워드는 어디서 가져올 수 있을까요…? 위를 보면 힌트가 나와있어요! /var/jenkins_home/secrets/initialAdminPassword 에 패스워드가 들어있다고 하네요. 저 패스워드를 확인하기 위해서는 도커 컨테이너 내부의 저 파일을 확인해야하는데 어떻게 확인할 수 있을까요? 도커 컨테이너에 접근하기 이번엔 도커 컨테이너 내부로 접근할 거에요. docker exec <컨테이너> <명령어> 이라는 명령어를 통해 컨테이너 내부에 명령어를 전달할 수 있어요. linux 명령어인 cat 이라는 명령어를 컨테이너 내부에 전달해볼게요. docker exec jenkins-prod cat /var/jenkins_home/secrets/initialAdminPassword 하니 비밀번호가 나오네요. 출력된 비밀번호를 입력해 젠킨스에 들어가면 해결! 💡 컨테이너에 명령어 전달 방법 docker run <컨테이너> <명령어> : 컨테이너를 실행할 때 명령어를 전달할 때 docker exec <컨테이너> <명령어> : 실행중인 컨테이너에 명령어를 전달할 때 컨테이너 내부 터미널로 접근하고 싶을 때 docker exec -it <컨테이너> bash 도커 컨테이너 삭제 이후 다시 시작하기 실험 정신 + 테스트로 젠킨스 설정을 이것저것 만지다가 걷잡을 수 없을 때가 있을거에요. (저는 자주 그렇답니다.) 돌이킬 수 있는 방법은 그냥 삭제하고 다시 시작하기라고 저는 생각합니다…xD 이제 컨테이너를 삭제하는 방법을 알아볼게요. 먼저, docker ps -a 를 통해 컨테이너 정보를 확인해요. jenkins가 실행되고 있네요! 삭제하기 전에 docker stop jenkins 명령어를 통해 정지해야해요. 이제 컨테이너를 삭제해봅시다! docker rm jenkins 💡 삭제 명령어 docker rmi <이미지> : 이미지 삭제 docker rm <컨테이너> : 컨테이너 삭제 docker rm $(docker ps -aq) : 모든 컨테이너 삭제 ($(~) 는 다른 명령어에서도 활용 가능해요!) docker system prune : 사용하지 않는 컨테이너 자원 모두 삭제 (제가 좋아하는 명령어에요!) 도커 볼륨 이제 다시 컨테이너 명령어를 입력해볼까요? docker run -d -p 8080:8080 -p 50000:50000 --name jenkins -v jenkins-home:/var/jenkins_home jenkins/jenkins:jdk11 다시 초기화 되어 있겠지 라는 기대감에 [localhost:8080](http://localhost:8080) 접근하면 이전 설정이 그대로 남아있는 것을 발견하실 거에요. ‘-‘?. 이러한 현상은 우리가 -v 를 통해 볼륨을 설정했기 때문이에요. 내 컴퓨터(호스트)에 세이브 파일이 연결되어있는 거죠! 이제 컨테이너 정지 + 삭제하고 우리의 세이브 파일(볼륨)도 삭제해볼게요. 우리가 갖고 있는 볼륨 정보를 확인하려면 docker volume ls 를 입력하면 돼요. jenkins-home 이라는 볼륨이 존재하네요! 이제 docker volume rm jenkins-home 명령어를 통해 볼륨을 삭제한다면 볼륨이 삭제가 된답니다. (docker volume prune 명령어를 이용해 사용하지 않는 볼륨도 삭제할 수 있답니다!) 이제 다시 시작하면 처음부터 실행이 되네요! 💡 docker inspect 관련 정보를 전부 볼 수 있는 단축키! 제가 제일 좋아하는 단축키에요. 컨테이너, 이미지, 볼륨 식별번호를 후에 입력하게 되면 관련 정보가 다 뜬답니다! 어떤 설정들이 적용되어 있나 확인이 가능하고 공부하기에 매우 좋은 단축키 같아요! 정리 💡 단축키 정리 컨테이너 및 이미지 확인 - 이미지 확인하기 docker images - 실행중인 컨테이너 확인하기 docker ps - 모든 컨테이너 확인하기 docker ps -a 컨테이너 생성에서 삭제까지 명령어 컨테이너 생성하기 : docker create <이미지 식별번호> 컨테이너 시작하기 : docker start <컨테이너 식별번호> 컨테이너 실행하기 : docker run <이미지 식별번호> (생성과 시작을 동시에 한다.) 컨테이너 중지하기 : docker kill <컨테이너 식별번호> 혹은 docker stop <컨테이너 식별번호> 컨테이너 삭제하기 : docker rm <컨테이너 식별번호> 컨테이너 명령 전달 docker run <컨테이너> <명령어> : 컨테이너를 실행할 때 명령어를 전달할 때 docker exec <컨테이너> <명령어> : 실행중인 컨테이너에 명령어를 전달할 때 컨테이너 내부 터미널로 접근하고 싶을 때 docker exec -it <컨테이너> bash 삭제 명령어 docker rmi <이미지> : 이미지 삭제 docker rm <컨테이너> : 컨테이너 삭제 docker rm $(docker ps -aq) : 모든 컨테이너 삭제 ($(~) 는 다른 명령어에서도 활용 가능해요!) docker system prune : 사용하지 않는 컨테이너 자원 모두 삭제 (제가 좋아하는 명령어에요!) 정보 확인 명령어 docker inspect <식별> 도커 볼륨 명령어 docker volume <옵션> 도커로 젠킨스를 실행하며 도커에 대해 간단히 알아봤어요!! 개인적으로 기술을 사용해가며 사용법을 익히는 방법을 선호하기에 글 정리도 위와 같이 진행되었네요. 도커파일과 도커컴포즈는 다루지 않았어요! 관련 내용은 다음 포스팅을 기대해주세요!",BE
297,"[git] git submodule 적용하기 12 Jul 2021 on 조잘조잘 jujeol 프로젝트 git submodule 적용기 💡 jujeol-jujeol git submodule 적용기 git submodule 이란? 레포지토리 안의 레포지토리입니다! 외부 라이브러리나, 다른 프로젝트를 내 레포지토리에 추가하고 싶은 일이 생겼을 때, 서브 모듈을 통해 레포지토리를 추가할 수 있어요. jujeol팀 에서는? 소셜 로그인에 필요한 key나, AWS를 활용하면서 필요한 key 처럼 공개 레포지토리에 올려두면 안 되는 credential 관련 내용들을 private repository에서 관리하기 위해 활용하였습니다! submodule을 적용하면 하나의 디렉토리가 해당 서브모듈의 repository로 변경됩니다. 적용기 레포지토리 만들기 우선 submodule 을 위한 레포지토리를 생성해야 합니다. 팀 organization 계정 을 만들고 비밀 정보를 보관할 레포지토리를 private으로 만들어주었습니다. 비밀 파일 만들기 기존에 개발자의 usb에서 usb로 전해지던 전설의 파일(application-oauth.yml)을 private repository에 루트 디렉토리에 생성해 줍니다. root
└── application-oauth.yml
main 브랜치에 commit을 해둡니다. 메인 프로젝트에 서브모듈 추가하기 다음 명령어를 통해 submodule을 추가합니다. 마지막 인자를 주지 않는다면 레포지토리 이름으로 디렉토리가 생성됩니다. # main project에서

git submodule add -b main https://github.com/jujeol-jujeol/[나만의 비밀 레포지토리].git
# -b main은 생략 가능

git submodule add -b main https://github.com/jujeol-jujeol/[나만의 비밀 레포지토리].git [나만의 비밀 디렉토리]
# git clone 때처럼 target folder를 지정할 수도 있다.
위 명령어의 결과로 .gitmodules 라는 파일이 생성됩니다. 내용을 까봅시다. [submodule ""auth""]
path = auth
url = https://github.com/jujeol-jujeol/jujeol-auth.git
auth 라는 이름으로 비밀 디렉토리를 생성해서 메인 프로젝트의 구조가 다음처럼 되었습니다. root
├── backend
├── frontend
└── auth # 새롭게 추가한 submodule
서브 모듈을 추가한 내용을 커밋해야 합니다. 다음 명령어로 커밋해 줍시다 git add *
git status # 상태확인
git commit -am ""feat: 서브모듈 적용""
커밋할 때 create mode 160000 [서브모듈 디렉토리] 메세지가 나오는데, 160000은 이 디렉토리를 다른 디렉토리와 다르게 취급하겠다는 의미라고 합니다. 원격 레포지토리로 푸쉬해줍시다. 팀원들에게 submodule을 적용시키기 최초 1회 : 다른 팀원의 경우 메인 프로젝트를 먼저 클론한 후, 다음 명령어를 입력한다. 최초 project clone시에 하면 되고, 이후로는 git clone –recurse-submodule # main project에서
# 명령 전후로 'git config --list --local'를 확인해 보자

# 서브모듈 시작
git submodule init
# 이 과정에서 private repository의 경우 github ID, password를 물어볼 수 있습니다.

# clone submodules
git submodule update

# 모든 서브모듈에서 main으로 checkout 한다 (당신은 master branch일지도)
git submodule foreach git checkout main
업데이트 시 : 이미 서브 모듈을 불러 온 이후 서브 모듈의 커밋을 불러와야 하는 상황이라면, 다음 명령어를 입력한다. # 메인프로젝트 루트에서
git submodule update --remote --merge
# --remote 뒤에 특정 레포지토이름을 인자로 주어 특정 submodule만 적용할 수 있다.
# --init 옵션을 준다면 현재 메인 프로젝트 커밋이 바라보고 있는 내용을 clone한다. --remote는 서브모듈 레포지토리의 최신 커밋을 가져온다.
# --merge
서브모듈 내용을 수정했을 때 1 - 커밋 이 문장을 마음에 새깁시다. 선 서브 후 메인!! 서브 모듈 디렉토리에서 무언가를 수정 하면, 경로를 서브모듈 경로로 이동한 뒤 일반 레포지토리와 똑같이 진행하면 됩니다. # 서브모듈 경로에서
# 헤드가 detached 상태인지 먼저 확인한다
$ git status

# 맞다면 체크아웃, 아니라면 패스
$ git checkout master
$ git pull

# 이후 변경내용 push
$ git commit -m ""커밋""
$ git push
유념해야 할 것은, 서브 모듈의 커밋을 메인 커밋보다 먼저 해야한다는 것입니다!! 메인 프로젝트는 서브 모듈을 그대로 저장하는 게 아니라, 서브 모듈의 url, path, commit을 저장합니다. 만약 메인 프로젝트를 먼저 커밋하고 서브모듈을 커밋하면, 메인 프로젝트가 나중에 커밋된 서브모듈의 변경 사항을 추적하지 못하므로 의도하지 않은 오류가 생길 수 있습니다. 서브모듈 내용을 수정했을 때 2 - 푸쉬 복습차원에서 ^^ 선 서브 후 메인!! 푸쉬도 커밋과 마찬가지로, 서브 먼저 , 메인을 다음에 하는 습관을 들여야한다. 기껏 서브모듈에서 먼저 커밋한 내용을 메인에서 커밋했다손 쳐도, 메인프로젝트 커밋에 포함된 서브모듈 커밋이 푸쉬되어 있지 않다면, 다른 개발자가 내 프로젝트를 클론해오고 서브모듈 또한 클론해올 때 서브모듈의 커밋을 찾아올 수 없곘죠… ㅠ 그런 상황을 방지하기 위해 깃에서는 두가지 기능을 제공합니다. # main project를 push하기 전에
# 1) submodule이 모두 push된 상태인지 확인하고, 확인이 되면 main project를 push
git push --recurse-submodules=check
# 2) submodule을 모두 push하고, 성공하면 main project를 push
git push --recurse-submodules=on-demand
기초 설정으로 잡고가고 싶다면 다음 설정을 부여할 수 있습니다. # default 설정 잡기
# push 시에 항상 check
git config push.recurseSubmodules check
# push 시에 항상 on-demand
git config push.recurseSubmodules on-demand
서브모듈을 도입 후 gradle에서 활용하기 서브 모듈은 도입했지만, backend 프로젝트 바깥에 있는 설정 파일을 빌드에 포함시켜야 하는 문제가 있었습니다. spring.config.include 키워드를 통해 외부 설정 파일을 import해보려고 했지만, 개발자가 어떻게 모듈을 설정했느냐에 따라 다르게 동작하여 기각했습니다. gradle.build를 활용해 빌드시에 backend와 같은 depth에 있는 파일을 복사하는 방안이 적절해보여 채택했습니다. processResources.dependsOn('copySecret')

task copySecret(type: Copy) {
from '../auth/application-oauth.yml' // auth 서브모듈에 있는 oauth 설정파일을 복사한다.
into 'src/main/resources'
}
결론 jujeol팀이 credential 파일을 private repository에서 관리하기 위해 도입한 submodule 도입기였습니당~! credential 파일은 내부 레포지토리라 볼 수 없지만 gradle 설정 등은 프로젝트 레포지토리에서 확인하실 수 있습니다. 참고 git-scm pinedance님 블로그",NON_TECH
315,"### 이슈 발생

프로젝트를 진행한지 6주정도 되었을 때, 실제 서비스를 위해
어느정도 기능이 구현되었다 생각했고, 우리는 도네이션을 하기 위한 PG 신청 작업을 진행했습니다..

하지만, 신청 및 문의한 PG사 대부분에서 저희 서비스 입점을 거절했었는데요.
PG사마다 서비스 정책이 다른만큼 반려사유가 다양했지만,
전체적으로 TYF의 **후원성 서비스 모델이 문제**가 되는 듯 보였습니다.

결국, 팀의 논의를 통해 모 PG사의 구체적인 반려사유였던
**후원자가 창작자에게 직접적으로 돈을 전달하는 과정**에 대한 해결방안으로 제안해준 방식을 이용해 문제를 해결해보고자 했습니다.

### 포인트 시스템 도입 결정

앞서 보았듯, PG사 승인을 받기위한 가장 확실한 방법은 **포인트 시스템을 도입**하는 것이였습니다.

하지만, 기존의 전체 프로세스에 포인트 시스템을 넣는건 비교적 큰 작업이였습니다.

서비스에서 제공하고 잇는 후원, 정산, 환불, 조회, 약관 등등 거의 대부분의 시스템에 포인트 기능이 들어가야했기때문에 수정해야될 것들이 많았습니다.

이러한 방대한 양때문에 저희 팀은 분야 상관없이 전부 모여,

제공하고 있는 기능들을 하나하나 가져와 어떤 형태로 포인트를 적용할 것인지에 대해
이야기하는 과정을 거쳐 요구사항을 명확하고 구체화 해나가기 시작했습니다.

### 서비스 플로우 재구성하기

포인트 시스템 도입을 위해 우선 서비스의 플로우부터 재구성했습니다.

전체적인 서비스 플로우를 후원자가 직접 창작자들에게 결제하는 방식이 아닌,
**후원자가 포인트를 구매하고 그 포인트를 창작자들에게 도네이션 하는 형태**로 변경했습니다.

결국 우리가 중간에서 돈을 가지고 있어야 했고, 이를 포인트 형태로 사용자에게 제공하는 방식이였는데요.

이에 맞추어 후원자에게는 포인트를 조회, 충전 할 수 있는 기능이 추가되어야 했고,
기존에 제공되던 후원, 정산, 환불 기능들을 실제 돈이 아닌 포인트를 기준으로 운영되게끔 변경했습니다.

### 백엔드 변경점

백엔드 팀에서 포인트 시스템을 적용하기 위해 전반적인 코드를 뜯어 고쳤습니다.

그중 가장 핵심이 되는 부분을 간단하게 이야기하자면,
우선 결제(payment)와 후원(donation) 엔티티의 의존성을 분리했습니다.

기존에는 둘의 연관 관계가 1:1 관계였고, 결제가 생성되어야 후원을 생성할 수 있었습니다.

이러한 방식을 결제와 후원을 서로 별개의 개념으로 가져가도록 연관관계를 분리했는데요.
이에 따른 모든 비즈니스 코드 및 테스트 코드들도 수정했습니다.
### 프론트엔드 변경점

포인트 시스템을 적용하면서 후원 프로세스에 포인트 정보를 보여주어야되었기에 프론트엔드에서도 많은 변경점이 있었습니다.

도네이션과 관련된 대부분의 기능의 뷰에 포인트 정보를 보여주게끔 바뀌었습니다.

또, 후원자는 자신의 포인트를 조회하고 그 포인트를 도네이션 하는 방식으로 변경되었는데,
이 과정에서 후원자가 포인트를 조회, 충전하는 과정이 필연적이였기에 기존에 비회원 방식으로 이루어지던 도네이션을 후원자의 회원가입이 필수적으로 요구하게끔 되었습니다.
### 후기

이후 2주간의 포인트 시스템 적용 스프린트를 거친후 변경사항과 함께 PG 승인을 재요청했습니다.

그리고 PG사에서 1차적으로 승인메일을 받았고, 이후 정상적으로 결제모듈을 연동하는데 성공했습니다.
어찌보면 프로젝트의 가장 핵심이 되는 기능이 바로 결제였고,
기술적인 문제보다 외부적 요인이 크게 영향을 주는 부분이라 걱정이 많았었습니다.

여기에, 한번 승인 요청 거절을 당하고 그 원인이 전체 서비스의 중심이 되는 플로우였던 만큼 힘들었는데요.

팀원들과 하나하나 어떻게 개선할지 논의하고 세부화하는 과정을 거쳐 무사히 포인트 시스템을 이식하는데 성공하고 실제로 결제가 되는 모습을 보는 순간의 짜릿함을 잊지 못할것 같습니다.",NON_TECH
331,"Thank you for _ _ _

TYF 개발이야기

결제모듈 도입기 및 성능테스트를 위한 TYF-PAY - 수리

안녕하세요. 땡큐포팀의 수리입니다.

TYF의 목표는 누구나 쉽게 창작자를 응원할 수 있는 간편 도네이션 플랫폼을 만드는 것 입니다. 저희 서비스에서 후원 만큼 응원 메시지 또한 중요한 요소입니다. 맨 처음 기획 시 결제 시스템에 대한 경험이 많지 않아 결제의 대안으로 송금 QR 방식을 생각 했습니다. 그러나 이것이 서비스의 목표를 달성하기에 적절하지 않은 방식임을 깨닫고 결제 모듈 도입으로 방향을 틀게 되었습니다.

해당 문서에서는 결제모듈 도입과정과 성능 테스트 시 결제 모듈에 대해 어떻게 처리 했는지에 대해 정리해보려고 합니다.

QR 송금의 한계

송금 QR을 도입하기 전, 가장 유명한 카카오페이의 송금 QR을 바탕으로 실험을 해보았습니다. 송금 QR을 후원자가 찍게되면, 이후의 흐름은 카카오페이 쪽에서 처리 하게 됩니다. 송금이 얼마 되었는지 혹은 완료 되었는지에 대한 응답을 받을 수 없었습니다.

저희 서비스의 원래 기획인 후원과 메시지를 함께 라는 조건을 만족하기 부적합 하였습니다. 송금 완료에 대한 응답을 받지 못하면, 후원이 완료되면 메시지를 작성한다 라는 시나리오를 충족 할 수 없었습니다.

그래서 포인트를 통한 후원으로 노선을 변경하였고, 결제모듈의 도입을 고려하게 됩니다.

결제모듈의 도입 아임포트

결제 모듈의 대안으로 최근 인기를 얻고 있는 아임포트를 도입하였습니다.

아임포트를 도입한 TYF의 결제흐름은 다음과 같습니다.

먼저 TYF 클라이언트에서 TYF서버로 결제준비 API를 호출합니다. 이때 해당 결제에 대한 아이템 정보(아이템 이름, 금액 등)를 보내게 됩니다. 그러면 TYF 서버는 새로운 Payment를 생성하고 DB에 저장합니다. TYF클라이언트에게 주문번호(merchant_uid)를 전달합니다. 현재 Payment의 상태는 Pending 입니다.

TYF 클라이언트는 MerchantUid를 바탕으로 아임포트에 결제 요청을 보냅니다. 결제가 완료되면 아임포트는 결제번호(imp_uid)를 TYF 쪽으로 보내줍니다.

TYF 클라이언트는 해당 imp_uid와 merchant_uid를 다시 TYF 서버쪽으로 전달합니다. 서버에서 해당 imp_uid에 대한 결제 정보를 아임포트에게 요청합니다. 서버는 아임포트로 부터 전달된 결제 정보와 TYF DB에 저장된 결제 정보를 대조하여 해당 결제에 대한 유효성을 검증합니다.

결제 요청은 클라이언트 상에서 이루어지는데, 공격자가 스크립트를 조작하여 금액을 낮은 값으로 변조 할 수 있습니다. 그래서 결제 후 서버에서 결제금액의 위변조 여부를 검증합니다.

유효성검증이 통과되면 Payement의 상태는 Paid가 됩니다.

성능테스트 시 결제모듈

TYF의 결제 시스템은 아임포트라는 외부모듈에 의존하고 있습니다.

성능테스트를 하기 위해서는 실제 서비스 환경과 유사한 환경에서 진행되어야 합니다. 어플리케이션의 동작과 자원의 사용을 모두 보아야 하기 때문입니다.

그러던 도중 우아한 형제들 기술블로그에서 결제 시스템의 성능, 부하, 스트레스 테스트 라는 글을 접하게 되었습니다.

결제 시스템 성능, 부하, 스트레스 테스트 | 우아한형제들 기술블로그
안녕하세요. 우아한형제들에서 결제시스템을 개발하고 있는 권용근입니다. 입사한 지 4개월 만에, 드디어 우아한형제들 기술 블로그에 글을 남기게 되어 감회가 새롭습니다. 저는 최근 결제 시스템의 개비를 진행하며 경험한 성능, 부하, 스트레스 테스트 경험을 작성해보려고 합니다. 입사하고 보니 저에게는 결제 API 단순화, 결제 시스템 데이터베이스 분리 및 파티션 도입, 비동기 결제 시스템 개발 이라는 굵직굵직한 작업들이 기다리고 있었습니다.
https://techblog.woowahan.com/2572/

해당 글을 참조하여 가짜 PG사인 TYF PAY 만들고, 테스트 하려는 시스템과 분리 시켜 Mock Server를 띄웠습니다.

TYF 서버는 결제 모듈 연동을 아래와 같이 추상화 해놓았습니다.

Java
Copy
public interface PaymentServiceConnector {

PaymentInfo requestPaymentInfo(UUID merchantUid);

PaymentInfo requestPaymentRefund(UUID merchantUid);
}



해당 인터페이스를 사용하여 퍼포먼스 환경에서 동작하는 TYF PAY용 connector를 만들고, Mock Server와 연동하였습니다.

Java
Copy
@Profile(""performance"")
public class MockPaymentServiceConnector implements PaymentServiceConnector {
@Value(""${tyf_pay_api_url}"")
private String TYF_PAY_API_URL;
}



TYF PAY는 요청에 대해 항상 정해진 응답값을 반환하도록 구현하였습니다.

Java
Copy
@PostMapping(""/payments/find/{merchantUid}"")
public ResponseEntity<PaymentInfo> findPayment(@PathVariable String merchantUid) {
return ResponseEntity.ok(new PaymentInfo(
UUID.fromString(merchantUid),
PaymentStatus.PAID,
1100L,
""1000포인트 충전"",
""imp_uid"",
MODULE
));
}



이렇게 성능 테스트를 위한 Mock 결제 모듈 연동이 끝났습니다.

성능 측정에 관한 내용은 다른 포스팅에서 찾아뵙겠습니다.",BE
332,"Thank you for _ _ _

TYF 개발이야기

땡큐한 정산기 - 로키

안녕하세요 

땡큐포팀의 로키입니다 .

저희는 누구나 쉽게 창작자를 응원할 수 있는 간편 도네이션 플랫폼인 Thank You For 서비스를 제공하고 있습니다. 앞의 소개에서 알 수 있듯이 저희 서비스의 핵심은 도네이션(= 후원)인데요. 후원자가 창작자에게 도네이션을 하는 과정도 중요하지만 창작자가 받은 도네이션 금액을 한치의 오차도 없이 정산하는 과정 또한 매우 중요합니다. 이번 포스팅에서는 저희 팀이 어떤 과정을 거쳐 정산 시스템을 구축했는지 작성해보고자 합니다.

정산 가능 계정으로 전환 

땡큐포 서비스를 통해 후원을 받기 위해서는 먼저 정산 가능 계정으로 전환을 해야합니다. 정산 가능 계정으로 전환을 위해서는 계좌 인증 절차를 거쳐야합니다. 계좌 인증을 위해서는 이름, 주민등록번호, 은행 정보, 계좌번호, 통장사본 이미지가 필요합니다. 계좌 인증을 요청하면 관리자가 검토한 뒤 승인 혹은 반려하게 됩니다. (계좌 인증을 위해 입력하신 정보는 모두 철저하게 암호화하고 있으니 안심하세요! )

회원가입 완료화면
ALT
(계좌인증 전)정산 페이지 화면
ALT

회원가입 완료화면
ALT

회원가입 완료화면
ALT

(계좌인증 전)정산 페이지 화면
ALT

(계좌인증 전)정산 페이지 화면
ALT

초기 모델 (v1.x.x)

정산 시스템은 정산 페이지에서 부터 시작이되는데요. 정산 페이지의 초기 모델에서는 후원 완료 금액, 정산 가능 금액, 정산 완료 총액의 3가지 정보를 제공하고 정산 가능 금액의 금액만큼 정산 요청을 할 수 있었습니다. 그렇다면 후원 완료 금액은 무엇을 의미하는지 헷갈리시는 분들이 계실 것 같습니다. 이렇게 헷갈리는 용어들이 생겨난 이유는 초창기 버전에 도입한 도네이션 환불 정책에 있습니다.

저희가 v1.x.x에서 채택한 환불 정책은 아래와 같습니다. (v2에서는 다른 환불정책 도입)

Markdown
Copy
도네이션 날짜로부터 7일 이전 - 땡큐포 페이지 내에서 즉시 환불가능

도네이션 날짜로부터 7일 이후 - 땡큐포 페이지에서 환불 불가(창작자 재량하에 환불)



위 정책처럼 도네이션 일로부터 7일 이전의 금액을 합친 값이 후원 완료 금액, 도네이션 일로부터 7일 이후부터는 정산 가능 금액 그리고 정산 완료 총액은 정산받은 모든 금액을 뜻합니다.

이 금액들은 DonationStatus(REFUNDABLE, EXCHANGEABLE, EXCHANGED, CANCELLED)를 기준으로 필터링한 뒤, 도네이션 금액을 더한 결과들을 보여주도록 하였습니다. (CANCELLED의 경우 후원이 취소된 경우를 의미합니다.)

이 금액들은 Java8 API인 LocalDateTime을 활용하여 관리자 페이지에 접근하는 시점을 기준으로 현재 도네이션을 받은 시점간의 차이가 7일 이상인지 아닌지를 판단하여 Donation을 필터링하였고, 필터링한 Donation 금액을 합친 결과들을 보여주도록 하였습니다. (후원 완료 금액의 경우 Donation의 상태가 EXCHANGED인 모든 도네이션 금액을 더한 결과)

땡큐포 서비스 제공 방식 변경

이렇게 땡큐포의 정산 시스템이 구축되는가 싶었지만 PG사측에서 땡큐포의 서비스 제공 형식이 PG사 입점 기준에 부적합하다는 답변을 받게되었고, PG사 입점을 위해 서비스 프로세스를 변경하게 되었습니다. (서비스 구조를 변경하게 된 이야기는 인치의 [프로젝트 포인트 시스템 적용기]를 참고해주세요 )

현재 모델 (v2.0.0)

위의 사진은 현재 이 글을 작성하는 날짜 기준 운영중인 2.0.0 버전에서 채택한 정산 관리 페이지인데요.

v1.x.x에서는 회원가입 없이도 도네이션이 가능했지만 v2.0.0의 땡큐포는 로그인을 한 상태에서만 도네이션이 가능하도록 서비스의 프로세스가 변경되었습니다. 또한 창작자에게 바로 도네이션을 하는 방식이 아니라 포인트를 충전한 뒤 충전한 포인트로 후원을 할 수 있도록 서비스의 흐름이 변경됐습니다. 바뀐 서비스 흐름에 따라 환불 정책 또한 아래와 같이 변경되었습니다.

Markdown
Copy
포인트 충전 날짜로부터 7일 이전 - 땡큐포 페이지 내에서 즉시 환불가능
 ㄴ (단, 환불하려는 계정의 `소유 포인트 >= 환불하는 포인트` 여야 한다.)

포인트 충전 날짜로부터 7일 이후 - 떙큐포 페이지 내에서 환불 불가능, 관리자에게 문의



정산 도메인인 Exchange Entity 또한 변경되었습니다.

Markdown
Copy
변경 전: 이름, 이메일, 계좌번호, 정산 금액,

변경 후: member Entity(N:1), 정산 금액, 정산 날짜(YearMonth), 정상 상태

Exchange Entity Field Information
ALT


사실 이렇게 변경하면 끝이라고 생각했습니다.

하지만 서비스 흐름이 변경됨에따라 고민해야할 점들이 생겼습니다.

Q1. 비효율적인 정산 금액 계산 방식 

기존의 정산 시스템은 정산 금액을 조회할 때 마다 Donation Table을 Full Scan하여 정산 금액을 직접 계산하는 구조였습니다. 당연하게도 이 방식은 매우 비효율적이었습니다. 그리고 도네이션 서비스의 흐름이 바뀌면서 정산 서비스의 흐름 또한 변경되었고, 정산 도메인은 아래와 같은 기능이 요구되었습니다.

Markdown
Copy
- 정산 신청/승인 시 도네이션 테이블 풀스캔 피하기
- 어느 시점에 1번만 스캔해서 정산 금액을 만들고 이후 이 값을 조회한다.
- 복잡한 쿼리 호출을 최대한 줄이는 구조로 변경한다.
- 최근 정산 기록을 볼 수 있어야 한다.
- 신청한 달의 모든 후원을 정산한다. (그 달의 며칠에 신청을 하던 상관없다.)
- 특정달에 정산신청한 사람들을 조회할 수 있어야한다.
- 정산내역에서 반려내역도 볼 수 있어야한다. (Exchange에 Status 필요)



위의 요구사항을 만족시키기 위해서 Exchange라는 새로운 도메인이 등장하게 되었습니다.

Q2. 정산 환급일은 언제가 적절할까? 

이전 정산 시스템은 정산을 신청하는 시점에 정산받을 수 있는 금액만큼 정산 금액을 산정하여 Exchange 엔티티를 생성했지만 새로운 Exchange 엔티티는 신청한 달의 모든 후원을 정산하도록 변경되었습니다. 그리고 현재 땡큐포의 정산 시스템은 창작자가 정산을 신청하면 관리자가 확인한 후 정산 금액만큼 입금을 해주는 구조입니다. 정산 신청이 불규칙적으로 들어올 것이고 거기에 관리자가 매번 응대하는 것은 힘들다고 판단하였습니다. 논의 끝에 매달 7일에 정산 금액을 환급해주는 것으로 결정하였습니다.

하지만 환급일을 지정하게 되면서 다음과 같은 문제가 발생했습니다.

Q3. 정산 신청 시점에 그달의 마지막 날의 정산 금액이 다를 수 있다. ≠ 

예를 들어 정산 신청 시점이 10월 28일이고 10월 1일 ~ 10월 28일까지 후원받은 금액이 10,000원이라고 했을 때, 만약 10월 29 ~ 10월 31일 사이에 5,000원 후원을 받았다면 정산 신청 시점에 생긴 Exchange(10,000원)와 10월에 정산받아야하는 총 정산 금액 5,000원 만큼의 차이가 생기게 됩니다

저희 땡큐포팀은 정산 요청시 Exchange 생성, 매달 1일 00:00시에 Exchange 금액을 갱신하는 방식으로 해당 문제를 해결하였습니다.

Exchange 금액 갱신 상세 코드

DonationStatus가 WAITING_FOR_EXCHANGE인 Exchange 엔티티들을 조회하여 해당 일자로 정산 금액을 업데이트해주는 스케줄러를 등록하여 창작자가 정산 신청을 한 달동안 후원받은 모든 금액을 정산받을 수 있도록 구현하였습니다.

Java
Copy
@Scheduled(cron = ""0 0 0 1 * *"")
public void updateExchangeAmount() {
List<ExchangeAmountDto> exchangeAmountDtos = exchangeRepository.calculateExchangeAmountFromDonation(YearMonth.now());

Map<Long, Long> idAmountMap = exchangeAmountDtos.stream()
.collect(Collectors.toMap(ExchangeAmountDto::getExchangeId, ExchangeAmountDto::getExchangeAmount));

exchangeRepository.findAllById(idAmountMap.keySet())
.forEach(exchange -> exchange.updateExchangeAmount(idAmountMap.get(exchange.getId())));
}

스케줄러에 등록한 기능
ALT


Java
Copy
@Override
public List<ExchangeAmountDto> calculateExchangeAmountFromDonation(YearMonth exchangeApproveOn) {
return queryFactory
.select(new QExchangeAmountDto(exchange.id, donation.point.sum()))
.from(exchange)
.leftJoin(exchange.member.receivedDonations, donation)
.groupBy(exchange.id)
.where(
 waitingStatus(),
 waitingForExchangeStatus(),
 beforeStartOfThisMonth(exchangeApproveOn)
)
.fetch();
}

private BooleanExpression waitingForExchangeStatus() {
return donation.status.eq(DonationStatus.WAITING_FOR_EXCHANGE);
}

private BooleanExpression waitingStatus() {
return exchange.status.eq(ExchangeStatus.WAITING);
}

private BooleanExpression beforeStartOfThisMonth(YearMonth exchangeApproveOn) {
// 정산승인월 1일 00:00 이전
LocalDateTime startOfNextMonth = exchangeApproveOn.atDay(1).atStartOfDay();
return donation.createdAt.before(startOfNextMonth);
}

후원 받은 도네이션으로부터 정산 금액을 계산하는 기능
ALT


이상 땡큐한 정산기였습니다. 

끝까지 읽어주셔서 감사합니다.",BE
366,"Thank you for _ _ _

TYF 개발이야기

환불 시스템 구축기 - 조이

안녕하세요 [Thank You For] 의 백엔드 팀원 조이입니다.

[Thank You For] 간편한 도네이션을 통해 후원자와 창작자를 이어주는 서비스입니다. 후원자님은 창작자님에게 도네이션을 하기 위해 TP(땡큐포 포인트)를 충전합니다.

현재는 결제 수단이 카카오페이밖에 없지만 후에 다양한 결제수단을 제공할 예정이고, 여러 PG사 연동을 보다 빠르고 쉽게 하기 위해 아임포트라는 결제 호스팅 서비스를 이용하고 있습니다. 실제 현금이 오가는 서비스는 결제만큼 환불도 중요합니다. 아임포트에서 환불 API를 제공하기 때문에 그냥 호출하면 된다고 생각하실 수도 있지만 여러 정책적, 기술적 고민들이 있었습니다.

환불 프로세스

먼저 환불 프로세스의 대략적인 흐름을 살펴보겠습니다.

포인트 충전을 위해 카카오페이를 통해 결제가 완료되면 유저의 이메일 주소로 충전 결제 내역 메일을 전송합니다. 충전 결제 내역에는 결제에 대한 여러 정보가 있습니다. 환불을 위해선 결제 번호가 필요합니다.

환불 신청은 [Thank You For] 사이트의 환불신청 페이지에서 할 수 있습니다. 아래는 환불신청 페이지의 첫 화면입니다. 환불 신청을 위해 위의 메일에서 받은 결제 번호를 입력합니다.

결제 번호가 올바르다면 유저에게 환불 인증번호 메일이 전송됩니다.

도착한 메일에 들어있는 환불 인증번호를 사이트의 인증번호 칸에 입력합니다.

환불 인증번호가 올바르다면 결제 내역을 보여줍니다.

환불 신청하기를 클릭하면 환불이 완료되고 포인트가 차감됩니다.

환불 인증번호 저장/관리

결제 번호를 입력 후 메일로 환불 인증번호(이하 인증번호)가 전송되는데 이 인증번호는 시간제한이 있어야하고, 지정된 시간이 지나면 파기되어야 합니다. 새로운 인증번호를 발급하면 기존의 인증번호는 파기되어야합니다. 각 인증번호마다 시간제한을 다르게 가지고 있어야 합니다.

이런 요구사항을 고려하여 인증번호 저장/관리에 Redis를 사용하기로 결정했습니다. Redis는 ""key-value"" 구조의 인메모리 기반 NoSQL DBMS입니다. 빠른 저장/조회가 필요하거나 캐시와 같이 휘발되어도 치명적이지 않은 데이터를 저장하는데 주로 사용됩니다.

Spring Data에는 Redis를 위한 모듈이 있습니다. Spring Boot로 Spring Data Redis 모듈의 의존성을 쉽게 관리할 수 있습니다. build.gradle 에 아래와 같이 의존성을 추가해줍니다.

Plain Text
Copy
dependencies {
 implementation 'org.springframework.boot:spring-boot-starter-data-redis'
}


Spring Data Redis는 2가지 접근방식을 제공합니다. Redis Repository와 RedisTemplate 이 있는데, 저희는 Spring Data JPA 처럼 객체를 기반으로 저장하는 Redis Repository 방식을 선택했습니다. 아래 코드를 보시면 JPA 엔티티 클래스, 레포지토리와 매우 유사한 형식이란걸 아실 수 있습니다.

@RedisHash

클래스를 선언하고 @RedisHash를 붙이면 해당 클래스는 인스턴스를 레디스에 저장할 수 있게 됨. JPA의 @Entity 느낌!
프로퍼티는 value, timeToLive가 있다.

value(String): 엔티티가 레디스에 저장될 때의 키값(KeySpace)를 지정. 보통 클래스이름을 쓴다.

timeToLive(long): TTL(time to live: 객체가 레디스에 얼만큼의 시간동안 저장되어 있을지-만료시간)을 지정.

@Id

Id로 지정할 필드에 붙인다. 보통 String으로 선언하지만 다른 타입도 가능.

@TimeToLive

TTL을 지정하는 기능인데, @TimeToLive와 @RedisHash의 프로퍼티 timeToLive를 둘 다 지정하면 @TimeToLive의 우선순위가 높아 실제 TTL로 적용된다.

unit(TimeUnit): TTL의 단위. default 는 TimeUnit.SECONDS

인증번호 재전송

인증번호가 메일로 전송되지만 인증 메일이 제대로 가지 않은 경우도 있습니다. 많은 서비스에서는 인증번호를 재발급할 수 있는 기능이 있습니다. 그리고 인증번호 재발급을 너무 많이 할 수 없도록 재전송 대기시간을 두기도하죠. 땡큐포의 환불 프로세스에서도 인증번호 재발급, 재발급 대기시간이 있습니다.

만약 인증번호의 만료기한(TTL)과 인증번호 재발급 대기시간이 같다면 VerificationCode만으로 재발급 대기시간을 구현할 수 있습니다. 해당 결제번호(merchantUid)의 VerificationCode가 존재하지 않는다면 재발급을 해주고, 존재한다면 아직 재발급 대기시간이 지나지 않았으니 재발급을 할 수 없다는 응답을 주면 됩니다.

하지만 저희의 요구사항에는 인증번호의 만료기한은 5분, 재발급 대기시간은 1분입니다. 인증번호의 만료기한과 재발급 대기시간이 다르다면 VerificationCode만으로는 재발급 대기시간을 특정할 수 없기 때문에 대기시간을 관리할 CodeResendCoolTime를 만들었습니다.

인증 실패시 처리 정책

사용자가 인증번호를 입력하여 서버로 인증요청이 오면 VerificationCodeRepository에서 인증번호를 조회, 비교합니다. 인증번호가 맞다면 다음 절차를 진행하겠지만 틀릴 수도 있습니다. 저희는 인증번호가 틀렸을 때 10번의 재시도 기회를 주고 모든 재시도횟수를 소진할 때까지 인증번호를 맞추지 못했다면 사이트의 환불 페이지에서는 해당 결제를 환불할 수 없도록 막고, 고객센터 문의를 통해서 환불을 진행하도록 했습니다.

환불인증 토큰

유저가 입력한 환불 인증번호가 맞다면 결제 정보를 보여주고 유저가 마지막으로 확인할 수 있도록 합니다. 그 후 환불 신청 버튼을 누르면 서버에서 환불 절차가 진행됩니다. 설명한 기능은 2개의 API를 통해 구현되었는데, 2개 API 모두 유저인증이 필요한 민감한 정보를 제공하고 있습니다. 하지만 저희는 로그인을 하지 않은 유저를 가정하고 구현했기 때문에 별도의 인증 방법이 필요합니다.

인증번호 인증을 완료한 유저에게 환불 프로세스를 위한 JWT 토큰을 발급했습니다. 인증이 완료되면 환불인증 토큰을 반환하고, 결제정보 조회 API, 최종환불신청 API 요청에 같이 보내게 됩니다.",BE
368,"Thank you for _ _ _

TYF 개발이야기

Oauth2 정리 - 파즈

TYF는 Oauth2를 사용하여 로그인/회원관리를 사용하고 있다.
채택 이유는 많은 개인정보를 직접 관리하기 부담스럽기도 하고, 사용자 입장에서 우리 서비스에 직접 개인정보를 작성하는 것 보다 다른 큰 서비스를 이용하는 것이 더 믿음을 가질 것이라고 예상했기 때문이다.

프로세스 흐름

로그인 흐름

회원가입 흐름 (return email 전까지는 로그인과 동일)

빈 등록과 추상화

Oauth에 필요한 각종 url과 id, secret을 yml 파일로 관리하였다.

이 값을 사용하기 위해 상위에

Plain Text
Copy
Oauth2


interface를 두었다.

그리고

Plain Text
Copy
@ConfigurationProperties()


를 이용해서 yml 파일을 읽어 클래스로 만들어 주었다.

@ConfigurationProperties() 는 setter를 이용해 값을 주입하게 되는데, setter를 사용하고 싶지 않았기에 @ConstructorBinding 을 사용하여 생성자 주입을 하게끔 만들었다. @ConstructorBinding 을 사용하게 되면 해당 클래스에서 빈 등록을 따로 해줄 수 없기 때문에 별도의 방법으로 빈 등록을 해주어야 한다.

Oauth2Config.class 에서 @EnableConfigurationProperties() 를 사용해서 빈 등록을 해주었다.
Member 엔티티 내부에서 Oauth에 대한 Enum 타입으로 관리를 해주고 싶었기에 enum 을 하나 생성해주었다.
inject 메서드는 아래의 enum 을 이용하기 위해서 쓰인다.

Oauth2Type 안에 빈을 주입하고 싶었으나 빈 주입 시점과 Enum 생성 시점이 다르기 때문에 Oauth2Config의 inject 메서드를 이용해서 주입해주었다. Enum에 setter를 기용함으로써 Enum의 고유한 성질인 불변이 깨지고, inject 호출이 이루어지고나서야 의도한대로의 기능을 동작하지만 이 상황에서는 충분히 기용할 만한 상황이라 판단하여 도입하였다.

Plain Text
Copy
 @Enumerated(EnumType.STRING)
 private Oauth2Type oauth2Type;


Member 내부에서는 다음과 같이 EnumType으로 값을 갖고있게 된다.
그리고 로그인/회원가입 시에 들어오는 request의 String oauthType을 판별하여 Oauth2 타입을 찾아주는데 사용한다.

TYF는 Oauth2를 사용함으로써 회원 정보 관리에 대한 부담을 덜어냄과 동시에 사용자가 서비스를 이용하는데 있어 불안요소를 없애고자 하였다.",BE
370,"Thank you for _ _ _

TYF 개발이야기

백엔드 CI/CD 격리 - 파즈

TYF는 CI/CD를 위한 툴로 Jenkins를 채택해서 사용하고 있다.
Dev, Prod, Performance 이렇게 3개의 서버에 CI/CD를 진행하고 있다.

문제는 프론트엔드와 동일한 git repository의 branch를 사용하고 있기 때문에 프론트엔드의 작업물이 반영될 때 마다
백엔드의 CI/CD가 일어난다는 문제가 있었다.

이를 방지하기 위해, 특정 상황에서만 빌드유발이 되게끔 설정 해놓았다.

Prod의 CI/CD를 예시로 설명하겠다.

빌드 유발 옵션을 Generic Webhook Trigger 를 이용하였다.
만약 이 옵션이 보이지 않을 경우에는 플러그인을 설치하면 된다.

그리고 웹훅을 이용하기 때문에 TYF가 사용하고 있는 repository의 웹훅 url을 젠킨스가 요구하는 url로 입력하였다.

위의 url을 밑의 url로 입력해주자.

Content Type는 application/json으로 설정해두자.
Generic Webhook Trigger를 사용하는 이유를 보기 이전에 Webhook이 어떤 것들을 날려주는지 먼저 확인해보자.
일단 깃허브의 웹훅 설정에서 Send me everything.으로 바꿔주자. 이 설정은 웹훅을 날리는 기준을 정하는 것이다. Push를 체크한다면 푸쉬에 대한 웹훅만 날리게 되는 반면, 밑의 설정은 모든 것을 날린다. PR의 open, closed, branch 로의 push, pr의 라벨 추가에 대한 이벤트까지 전부를 말이다.

현재 보고 있는 화면의 위쪽 탭을 보면

Plain Text
Copy
Recent Deliveries


라는 탭이 있을 것이다.

이 곳을 보면 웹훅을 날린 정보들을 확인할 수가 있을 것이다. 우리는 바로 이 정보들을 가지고 조건을 걸어서 빌드 유발을 시켜줄 수가 있다. merged가 close되고, branch 명은 main이며 라벨 정보가 내가 선택한 라벨일 경우에 빌드 유발이 되게끔 해보겠다.

이 Payload에 적혀있는 값 들을 한 번 살펴보자.
나는 jsonpath.com에 이 값 들을 가져다 놓은 후 필요한 필드들을 추출하는 작업을 해보았다.

Plain Text
Copy
$.pull_request.merged


에는 해당 PR이 merged 되었는지를 나타낸다. 결과는 true인 것을 확인할 수가 있었다.

Plain Text
Copy
$.pull_request.base.ref


는 머지의 도착(?) 브랜치다. Prod 같은 경우에는 main이다.

Plain Text
Copy
$.pull_request.labels


를 하게되면 해당 PR에 달아 놓은 라벨들이 모두 나오게 된다. JSONPath 문법에 따라서

Plain Text
Copy
$.pull_request.labels..name


or

Plain Text
Copy
$.pull_request.labels.[]name


을 하게되면 해당 라벨들의

Plain Text
Copy
name


필드들이 모두 나오게 된다. 이렇게 뽑아낸 필드들을 젠킨스에서 변수로 설정을 해주면된다.

Plain Text
Copy
Post content parameters


추가를 누르고 변수명을 기입하고 JSONPath Expression을 기입한다.

그리고 스크롤을 밑으로 내려서 나오는 Optional filter 탭에서 Expression에는 정규표현식을, Text에는 변수들을 작성한다. 나는 머지가 됐고, 브랜치가 main이며 라벨의 이름이 server이면 빌드가 유발되게끔 해놓은 것이다.

regex.com에서 정규표현식을 작성하기가 수월해서 이곳에서 테스트해봤다.

이렇게 말이다. 위의 상황에서는 라벨 값에 server, document 이렇게 2개가 들어온 상황을 연출한 것이고, 그 중 server가 있는지 체크를 한 것이다. 마지막으로 한 가지가 더 남았다.

Generic Webhook Trigger를 사용하기 위해서는 토큰을 설정해주어야 한다. 토큰을 활용한 무언가에 대해서는 나도 자세히는 잘 모르겠다. 먼저 Add를 눌러서 토큰을 만들어주자.

위의 Secret에 입력할 값은 이전 캡처의 Token 쪽에 들어갈 값과 일치시켜야 한다. ID는 본인의 해당 토큰에 대한 ID니깐 ㄴ임의로 작성을 하고 Add를 눌러준다. 그리고 Token필드에 이곳에서 만든 Secret의 값을 적어준다.

깃허브 웹훅 URL에서 /generic-webhook-trigger/invoke로 적어놨는데 이곳에 ?token={각자의 토큰 값}을 추가해주자. 이렇게하면 빌드 유발에 대한 설정이 끝난다.

다음과 같은 방법으로 TYF는 프론트엔드의 작업물이 반영될 때마다 Jenkins 빌드가 유발되지 않게끔 해두었다.
현재 걸어둔 조건은 도착 브랜치, 라벨, 머지유무 이지만 이 방법을 응용해서 원하는 빌드유발을 만들 수 있을 것이다.",BE
420,"PG사 연동 후 첫 배포 이벤트

카카오 페이의 피드백을 받고, 반영을해서 PG 연동에 성공을 하고 실제 서비스를 하게 되었습니다 그래서 저희 TYF팀은 실제 사용자 분들을 모시고, 피드백을 받아보고자 이벤트를 진행하게 됩니다.

많은 분들이 참여하셔서 좋은 피드백을 주시는 것을 기대 하였고, 그 수고에 조금이나마 보답하고자 상품도 준비했습니다.

첫번째로는 회원 가입 후 1000 포인트를 충전하여 후원하시면 1000포인트 환급 이벤트

두번째로는 후원을 하시면 추첨을 통해 비타 500 기프티콘 증정

세번째로는 계좌인증까지 진행하신 분들을 대상으로 추첨으로 베스킨라빈스 싱글킹, 스타벅스 아메리카노 3명 증정

해당 이벤트를 진행 한 후 가입자 수가 저희 멤버 6명에서 현재는 100분이 넘게 사용하시는 서비스로 성장하였습니다. 

이벤트를 진행하고 나서 받은 피드백은 아래와 같아요. 좋았던 점으로 용기를 많이 얻게 되었어요. TYF 멤버 모두 아쉬운 점 피드백을 보기 전에 덜덜 떨면서 열었는데, 다 고개가 끄덕여지는 좋은 피드백 들 이었습니다. 

아래에는 저희가 받은 피드백들을 정리한 내용입니다! 내용이 길어서 좋았던 점, 아쉬운 점, 자유 피드백 순으로 토글리스트로 숨겨 놓았습니다.

좋았던 점

아쉬운 점

자유 피드백

서비스를 배포하고나서, 우아한 테크코스 크루분들로 부터 즉각적인 피드백도 실시간으로 반영하는 경험을 할 수 있었습니다. 그리고 이벤트 준비를 통해 실제 사용자 분들이 느끼시는 바에 대해서도 알 수 있었습니다.",NON_TECH
421,"결제 연동 준비 페이지

1. 사업자 등록
사업등록 가이드
사업유형 가이드
행정절차안내
창업 세무/법률/경영 상담
공동사업자란?
세무/신고/세금 가이드
통신판매업자 신고
사업용 계좌 등록하는법
세무 상담 기록
법률 상담 기록
폐업신고
푸터 필수 정보
2. PG 연동 준비
1. 토스페이먼츠
2. 카카오페이
3. 무형의 재화 서비스로 pg사 신청하는법 (KCP)
4. 아임포트, PG신청 가이드
3. 기타
표준 약관 양식
회사 유선전화 등록하기
현금 흐름 케이스 스터디
창작자 (판매자) 정산은 따로 수기로해야되나? (세금계산서)
유사 서비스 케이스 스터디
DONE / TODO
총 비용

1. 사업자 등록

사업등록 가이드

개인사업자 등록 절차 신청방법 및 조건,비용 총 정리
안녕하세요? 오늘은 개인사업자 신청하는 방법에 대해서 알아보았습니다. 이는 세무서에 직접가셔서 신청을 하셔도 되지만 만약 거리가 멀거나 갈 수 없다면 인터넷 홈텍스(HomeTax)에서 등록 신청을 하면됩니다. 참고로 인터넷으로 사업자등록증 신청을 할 때는 공인인증서가 꼭 필요하며 등록을 하고 며칠 뒤에 세무서에서 본인에게 신청한 것이 맞는지 확인 전화가 오며 사업자등록증 진행을 하게 됩니다. ※일반?
https://strong-coi.tistory.com/347

국세청
국세청 홈페이지에 오신 것을 환영합니다.
https://www.nts.go.kr/nts/cm/cntnts/cntntsView.do?mi=2444&cntntsId=7777

사업유형 가이드

사업자유형(개인/법인, 과세/면세, 일반/간이)
사업자등록이란, 납세의무자에 해당하는 사업자를 국세청에 등록 하는 것을 의미합니다. 사업자등록이 완료되어야 은행 등의 금융기관 개설이나 세금계산서 발행 등 사업의 기본적인 상행위를 할 수 있는 것입니다. 사업자 구분 등록하는 사업자 유형 사업자 유형에 따라 과세의 항목이나 세율을 달리 하고 있으며, 구분은 아래와 같습니다. 사업자는 아래와 같...
https://help.jobis.co/hc/ko/articles/360001278893-%EC%82%AC%EC%97%85%EC%9E%90%EC%9C%A0%ED%98%95-%EA%B0%9C%EC%9D%B8-%EB%B2%95%EC%9D%B8-%EA%B3%BC%EC%84%B8-%EB%A9%B4%EC%84%B8-%EC%9D%BC%EB%B0%98-%EA%B0%84%EC%9D%B4-

행정절차안내

서울창업허브
서울창업허브
https://seoulstartuphub.com/edu/guide.do

창업 세무/법률/경영 상담

서울창업허브
서울창업허브
https://seoulstartuphub.com/hub/advice.do

영세납세자지원단 서비스 신청
「영세납세자지원단」이란? 경제적인 사정 등으로 세무대리인을 선임하지 못하는 영세납세자가 세금에 대한 고민 없이 생업에 전념할 수 있도록 나눔세무·회계사가 사업주기별 맞춤형 서비스를 통해 세금문제 해결을 도와드리는 제도입니다. 「영세납세자지원단은」 어떻게 설치·구성되어 있나요? 전국 모든 세무서에 설치되어 있으며, 세무서 납세자보호담당관(지원단장), 나눔세무·회계사 및 업무관리담당자로 구성되어 있습니다. 나눔세무·회계사는 재능기부 등을 통해 영세납세자 지원활동에 헌신적으로 참여할 세무대리인으로 국세청 홈택스, 한국세무사회·한국공인회계사회 홈페이지에서 확인할 수 있습니다.
https://www.nts.go.kr/taxpayer_advocate/cm/cntnts/cntntsView.do?mi=11497&cntntsId=8303

공동사업자란?

동업 시(공동사업자)세금은 어떻게 납부하나요?
2인 이상의 사업자가 공동사업을 하는 경우의 세금납부를 안내드립니다. 사업자등록신청은 공동사업자 중 1인을 대표자로 하여 대표명의로 신청하며, 손익분배 비율이 명시한 동업계약서를 첨부하여 신고합니다. 공동사업자의 소득세와 부가가치세 신고는 아래와 같습니다. A.부가가치세 부가가치세 신고기준은 개인별이 아닌 사업장 기준입니다. 따라서, 공동사업자의 여부에 상관없이 신고방법은 동일합니다. 해당 사업장의 총 매출액과 매입액에 대해 신고를 진행합니다.
https://help.jobis.co/hc/ko/articles/360006020114-%EB%8F%99%EC%97%85-%EC%8B%9C-%EA%B3%B5%EB%8F%99%EC%82%AC%EC%97%85%EC%9E%90-%EC%84%B8%EA%B8%88%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%82%A9%EB%B6%80%ED%95%98%EB%82%98%EC%9A%94-

세무/신고/세금 가이드

자비스 고객센터
https://help.jobis.co/hc/ko

통신판매업자 신고

imweb.me
https://imweb.me/faq?mode=view&category=29&category2=39&idx=5954

nomadtimes.tistory.com
https://nomadtimes.tistory.com/37

사업용 계좌 등록하는법

카카오뱅크 신용카드로 국세청에 사업자 계좌등록 및 사업자 카드등록 따라하기
스마트스토어 정산대금 계좌 어떤거 쓰고 계시나요? 저는 기존에 사용하던 카카오뱅크계좌를 이용하고 있습...
https://m.blog.naver.com/ndslds/221953178280

세무 상담 기록

일반

부과세

1-6월분 7/25 납부

7-12월분 1/25 납부

종소세

5월

중개수수료 매출 신고하기
중개플랫폼을 제공하는 서비스의 경우, 중개수수료에 대한 부분만이 실제 매출이기 떄문에 매출 총액은 중개플랫폼의 매출이 아니라 실판매자의 매출로 잡아야 합니다.대형 온라인 마켓 등의 사이트에서는 실판매자의 사업자번호로 매출이 잡힐 수 있도록 시스템 설정이 되어있기도 합니다만, 자체 홈페이지 등으로 중개플랫폼 앞으로 매출이 발생한 경우에는 아래와 같은 ...
https://help.jobis.co/hc/ko/articles/115002960334-%EC%A4%91%EA%B0%9C%EC%88%98%EC%88%98%EB%A3%8C-%EB%A7%A4%EC%B6%9C-%EC%8B%A0%EA%B3%A0%ED%95%98%EA%B8%B0-

종합소득세 잡히는 금액: 총 수입 - 총 비용

부가세 잡히는 금액: 카드결제의 10퍼센트

정산 === 비용,

정산할 때도 증빙이 필요하다 - 세금계산서,

이외에도 소모품, 인건비 등을 비용으로 하기에 증빙해야됨.

현금을 쓰면 증빙 어려움

법률 상담 기록

문의 내역

서비스 흐름에 법적으로 문제될 소지가 있는지

환불 며칠까지 보장해줘야하나

정산, 보름에 한 번 가능?

약관 고지 같은거 어떻게해야할까

기본적으로, 중개 서비스다

전자상거래법 적용됨

청약철회: 환불 1주

횡령, 사기로 - 형사 고소 당할 수 있음.

고지 확실히 할 것

회사 운영 계좌랑 결제 계좌를 잘 분리 해야할듯

폐업신고

폐업 신고와 절차 - 개인사업자
■ 주요 내용 - 폐업 일자 : 신고 일자가 아닌 거래활동 중지를 신청하는 날을 의미합니다. 폐업 일자 기준으로 세무처리가 진행되어, 폐업 일자 이후 세금계산서 발행 등은 불가능합니다. - 폐업 부가세 : 폐업 일자가 속한 달의 다음 달 25일까지 신고해야 합니다. - 폐업 법인세 : 폐업 일자가 속한 해의 다음 해 5월 종합소득세 신고시 반영하여야...
https://help.jobis.co/hc/ko/articles/115007504588

푸터 필수 정보

가비아 라이브러리
홈페이지를 만들 때 필수로 들어가야 하는 내용이 있다는 사실, 알고 계셨나요? 홈페이지에는 운영하고자 하는 서비스 혹은 운영 방식에 따라 반드시 넣어야 하는 내용이 있습니다. 어떤 것들이 있는지 하나씩 살펴보도록 할게요. 영리 목적으로 운영되는 홈페이지라면 ""이용약관""이 필수적으로 들어가야 합니다. 분쟁이 생겼을 때, 이용약관이 판결의 기준이 되기 때문에 중요한 항목이라고 할 수 있습니다.
https://library.gabia.com/contents/domain/3076/

2. PG 연동 준비

1. 토스페이먼츠

토스페이먼츠 전자결제
https://app.tosspayments.com/

2. 카카오페이

카카오페이 비즈니스
카카오페이와 함께 비즈니스하세요.
https://biz.kakaopay.com/online/apply/mall/1

3. 무형의 재화 서비스로 pg사 신청하는법 (KCP)

[KCP] PG사 무형의 상품 신청시 사이트에 기재되어야할 내용 : 애드리빙 - 관리페이지 개요
충전 사용시 기재내용 예)최대 3개월까지 가능환불규정은 필수적으로 노출되어야 하며, 환불안내 (운영시 규정) 자세한 내용 등록이 필요헙니다.작성 예)※ 요금결제 안내결제금액 만큼 글등록 권한이 부여됩니다.예) 3개월 구매시 3개월간 글 등록 가능 ※ 환불안내1. 구매한 충전권의 기간 1개월의 1/2이 지나기 전에는 언제든지 전체 또는 일부 환불(회수)이 가능합니다.2.
https://appsweb.kr/1148/?q=YToxOntzOjEyOiJrZXl3b3JkX3R5cGUiO3M6MzoiYWxsIjt9&bmode=view&idx=3660362&t=board&category=16686U23g5

4. 아임포트, PG신청 가이드

개발자를 위한 무료 결제연동 API, 아임포트
3.4% 1.5% 3.4% 무료 없음 여기에 없는 PG는 아임포트를 이용할 수 없나요? 제휴프로모션이 없는 PG사도 아임포트와 연동할 수 있습니다. 단, 미제휴 PG사는 해당 PG사의 고객센터를 통해 가입해야 하며, 아임포트가 PG사로부터 확인할 수 있는 정보가 제한적이기 때문에 아임포트를 통한 기술지원과 장애대응은 제공되지 않습니다. 아임포트와 제휴된 PG사 이용을 추천해 드립니다.
https://www.iamport.kr/pg

3. 기타

표준 약관 양식

공정거래위원회
공정거래위원회에 오신것을 환영합니다.
https://www.ftc.go.kr/www/cop/bbs/selectBoardList.do?key=201&bbsId=BBSMSTR_000000002320&bbsTyCode=BBST01

회사 유선전화 등록하기

단말기로 인터넷 전화 신청하는 법

[스타트업] 회사 전화 개통하기
오늘부터 시간이 날 때마다 회사를 창업하면서 겪었던 소소한 스토리들을 적어볼까 한다. 스타트업에 대한 거창한 이야기들은 다른 블로그나 미디어에서 많이들 다루고 있으니, 여기서는 정말 깨알 같은 팁들을 적어 볼 예정이다. 다시 한 번 강조하면, 정말 깨알 같은 팁이다. 뭔가 대단한 걸 기대하면 안됨! 회사를 설립하고 나면 회사번호가 필요하다. ""그냥 제 핸드폰 번호로 하면 안되나요?""
https://xlos.tistory.com/1688

2. 아톡 어플 사용하면 어플로 인터넨전화(070)번호 부여받을 수 있음( 월 2200원)

아톡
070 인터넷 전화어플 아톡, 저렴한 국제전화는 물론 투넘버 및 안심번호 서비스를 제공합니다.
http://atalk.co.kr/

현금 흐름 케이스 스터디

국내 유사 서비스

투네이션 (https://help.toon.at/hc/ko)
수수료 정보
투네이션 수수료 정보
투네이션의 정산 수수료에는 결제대행사(PG사) 이용 수수료, 원천세 등의 세금, 투네이션 사용 수수료가 모두 포함되어 있으며, 그 외의 세금에 대해서는 도네이터(시청자)가 충전 시 부담합니다. [정산 시 적용되는 결제 수단 별 수수료] - 신용 카드 : 6.3% - 인터넷 뱅킹(실시간 계좌 이체) : 5.3% - 가상 계좌 : 9.9% - PAYCO...
https://help.toon.at/hc/ko/articles/115001375373

수수료 정보
투네이션 수수료 정보
투네이션의 정산 수수료에는 결제대행사(PG사) 이용 수수료, 원천세 등의 세금, 투네이션 사용 수수료가 모두 포함되어 있으며, 그 외의 세금에 대해서는 도네이터(시청자)가 충전 시 부담합니다. [정산 시 적용되는 결제 수단 별 수수료] - 신용 카드 : 6.3% - 인터넷 뱅킹(실시간 계좌 이체) : 5.3% - 가상 계좌 : 9.9% - PAYCO...
https://help.toon.at/hc/ko/articles/115001375373

투네이션 수수료 정보
투네이션의 정산 수수료에는 결제대행사(PG사) 이용 수수료, 원천세 등의 세금, 투네이션 사용 수수료가 모두 포함되어 있으며, 그 외의 세금에 대해서는 도네이터(시청자)가 충전 시 부담합니다. [정산 시 적용되는 결제 수단 별 수수료] - 신용 카드 : 6.3% - 인터넷 뱅킹(실시간 계좌 이체) : 5.3% - 가상 계좌 : 9.9% - PAYCO...
https://help.toon.at/hc/ko/articles/115001375373

트윕 (https://twip.kr/)

창작자 (판매자) 정산은 따로 수기로해야되나? (세금계산서)

정산 자동화방법..급구..

유사 서비스 케이스 스터디

트윕 케이스

투네이션 케이스

재능넷 케이스

크몽케이스

토스페이먼츠에 문의하기 (우리 유형의 서비스는 어떤 준비를 해야하는가)
문의 내역

문의 내역

창업센터 상담받기 (어떤 약관이 필요할까, 소비자 보호)
문의 내역

문의 내역

DONE / TODO

사업자통장 발급

국세청 신고

에스크로 신청

통신사업자 등록

PG 승인 신청
승인 문의 내용
토스페이먼츠
일단 심사를 신청해라
다날
후원금을 정산한다면 불가하다.
카카오페이
BM을 변경해라 (포인트충전방식으로)

승인 문의 내용

토스페이먼츠
일단 심사를 신청해라

일단 심사를 신청해라

다날
후원금을 정산한다면 불가하다.

후원금을 정산한다면 불가하다.

카카오페이
BM을 변경해라 (포인트충전방식으로)

BM을 변경해라 (포인트충전방식으로)

총 비용

기업용 인증서 발급 4,400원

기업용 OTP 발급 5,000원

도메인 구매비용 10,000원

통신판매사업자 등록면허세 40,500원

사업장 전화번호 월 2,200원",NON_TECH
440,"Spring Boot와 Redis 기반의 캐싱 - 성능 개선기 2021, Oct 21 안녕하세요 마크입니다 :) 목차 목차 들어가며 In-Memory가 Disk I/O보다 빠른 이유 메모리 계층구조 관점 프로세스 관점 결과 가정 성능 측정 및 분석 캐싱 적용 전 캐싱 적용 후 결론 아쉬운 점 들어가며 이전 글에서 스프링 부트와 Redis 기반의 캐시 구현을 다뤘다면, 이번 글은 성능 개선과 관련된 글이다. DB는 운영체제 관점에서 보면 두 가지로 분류할 수 있다. 데이터를 Memory에 저장하는 DB 데이터를 Disk I/O를 통해 디스크에 저장하는 DB 보통 영속화가 필요한 데이터는 Disk I/O를 통해 디스크에 저장하고, 영속화 할 필요가 없는 데이터는 Memory에 저장한다. 메모리 계층구조와 프로세스 관점에서 보면 두 저장 방식의 가장 큰 차이점은 쓰기/읽기 속도다. 그래서 프로젝트에 비교적 많은 요청이 들어오고, DB에 부하가 많이 걸리는 비로그인 홈피드 조회부분에 Memory 기반의 캐싱을 적용시켰다. 이번 글은 필자가 어떤 이유로 인해 캐싱을 적용시켰는지 살펴보고, 실제 프로젝트를 통해 성능 비교를 진행하여 개선 사항을 분석하고자 한다. 우선 필자가 캐싱을 적용시켜야겠다고 생각한 이유는 두 가지다. In-Memory가 Disk I/O보다 빠르다는 이론. 현재 홈피드 조회 부분으로 인한 DB CPU 부하가 많이 걸리는 문제. In-Memory가 Disk I/O보다 빠른 이유 캐싱을 적용시킨 첫 번째 이유에 대해서 알아본다. 메모리 계층구조 관점 많은 분들이 알다시피, 컴퓨터의 메모리 계층 구조는 아래와 같다. 출처: https://link.springer.com/article/10.1007/s00778-019-00546-z 위로 갈 수록 속도는 빠르지만, 용량은 작다. 반대로, 아래로 갈수록 속도는 느리지만, 용량은 크다. CPU Register와 CPU Caches (L1, L2, L3)는 CPU 내부에 존재한다. 당연히 CPU가 아주 빠르게 접근할 수 있다. 메인 메모리는 CPU 외부에 존재한다. 레지스터와 캐시보다 더 느리게 접근할 수 밖에 없다. 메인 메모리를 읽어오기 위해선 인터럽트를 통해 커널모드에서 가져올 수 있다. 하드 디스크는 심지어 CPU가 직접 접근할 방법조차 없다. CPU가 하드 디스크에 접근하기 위해서는 하드 디스크의 데이터를 메모리로 이동시키고, 메모리에서 접근해야한다. 아주 느린 접근 밖에 불가능하다. 이렇게 보면 메인 메모리에 데이터를 저장하는 In-Memory DB와 하드 디스크에 데이터를 저장하는 Disk I/O DB의 성능 차이는 어쩌면 당연하다. 프로세스 관점 메모리 계층구조 관점에서 차이는 분명하다. 하지만 프로세스 관점에서 더 큰 차이가 발생한다. Disk I/O 운영체제 입장에선 데이터 베이스도 그저 프로세스 일 뿐이기 때문이다. 운영체제는 CPU가 한정적이라 시분할 방식으로 동작하며, Disk I/O와 같이 외부 장치를 이용하는 시스템 콜을 사용할 땐 프로세스가 한동안 CPU를 양도하고 해당 프로세스를 Disk I/O 큐에 넣는다, 그리고 Disk I/O 작업이 끝날 때까지 Blocked 상태가 된다. 출처: 운영체제와 정보기술의 원리 Disk I/O가 걸릴 때의 프로세스 상태 변화 출처: 운영체제와 정보기술의 원리 위와 같이 프로세스의 정보를 담은 PCB들을 요청에 따라 CPU Ready Queue 혹은 장치들의 큐에 넣어 요청을 처리한다. 그리고 이때 장치들의 큐에 들어간 프로세스의 상태는 Blocked 상태가 된다. Blocked 상태의 프로세스는 해당 I/O 요청이 완료되고, 처리 결과(데이터)를 메모리로 옮기고 나서야 Ready 상태가 된다. Memory 반면에 메인 메모리에 데이터를 저장하는 경우, 다른 장치들의 I/O가 걸리지않는다. 메인 메모리에 쓰기/읽기 작업을 위해 인터럽트를 통해 사용자모드를 커널모드로 바꿀 뿐, 프로세스의 상태는 Blocked이 되지않고, Ready와 Running 상태를 반복한다. 즉, Memory가 프로세스 관점에서도 Disk I/O보다 빠를 수 밖에 없다. 결과 가정 위와 같이 메모리 계층 구조와 프로세스 관점에서 보면 In-Memory가 성능이 더 좋을 수 밖에 없어보인다. 그래서 캐싱을 적용시키기 전에도 캐싱을 적용시키면 성능이 당연히 좋아질 수 밖에 없다고 생각했다. 실제로 좋아지는지 아래 성능 측정을 통해 알아본다. 성능 측정 및 분석 이제 캐싱을 적용시켜 성능 측정을 해보고 실제로도 성능 개선이 되는지 알아보고자 한다. 테스트에 사용될 데이터양은 아래와 같다. 테스트 DB에 저장된 데이터 개수 주요 대상인 게시물은 대략 100만건이다. 각 테스트 시간은 10분으로 설정하였다. 캐싱 적용 전 캐싱 적용 전, 테스트에 사용될 인프라 구조는 아래와 같다. 처음에 VUser는 최대치를 뽑아내기 위해 1,000부터 시작하여 다양한 VUser 설정값으로 테스트를 진행하였다. 그리고 VUser를 700 설정만으로도 위와 같이 에러율이 너무 높아 금방 테스트가 종료되는 것을 볼 수 있었다. 실제로 2-3분 사이에서 테스트가 종료되었다 원인을 찾기 위해 vmstat을 통한 서버 모니터링을 진행하였다. DB의 CPU는 굉장히 부하가 많이 걸리고, WAS의 CPU는 부하가 별로 안 걸리는 것을 볼 수 있다. DB에서의 부하로 인지 하였고, WAS는 비교적 부하가 걸리지 않아 로그를 찾아보았다. WAS에서 어떤 에러를 던지는 확인하였고, 위와 같이 Hikari Pool에서의 에러를 볼 수 있었다. 위 에러는 여유 커넥션이 존재하지 않아 요청 시간이 30초를 넘어버려 발생하는 에러이다. 참고 즉, 요청은 계속 들어오는 상황에서 DB는 요청당 처리 속도가 비교적 느리니 커넥션이 남아나질 않는 것이다. 물론 Hikari Pool의 설정을 통해 조금의 성능 개선이 있을 순 있지만, 비약적인 성능 개선 효과를 없을 것 이라 판단했다 결론적으론 600도 실패하였고, 500을 설정하여 테스트를 진행하게 되었다. 캐싱 적용 전 / VUser: 500, 1-1000 랜덤 페이지 조회 TPS가 비교적 낮게 나왔으며, MTT의 경우 24.9초가 걸리는 것을 볼 수 있다. 물론 이 부분은 쿼리 튜닝을 통해 충분히 성능을 개선 시킬 수도 있다. 실제로 코다와 케빈이 성능을 개선 시킬 수 있다는 것을 증명했다. 또한, 아래와 같이 DB를 Replication함에도 불구하고, DB의 부하가 굉장히 많이 걸리는 것을 볼 수 있다. 캐싱 적용 전 테스트 vmstat (중간에 한 장면을 스크린샷 찍은 것) 한 가지 의문은 Disk I/O의 병목이 생기지 않는다는 점이다. (bi와 wa를 보면 올라가지 않는다) 이 부분에 대해선 몇몇 크루들과 토론을 진행하였고, 운영체제와 DB에서의 캐싱으로 인해 Disk I/O의 부하가 비교적 적게 걸린다고 판단하였다. 그 이유는 medium은 메모리가 4기가나 되며, 테스트가 진행됨에 따라 메모리의 용량이 점차 작아지는 것을 발견했기 때문이다. 이외에도 호기심에 필자가 nano 환경에서 비슷한 테스트를 진행하였으며, 메모리가 적고 조회하는 데이터의 용량이 클 땐 Disk I/O의 병목이 발생하는 것을 발견할 수 있었다. 필자가 nano 환경에서 메모리 보다 더 높은 용량의 데이터를 조회 테스트했을 때의 모니터링 결과 Disk I/O 발생 즉, 단기간에 메모리보다 더 많은 데이터를 조회하는 테스트를 한다면 Disk I/O가 발생한다. 예를 들어, 1 ~ 1000의 랜덤 범위가 아닌, 1 ~ 999999정도의 랜덤 범위로 테스트를 하면 Disk I/O 부하가 발생할 수 있다. 캐싱 적용 후 캐싱 적용 후, 인프라 구조는 아래와 같다. 큰 차이점은 없다. 그저 중간에 Redis가 추가된 것 뿐이다. WAS에서 먼저 Redis에 데이터가 있는지 확인 후, 없으면 DB에 조회하는 대표적인 Look Aside Cache 전략을 적용시켰다. 이번에도 동일하게 VUser를 1000부터 테스트를 진행하였지만, 이번엔 nGrinder의 메모리 부족으로 에러가 발생하였다. 시작한지 2분정도 되었을 때, 에러가 발생하여 테스트가 종료되었다. 이는 현재 우아한 테크코스에서 제공하는 EC2의 보안 관계상 nGrinder의 컨트롤러와 에이전트를 모두 하나의 EC2안에 도커로 띄워서 메모리 부족으로 인한 문제로 판단된다. 즉, 현재 환경에선 VUser를 1000으로 설정하여 테스트를 진행하지 못한다. 캐시/버퍼 메모리를 비우고, 필요없는 메모리를 지워도 동일한 문제가 발생한다. 여러 VUser로 실험을 진행하였고, 700까진 그래도 간당간당히 가능했다. 결국 700으로 설정하여, 테스트를 진행하게 되었다. 캐싱 적용 후 / VUser: 700, 1-1000 랜덤 페이지 조회 TPS는 296으로 나왔으며, MTT의 경우 2초가 걸리는 것을 볼 수 있다. MTT가 비교적 높은 이유는 테스트 초기에 1000개의 데이터를 캐싱하는 과정에서 평균 값이 높아졌기 때문이다. 이는 캐싱을 배치 처리하거나, 쿼리를 튜닝하면 해결되는 문제라고 판단된다. VUser 500으로 했을 땐 TPS가 340정도 나왔고, 600도 비슷하게 나왔다. 필자 생각엔 VUser를 1000으로 해도 문제없이 200후반에서 300대가 나올 것으로 예상된다. 그 이유는 실제 1000을 테스트 시 nGrinder가 터질 때 (2분 정도)까지 TPS가 187.9 찍히는 것을 확인했기 때문이다. 추후에 nGrinder의 메모리 문제를 해결시 따로 다시 테스트해 볼 예정이다. 캐싱 적용 후 SLAVE 1 DB vmstat 모니터링 또한, 위와 같이 캐싱이 완료된 후로는 DB의 부하가 줄어드는 것을 볼 수 있었다. 결론 결론적으로 성능 개선 사항은 아래와 같다. TPS 16.9 (캐싱 전) -> 300 ~ 340 (캐싱 후) MTT 24.9초 (캐싱 전) -> 2초 (캐싱 후) TPS의 경우 대략 18배, MTT는 대략 12배 정도 개선되었다. 물론 쿼리 튜닝을 하면 캐싱 전 성능과 전체적인 MTT의 향상이 예상된다. 쿼리 튜닝이 어느정도 완료되면, 그때 다시 테스트하여 글을 올릴 예정이다. 이외에도, Redis의 직렬화/역직렬화 최적화 혹은 Redis Replication을 통해 캐싱의 성능도 더 향상시킬 수 있을 듯 하다. 아쉬운 점 크게 아쉬운 점은 처음 가정했던 Disk I/O 부하가 발생하지 않았다는 점이다. 필자 판단으론 조회시 랜덤 페이지의 범위가 작아서 그런 듯하다. 이 부분은 추후에 쿼리 튜닝이 전부 완료되고, 더 높은 페이지의 범위까지 커버가 될 때 한번 더 테스트해봐야 할 듯 하다. Twitter Facebook Google+ # 백엔드 # Cache # Spring # Redis # 성능",BE
441,"하이버네이트 default-batch-fetch-size 가 안되는 현상 😢 2021, Oct 22 🐾 목차 🐾 목차 💡 Intro 🌩 hiberbate.default_batch_fetch_size 🌩 본 프로젝트 문제 상황 Entity 구조 문제상황 🌩 이상현상 들여다보기 Entity 구조 상황 재현 🌩 Help 💡 Intro JPA를 프로젝트에서 사용하면서 연관 엔티티를 호출할 때 생기는 N+1을 해결한 경험이 있다. 이때 해결 방법으로 hibernate의 default_batch_fetch_size를 yml에 설정하여 해결했었다. 참고링크 해결부분 프로젝트를 전반적으로 체크하던 와중에 위 설정에 의한 in query가 실행되지 않고 여전히 N+1 문제가 발생하는 부분을 발견하였다. 해당 현상을 공유하기 위해 글을 작성한다. (여전히 이유는 못 찾았다 😢) 🌩 hiberbate.default_batch_fetch_size 우선 간단하게 위 설정에 대해서 짚고 넘어가보자. 설정할 수 있는 방법은 두 가지 이다. @BatchSize(size={sizeNum}) 어노테이션 활용 클래스, 메소드, 필드 레벨에서 사용할 수 있다. 해당 사이즈 만큼의 상위 엔티티 id가 in query로 나간다. spring.jpa.properties.hibernate.default_batch_fetch_size={batchSize}를 application.properties에 지정 전역적으로 적용이 되어서 상위 엔티티의 lazy loading된 하위 엔티티를 한꺼번에 in query로 로딩한다. Hibernate javadocs 공식 문서에 다음과 같이 서술한다. Defines size for batch loading of collections or lazy entities. For example...
@Entity
@BatchSize(size=100)
class Product {
...
}

will initialize up to 100 lazy Product entity proxies at a time.
@OneToMany
@BatchSize(size = 5) /
Set getProducts() { ... };

will initialize up to 5 lazy collections of products at a time
즉, 속한 collection이나 lazy entities 들을 한꺼번에 batch로 로딩해준다. Batch로 로딩할 경우 하나의 쿼리로 연관 엔티티를 한꺼번에 가지고 올 수 있어서 성능이 향상된다. 다음 Hibernate Document를 확인해보자. @BatchSize specifies a “batch size” for fetching instances of this class by identifier. Not yet loaded instances are loaded batch-size at a time (default 1). not yet loaded instance를 batch로 로딩할 수 있는 설정이라고 한다. 🌩 본 프로젝트 문제 상황 Entity 구조 다소 복잡하지만 우리 프로젝트에서의 상황을 살펴보자. 조금 이해하기 쉽게 프로젝트의 일부 엔티티 관계를 그림으로 표현해 보았다. 사용자는 포트폴리오를 만들고 본인이 진행한 여러 프로젝트들을 포함시킬 수 있다. 각 프로젝트마다 프로젝트를 나타내는 태그를 여러개 추가할 수 있다. 예를 들어 Java, Web 등등의 태그로 키워드를 나열할 수 있다. 프로젝트와 태그는 다대다 관계이기 때문에 중간 테이블인 ProjectTag로 연결되어 있다. ProjectTag는 프로젝트 id와 태그 id를 가지고 있다. 코드가 더 편한 사람들을 위해 Entity를 추가해본다. 편의를 위해 getter, 생성자, 다른 메소드와 관련 없는 필드들은 생략한다. Portfolio.java @Entity
public class Portfolio {

@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

// 다른 필드 생략

@OneToMany(
mappedBy = ""portfolio"",
fetch = FetchType.LAZY,
cascade = CascadeType.PERSIST,
orphanRemoval = true
)
private final List<Project> projects;
Project.java @Entity
public class Project {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

// 다른 필드 생략

@OneToMany(
mappedBy = ""project"",
fetch = FetchType.LAZY,
cascade = CascadeType.PERSIST,
orphanRemoval = true
)

private List<ProjectTag> tags;
}
ProjectTag.java @Entity
@Table(
uniqueConstraints = {
@UniqueConstraint(columnNames = {""tag_id"", ""project_id""})
}
)
public class ProjectTag {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""tag_id"")
private Tag tag;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""project_id"")
private Project project;
}
Tag.java @Entity
public class Tag {

@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@Column(nullable = false, unique = true, length = MAX_TAG_LENGTH)
private String name;
}
문제상황 동일한 Assembler(같은 코드)로 응답 DTO를 만들때 포트폴리오를 조회할 때는 in 쿼리로 나가고, 포트폴리오를 업데이트 할 때는 n+1 쿼리가 나간다. @Transactional(readOnly = true)
public PortfolioResponseDto read(String username, UserDto userDto) {
Optional<Portfolio> portfolio = portfolioRepository.findPortfolioByUsername(username);

if (userDto.isGuest() && portfolio.isEmpty()) {
throw new NoSuchPortfolioException();
}

return portfolioDtoAssembler.toPortfolioResponseDto(
portfolio.orElseGet(() -> portfolioRepository.save(Portfolio.empty(getUser(username))))
); // default_batch_fetch_size로 인한 In 쿼리 수행
}

@Transactional
public PortfolioResponseDto update(PortfolioRequestDto portfolioRequestDto, UserDto userDto) {
Portfolio portfolio = portfolioRepository.findById(portfolioRequestDto.getId())
.orElseThrow(NoSuchPortfolioException::new);
User user = getUser(userDto.getUsername());

if (!portfolio.isOwnedBy(user)) {
throw new UnauthorizedException();
}
portfolio.update(portfolioDtoAssembler.toPortfolio(portfolioRequestDto));

entityManager.flush();

return portfolioDtoAssembler.toPortfolioResponseDto(portfolio); //Tag를 lazy loading 할때 n+1 쿼리 발생
}
전체 코드보기 쿼리 결과 // read 메소드 실행 시
Hibernate:
select
tag0_.id as id1_13_0_,
tag0_.name as name2_13_0_
from
tag tag0_
where
tag0_.id in (
?, ?
)

// update 메소드 실행 시
Hibernate:
select
tag0_.id as id1_13_0_,
tag0_.name as name2_13_0_
from
tag tag0_
where
tag0_.id=?
Hibernate:
select
tag0_.id as id1_13_0_,
tag0_.name as name2_13_0_
from
tag tag0_
where
tag0_.id=?
위 쿼리가 수행되는 Portfolio -> PortfolioResponseDto 로 변환시키는 assembler의 코드는 다음과 같다. 필드가 많아서 당황스럽겠지만 Project 부분만 보고 감만 잡으면 된다. (* 표시해둔 곳) 간단히 말하면 get을 통해 lazy loading 하위 엔티티의 값을 가져온다. public PortfolioResponseDto toPortfolioResponseDto(Portfolio portfolio) {
return new PortfolioResponseDto(
portfolio.getId(),
portfolio.getName(),
portfolio.isProfileImageShown(),
portfolio.getProfileImageUrl(),
portfolio.getIntroduction(),
portfolio.getCreatedAt(),
portfolio.getUpdatedAt(),
toContactResponsesDto(portfolio.getContacts()),
toProjectResponsesDto(portfolio.getProjects()), // *
toSectionResponsesDto(portfolio.getSections())
);
}

private List<ProjectResponseDto> toProjectResponsesDto(Projects projects) { // *
return projects.getValues().stream()
.map(this::toProjectResponseDto)
.collect(toList());
}

private ProjectResponseDto toProjectResponseDto(Project project) {
List<TagResponseDto> tags = project.getTags().stream()
.map(this::toTagResponseDto) // *
.collect(toList());

return new ProjectResponseDto(
project.getId(),
project.getName(),
project.getStartDate(),
project.getEndDate(),
project.getType().getValue(),
project.getImageUrl(),
project.getContent(),
tags
);
}

private TagResponseDto toTagResponseDto(ProjectTag tag) { // *
return new TagResponseDto(
tag.getTagId(),
tag.getTagName()
);
}
🌩 이상현상 들여다보기 프로젝트 코드로 들여다보기는 복잡하여 파악하기 어려움으로 동일한 상황을 간단한 테스트코드로 재현해보았다. Entity 구조 Member.java @Entity
public class Member {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

private String name;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""team_id"")
private Team team;

// getter 및 생성자 생략
}
Team.java @Entity
public class Team {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

private String name;

// getter 및 생성자 생략
}
상황 재현 먼저 application.properties에 다음 설정을 해주었다. spring.jpa.properties.hibernate.default_batch_fetch_size=10
다음은 In 쿼리가 정상동작하는 테스트코드다. @DisplayName(""Member 리스트 조회 시 Team을 lazy loading 할 때 in 쿼리 Team이 한꺼번에 조회된다."")
@Test
void team_inquery_working() {

// given
Team teamA = new Team(""TeamA"");
Team teamB = new Team(""TeamB"");
teamRepository.save(teamA);
teamRepository.save(teamB);

testEntityManager.flush();
testEntityManager.clear();

Member member1 = new Member(""member1"");
Member member2 = new Member(""member2"");
member1.setTeam(teamA);
member2.setTeam(teamB);

memberRepository.save(member1);
memberRepository.save(member2);

testEntityManager.flush();
testEntityManager.clear();

// when
List<Member> members = new ArrayList<>();
members.add(memberRepository.findById(1L).get()); // Member 를 조회하는 쿼리가 생성된다.
members.add(memberRepository.findById(2L).get());

List<String> teamNames = members.stream()
.map(member -> member.getTeam().getName())
.collect(toList()); // Team 을 조회하는 쿼리가 in 쿼리로 수행된다.

// then
assertThat(teamNames).hasSize(2);
}
[실행 Query] Hibernate:
select
team0_.id as id1_1_0_,
team0_.name as name2_1_0_
from
team team0_
where
team0_.id in (
?, ?
)
다음은 프로젝트 상황은 동일하게 재현한 in 쿼리가 수행되지 않는 테스트코드다. @DisplayName(""Team을 initialize 할 때 in 쿼리가 수행되지 않는다."")
@Test
void team_inquery_notWorking() {

EntityManager em = testEntityManager.getEntityManager();

// given
Team savedTeamA = teamRepository.save(new Team(""TeamA""));
Team savedTeamB = teamRepository.save(new Team(""TeamB""));

testEntityManager.flush();
testEntityManager.clear();

// when
Member member1 = new Member(""member1"");
Member member2 = new Member(""member2"");

Team teamA = em.getReference(Team.class, savedTeamA.getId()); // Team은 프록시 객체다.
Team teamB = em.getReference(Team.class, savedTeamB.getId());
member1.setTeam(teamA);
member2.setTeam(teamB);

memberRepository.save(member1);
memberRepository.save(member2);

testEntityManager.flush();

List<Member> members = new ArrayList<>();
members.add(memberRepository.findById(1L).get()); // 영속성 컨텍스트에 있는 Member를 로딩한다.
members.add(memberRepository.findById(2L).get());

List<String> teamNames = members.stream()
.map(member -> member.getTeam().getName()) // 각 멤버의 개수만큼 team을 select하는 쿼리를 실행한다.
.collect(toList());

// then
assertThat(teamNames).hasSize(2);
}
[실행 Query] Hibernate:
select
team0_.id as id1_1_0_,
team0_.name as name2_1_0_
from
team team0_
where
team0_.id=?

Hibernate:
select
team0_.id as id1_1_0_,
team0_.name as name2_1_0_
from
team team0_
where
team0_.id=?
findAll()로 전체 멤버를 조회할 수 있지만, 그렇다면 두번째 테스트코드의 경우 모든 멤버가 영속성 컨텍스트에 있음에도 불구하고 영속성 컨텍스트는 전체 데이터인지 알 수 없기 때문에 Member를 조회하는 쿼리를 날린다. 영속성 컨텍스트에 이미 있는 엔티티를 가져온다는 것을 확인하기 위해 Member를 findById()로 가져왔다. 🌩 Help default_batch_fetch_size 설정이 먹히지 않는 이유로 해당 엔티티 (대상 엔티티 혹은 상위 엔티티)가 영속성 컨텍스트에서 관리되지 않을 경우를 생각해볼 수 있다. 위 개념에서 다루었듯이 아직 초기화 되지 않은 collections 혹은 lazy products에 대해서 한꺼번에 로딩해주는 역할을 하기 때문이다. 만일 하위 엔티티가 영속성 컨텍스트에서 관리되고 있지 않다면 로딩할 프록시 또한 없을 것이고 상위 엔티티가 관리되고 있지 않다면 연관관계를 파악할 수 없으므로 in 쿼리에 인자로 보낼 id 값이 없을 것이다. 위 테스트 코드를 보았을 때 첫번째와 두번째 상황을 요약해보자. 첫번째 테스트코드 - in 쿼리 동작 Member가 findById로 조회되고 Team은 프록시 객체이다. (Lazy loading) Member 리스트의 팀 목록을 조회할 때 Team의 Id가 in 쿼리로 들어간다. 두번째 테스트코드 - in 쿼리 동작 안함 Team은 이미 존재한다. 새로운 Member를 생성하고 Team을 em.getReference()를 통해 Team의 프록시 객체를 Member의 Team을 지정한다. 이후 save()를 통해서 Member 엔티티를 저장하고 flush 하여 데이터베이스에 반영한다. findAll()를 통헤 멤버 List를 가져온다. 이때 영속성 컨텍스트에 있는 Member가 조회된다. 해당 Member의 Team은 em.getReference()로 조회된 프록시 객체이다. 다음과 같은 이유로 두 테스트코드가 같은 상황이라고 생각한다. Member가 영속성 컨텍스트에 실제 엔티티로 관리되고 있다는 것. Member와 연관된 Team 엔티티가 모두 프록시 객체이며 영속성 컨텍스트에 있다는 것. 연관관계는 두 경우 모두 잘 매핑이 되어 있다는 것. 두번째 테스트코드의 마지막 flush() 이후 clear()를 통해 영속성 컨텍스를 한번 초기화 하면 in 쿼리가 정상동작한다. 실제 조회된 Member 리스트의 내부를 디버깅해 들여다 보았다. In query가 정상 동작하는 members In query가 동작하지 않는 members 디버깅했을 때 두가지 상태가 모두 똑같지만 하나는 in 쿼리가 동작하고 하나는 동작하지 않는 이유를 결국 못 찾았다. 😢 우선은 현상만 기록하고 계속 알아볼 예정이다 !! 혹시 아시는 분은 .. 연락주세요.. 깃헙이나 이메일, 댓글 아무거나 환영 !! 🎉 [참고자료] apiref.com/hibernate5/BatchSize.html https://docs.jboss.org/hibernate/orm/4.3/manual/en-US/html/ch05.html https://wckhg89.tistory.com/10 백엔드 코다입니다 🙌 Twitter Facebook Google+ # 백엔드 # JPA",BE
442,"Nginx에 HTTP 2.0을 적용하는 방법 2021, Oct 22 안녕하세요, 다니입니다. 🌻 필자는 Docker로 Nginx를 구성했다. 따라서, 이어지는 과정은 Docker를 기반으로 진행된다. (추가) 덧붙임에 Nginx를 직접 컴파일하는 경우에 대한 정보를 적어뒀다. HTTPS 설정 (HTTP + TLS) 만약 Nginx 서버가 HTTP를 사용 중이라면, 먼저 HTTPS를 사용하게 변경해야 한다. TLS 인증서 발급 Let’s Encrypt를 활용하여 무료 TLS 인증서를 발급받는다. $ docker run -it --rm --name certbot 
-v '/etc/letsencrypt:/etc/letsencrypt' 
-v '/var/lib/letsencrypt:/var/lib/letsencrypt' 
certbot/certbot certonly -d 'yourdomain.com' --manual --preferred-challenges dns --server https://acme-v02.api.letsencrypt.org/directory
DNS 발급 내도메인.한국을 활용하여 무료 DNS를 발급받는다. DNS 정보 추가 DNS 정보에 서버의 Public IP(from EC2)와 DNS TXT Record(from Let’s Encrypt)를 추가한다. TLS 인증서 추가 Dockerfile 파일, nginx.conf 파일에 TLS 인증서를 추가한다. // Dockerfile
FROM nginx

COPY nginx.conf /etc/nginx/nginx.conf
COPY fullchain.pem /etc/letsencrypt/live/{도메인_주소}/fullchain.pem
COPY privkey.pem /etc/letsencrypt/live/{도메인_주소}/privkey.pem
// nginx.conf
events {}

http {
upstream app {
server {WAS_IP:WAS_PORT};
}

# Redirect all traffic to HTTPS
server {
listen 80;
return 301 https://$host$request_uri;
}

server {
listen 443 ssl;
ssl_certificate /etc/letsencrypt/live/{도메인_주소}/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/{도메인_주소}/privkey.pem;

# Disable SSL
ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

# 통신과정에서 사용할 암호화 알고리즘
ssl_prefer_server_ciphers on;
ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5;

# Enable HSTS
# client의 browser에게 http로 어떠한 것도 load 하지 말라고 규제합니다.
# 이를 통해 http에서 https로 redirect 되는 request를 minimize 할 수 있습니다.
add_header Strict-Transport-Security ""max-age=31536000"" always;

# SSL sessions
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;

location / {
proxy_pass http://app;
}
}
}
HTTP 2.0 적용 nginx.conf 파일에 http2를 추가한다. // nginx.conf
events {}

http {
upstream app {
server {WAS_IP:WAS_PORT};
}

# Redirect all traffic to HTTPS
server {
listen 80;
return 301 https://$host$request_uri;
}

server {
// 이곳에 추가한다.
listen 443 ssl http2;
ssl_certificate /etc/letsencrypt/live/{도메인_주소}/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/{도메인_주소}/privkey.pem;

# Disable SSL
ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

# 통신과정에서 사용할 암호화 알고리즘
ssl_prefer_server_ciphers on;
ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5;

# Enable HSTS
# client의 browser에게 http로 어떠한 것도 load 하지 말라고 규제합니다.
# 이를 통해 http에서 https로 redirect 되는 request를 minimize 할 수 있습니다.
add_header Strict-Transport-Security ""max-age=31536000"" always;

# SSL sessions
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;

location / {
proxy_pass http://app;
}
}
}
Docker Image 생성 지금까지의 수정 내용을 반영하는 Docker Image를 생성한다. $ docker build -t {docker_image_repository_name} {path}
Docker 실행 해당 Docker Image를 이용하여 Docker Container를 실행한다. $ docker run -d -p 80:80 -p 443:443 --name {docker_container_name} {docker_image_repository_name}
결과 비교 API 응답 크기가 줄어들고, 속도가 빨라졌다. HTTP 1.1의 단점을 개선한 게 HTTP 2.0인데, 이와 관련한 내용은 다음에 정리하자. HTTP 1.1 HTTP 2.0 덧붙임 Nginx를 직접 컴파일하는 경우 Nginx는 일부 모듈을 default로 제공한다. 하지만, http2에서 사용하는 모듈(–with-http_v2_module)은 default로 제공되지 않고, 따로 설정해야 한다. // configure 파일이 있는 위치로 이동한다.
[REVERSE_PROXY][07:12:15][ubuntu@ip-xxx-xxx-x-xxx ~/nginx_with_health_check/nginx-1.14.2]
$ ls
... conf configure contrib ...

// configure를 실행하여 http2 모듈을 설정한다.
$ ./configure --prefix=/usr/local/nginx --with-http_v2_module --with-{이_외_다른_모듈} ...

// nginx를 컴파일하고, 설치한다.
$ make
$ make install
결과 비교 이때에도 API 응답 크기와 속도가 개선된 것을 확인할 수 있다. HTTP 1.1 HTTP 2.0 References 우아한테크코스 강의 자료 우아한테크코스 완태 Nginx 공식 문서 - Module ngx_http_v2_module Defining http2 without ssl leads to HTTP/1.1 client failure Nginx 공식 문서 - Building nginx from Sources HTTP/2 Now Fully Supported in NGINX Plus (썸네일) Twitter Facebook Google+ # 백엔드 # Network # Nginx # HTTP 2.0",BE
443,"비효율적인 Blocking 코드를 WebClient를 통해 개선하기 2021, Oct 31 1. 들어가며 안녕하세요, 케빈입니다. Pick-Git 서비스는 사용자의 GitHub 활동 통계를 시각화하는 기능을 제공 중입니다. 그런데 개발 과정에서 활동 통계 조회 페이지 로딩 시간이 너무 오래 걸린다는 피드백을 자주 받았습니다. 프론트엔드의 렌더링 이슈인줄 알았지만 통계 정보 조회 API 요청 처리에 너무 많은 시간이 소요되는 문제가 확인되었습니다. 요청 처리에 4초가 걸리던 로직을 WebClient를 통해 0.9초로 개선한 사례를 공유하고자 합니다. 2. 다섯 번의 외부 API 요청 GitHubContributionCalculator.java @Component
public class GithubContributionCalculator implements PlatformContributionCalculator {

private final RestTemplate restTemplate;

public GithubContributionCalculator(RestTemplate restTemplate) {
this.restTemplate = restTemplate;
}

@Override
public Contribution calculate(String accessToken, String username) {
int starCounts = calculateStars(accessToken, username);
int commitCounts = counts(""/search/commits?q=committer:%s"", accessToken, username);
int prCounts = counts(""/search/issues?q=author:%s type:pr"", accessToken, username);
int issueCounts = counts(""/search/issues?q=author:%s type:issue"", accessToken, username);
int repoCounts = counts(""/search/issues?q=author:%s type:issue"", accessToken, username);

return Contribution.builder()
.starsCount(starCounts)
.commitsCount(commitCounts)
.prsCount(prCounts)
.issuesCount(issueCounts)
.reposCount(repoCounts)
.build();
}

private int calculateStars(String accessToken, String username) {
// RestTemplate으로 GitHub에 조회 API 요청
}

private int counts(String url, String accessToken) {
// RestTemplate으로 GitHub에 조회 API 요청
}
}
활동 통계를 조회하는 프로덕션 코드입니다. GitHub은 우리가 원하는 형태의 유저 활동 통계 API를 제공하지 않습니다. 따라서 시각화하고 싶은 데이터를 제공하는 API들을 찾아 개별적으로 전부 요청해야 합니다. 현재 Pick-Git 서비스는 5가지의 정보를 시각화합니다. Star 개수. Commit 개수. PR 개수. Issue 개수. Repository 개수. 2.1. Sync vs Async & Blocking vs Non-Blocking 문제 상황을 본격적으로 분석하기 전에 동기와 비동기, Blocking과 Non-Blocking에 대한 개념을 이해해야 합니다. 자세한 내용은 블로킹 Vs. 논블로킹, 동기 Vs. 비동기 글을 참고해주세요. 간단하게 요약하자면 다음과 같습니다. 동기-비동기란 특정 주체가 호출되는 함수의 작업 완료 여부를 신경쓰는지의 여부 차이다. Blocking-Nonblocking이란 특정 주체가 함수를 호출할 때 제어권을 양도하는지의 여부 차이다. Test.js function a() {
let result = b();
console.log(result);
console.log(""a is done"");
}

function b() {
return 10;
}

/*
실행 결과
10
a is done
*/
Synchronous(동기)란 작업을 요청한 후 작업의 결과가 나올 때까지 기다린 후 처리하는 것을 의미합니다. A 함수가 B 함수를 호출했을 때, A 함수가 B 함수의 수행 결과 및 종료를 신경쓰는 경우를 예로 들 수 있습니다. 일반적인 경우 Blocking과 동일한 의미로 사용될 수 있습니다. Test.js function a() {
fetch(url, options)
.then(response => console.log(""response arrives""))
.catch(error => console.log(""error thrown""));
console.log(""a is done"");
}

/*
실행 결과
a is done
response arrives
*/
반면 Asynchronous(비동기)란 두 주체가 서로의 시작/종료 시간과는 관계 없이 별도의 수행 시작/종료시간을 가지고 있는 것을 의미합니다. A 함수가 B 함수를 호출했을 때, 호출된 함수의 수행 결과 및 종료를 호출된 함수 혼자 직접 신경 쓰고 처리하는 경우를 예로 들 수 있습니다. 대게 결과를 돌려주었을 때 순서와 결과(처리)에 관심이 있는지 아닌지로 판단할 수 있습니다. 비동기를 사용하면 두 개의 요청을 동시에 보내기 때문에 더 빠른 응답 속도를 보여줄 수 있습니다. 또한 현재 스레드가 Blocking되지 않고 다른 작업을 수행할 수 있어서, 더 적은 수의 리소스(스레드)로 더 많은 양의 요청을 처리할 수 있습니다. 2.2. Non-Blocking 레거시 코드 GitHubContributionCalculator.java private int counts(String url, String accessToken) {
HttpHeaders httpHeaders = new HttpHeaders();
httpHeaders.setBearerAuth(accessToken);
httpHeaders.set(""Accept"", ""application/vnd.github.cloak-preview"");

RequestEntity<Void> requestEntity = RequestEntity
.get(url)
.headers(httpHeaders)
.build();

return new RestTemplate()
.exchange(requestEntity, String.class)
.getBody();
}
문제는 활동 통계 조회를 위해 GitHub으로 API 요청을 보내는 RestTemplate 코드가 동기(Synchronous)적인 Blocking 로직입니다. Star 개수 요청에 대한 응답이 오면 이후 Commit 개수 요청을 보내고, Commit 개수 요청에 대한 응답이 오면 그 다음 통계 데이터에 대한 요청을 보내는 식입니다. 따라서 기존의 로직을 처리하는 수행 시간은 5개 GitHub API 요청 수행 시간의 합과 같습니다. 실제로 한 유저의 GitHub 활동 통계 정보를 조회하는 서비스 로직의 속도를 측정해본 결과, 평균 4.3초 정도가 소요되었습니다. 3. Async + Non-Blocking 개선 이러한 로직을 비동기적이고 Non-Blocking하게 동작하도록 개선하면 병목을 다소 해소할 것으로 기대했습니다. 비동적이고 Non-Blocking하게 동작하게 한다면 수행 시간은 5개 GitHub API 요청 수행 시간 중 최대가 됩니다. 3.1. 기술 선택 비동기 및 Non-Blocking을 구현하는 방법은 다양합니다. AsyncRestTemplate. RestTemplate + Spring의 @Async 애너테이션 사용. RestTemplate + Java Concurrency API인 CompletableFuture 사용. WebFlux 사용. 현재 프로덕션 코드에서 API 호출하는 부분은 전부 RestTemplate을 사용 중입니다. 현재 구조에서는 1, 2, 3번을 사용하는 것이 가장 쉬워보입니다. 그러나 RestTemplate Javadoc을 살펴보면 다음 내용을 확인할 수 있습니다. NOTE: As of 5.0, the non-blocking, reactive org.springframework.web.reactive.client.WebClient offers a modern alternative to the RestTemplate with efficient support for both sync and async, as well as streaming scenarios. The RestTemplate will be deprecated in a future version and will not have major new features added going forward. See the WebClient section of the Spring Framework reference documentation for more details and example code. NOTE: As of 5.0 this class is in maintenance mode, with only minor requests for changes and bugs to be accepted going forward. Please, consider using the org.springframework.web.reactive.client.WebClient which has a more modern API and supports sync, async, and streaming scenarios. 요약하자면 Spring 5.0 이상부터 RestTemplate은 Deprecated될 예정이기 때문에, 모던한 API를 기반으로 동기 및 비동기를 모두 지원하는 WebClient를 사용하도록 권고하고 있습니다. 실제로 AsyncRestTemplate은 이미 Deprecated된 상태입니다. CompletableFuture 등을 직접 다루는 것 또한 좋은 학습 기회가 되겠지만, 굳이 Deprecated될 API를 사용해야할 필요가 없다고 판단했습니다. 4. Spring WebFlux WebFlux에 대한 개념을 전부 다루기에는 개념이 너무 방대하기 때문에 최대한 간략하게 요약했습니다. 더 디테일한 내용을 알고싶다면 Spring Web on Reactive Stack와 Webflux 공부하자 1편 및 Armeria로 Reactive Streams와 놀자! – 1 등의 글을 참고해주세요. 4.1. Reactive Reactive란 변화에 반응하는 것을 중심에 두고 만든 프로그래밍 모델을 의미합니다. I/O 이벤트에 반응하는 네트워크 컴포넌트. 마우스 이벤트에 반응하는 UI 컨트롤러. Non-Blocking이란 작업을 기다리기보다는 완료되거나 데이터를 사용할 수 있게 되면 알림 등으로 반응하므로 Reactive라고 표현할 수 있습니다. 즉, Reacitve 프로그래밍이란 기존 프로그래밍에서 다양하게 사용하던 방식(필요한 데이터가 있으면 데이터를 Consume하는 쪽에서 함수를 호출해서 데이터를 Pull)을 어떤 변화(이벤트)가 발생할 때 데이터를 Produce하는 쪽에서 Push하는 방식으로 변화한 것입니다. 이벤트 드리븐 아티텍쳐, 옵저버 패턴, Pub-Sub 패턴 등의 개념과 유사합니다. 서버 관점에서 Reactive를 사용하는 이유는 Async & Non-Blocking을 이용해서 더 적은 자원으로 더 많은 트래픽을 처리하기 위함입니다. Servlet 3.1은 Non-Blocking I/O를 위한 API를 제공합니다. 그러나 Servlet으로 Non-Blocking을 구현하려면 다른 동기 처리나 Blocking 방식을 사용하는 API를 사용하기 어렵습니다. 어떠한 Non-Blocking과도 잘 동작하는 공통 API가 필요한 시점이 도래했습니다. 특히 Async & Non-Blocking 환경에 자리 잡은 Netty와 같은 서버 때문이라도 새로운 API가 필요했습니다. Spring WebFlux는 적은 스레드로 동시 처리를 제어하고 적은 하드웨어 리소스로 확장할 수 있는 Non-Blocking Web Stack입니다. 4.2. Spring WebFlux vs Spring MVC Spring WebFlux는 Spring 5에서 새롭게 추가된 모듈입니다. WebFlux는 Reactive 스타일의 어플리케이션 개발을 도와주는 reactive-stack web framework입니다. Non-Blocking에 Reactive Streams을 지원합니다. Reactive Stack인 경우에는 Reactor가 필수이고, Netty와 같은 Async & Non-Blocking 모델의 네트워킹 프레임워크를 사용합니다. 물론 Reactive Stack에서도 Tomcat 및 Jetty와 같은 서블릿 기반의 컨테이너를 사용할 수 있습니다. 그러나 이 경우에는 Servlet 3.1 Non-Blocking I/O를 사용합니다. 어플리케이션 특성에 맞게 Reactive Stack 및 Servlet Stack 중 하나를 선택해서 사용해야 합니다. Reactor 및 Reactive Streams은 후술합니다. Spring MVC와 WebFlux의 공통점은 @Controller, Reactive 클라이언트입니다. 둘 다 Tomcat, Jetty, Undertow 등의 서버에서 실행이 가능합니다. Spring WebFlux는 이벤트 루프, 동시성 모델, Netty 서버 위에서 구동 가능 등의 장점이 있습니다. Spring MVC에서 Spring WebFlux로 넘어가면서 Servlet 구조의 멀티 스레드에서 이벤트 기반 Reactive 프로그래밍으로 패러다임이 전환됩니다. Spring WebFlux는 기본적으로 Netty를 통해 네트워킹을 합니다. 스레드의 수는 1 요청 클라이언트당 하나씩 생성되는 구조가 아니라, 연결을 수락하는 하나의 보스 스레드와 CPU 코어 수에 비례하는 N개의 워커 스레드로 구성됩니다. 만약 워커 스레드가 Block이 오래 걸리면 클라이언트의 요청을 원활히 수행할 수 없습니다. 또한 워커 스레드에서 또 다른 스레드를 매 번 생성한다면 기존 멀티 스레드 모델보다 오히려 더 많은 스레드가 필요하게 됩니다. CPU 연산이 오래 걸리거나 I/O 블로킹 작업인 경우에는 Async & Non-Blocking 데이터 스트림으로 작업을 처리할 필요가 있기 때문에, Spring WebFlux는 Project Reactor라는 Reactive Stack이 기본적으로 추가되어 있습니다. 4.3. Reactive Streams Spring WebFlux에서 사용하는 Reactive 라이브러리가 Reactor입니다. 그리고 Reactor가 Reactive Streams의 구현체입니다. 공식 문서에 따르면 Reactive Streams은 Non-Blocking Back Pressure를 통한 비동기 스트림 데이터 처리 표준을 제공하기 위한 명세입니다. 비동기 스트림 처리를 위한 명세는 4개로 정의할 수 있습니다. Reactor public interface Publisher<T> {
public void subscribe(Subscriber<? super T> s);
}

public interface Subscriber<T> {
public void onSubscribe(Subscription s);
public void onNext(T t);
public void onError(Throwable t);
public void onComplete();
}

public interface Subscription {
public void request(long n);
public void cancel();
}
Publisher : 경계가 미확정된 순차적 데이터들을 생성하는 컴포넌트. Subscriber : 순차적 데이터를 받아서 처리하는 컴포넌트. Subscription : Publisher에 의해 발행되는 구독 정보 컴포넌트. Processor : Publisher & Subscriber Stream의 미들웨어. 다양한 Processor들이 Reactor에서 제공됩니다. Subscriber가 subscribe 함수를 사용해 Publisher에게 구독을 요청합니다. Publisher는 onSubscribe 함수를 사용해 Subscriber에게 Subscription을 전달합니다. Subscription은 Subscriber와 Publisher간 통신의 매개체가 됩니다. Subscriber는 Publisher에게 직접 데이터 요청을 하지 않고, Subscription의 request 함수를 통해 Publisher에게 전달합니다. Publisher는 Subscription을 통해 Subscriber의 onNext에 데이터를 전달하고, 작업이 완료되면 onComplete 혹은 에러가 발생하면 onError 시그널을 전달합니다. Subscriber와 Publisher, Subscription이 서로 유기적으로 연결되어 통신을 주고받으면서 subscribe부터 onComplete까지 연결되고, 이를 통해 Back Pressure가 완성됩니다. 4.4. Back Pressure Back Pressure란 Producer의 속도가 Consumer 속도를 압도하지 않도록 이벤트 속도를 제어하는 것을 의미합니다. Reactive 프로그래밍은 발행자(Publisher)가 구독자(Subscriber)에게 밀어 넣는 방식으로 데이터가 전달됩니다. 처리 속도가 불균형하다면 대기 중인 이벤트를 Queue에 저장합니다. 문제는 서버가 가용할 수 있는 메모리는 한정되어 있다는 점입니다. 서버의 고정 길이 버퍼가 꽉 차면 Subscriber는 신규로 수신된 메시지를 거절합니다. 거절된 메시지를 재요청하는 과정에서 네트워크와 CPU 연산 비용이 추가로 발생합니다. 서버가 가변 길이 버퍼를 사용한다면 이벤트를 저장할 때 ‘out of memory’ 에러가 발생할 수 있습니다. 이러한 Push 방식의 단점을 극복하기 위해, Publisher는 Subscriber가 필요한 만큼의 데이터만 전달하는 Pull 방식을 사용할 수 있는데 이것이 Back Pressure의 기본 원리입니다. Subscriber가 10개를 처리할 수 있다면 Publisher에게 10개만 요청합니다. Publisher는 요청받은 만큼만 전달하고, 구독자는 기존 Push 방식에서 발생하는 부작용에서 자유로워집니다. Subscriber가 이미 8개의 일을 처리하고 있다면 추가로 2개만 더 요청해서 자신이 현재 처리 가능한 범위 내에서만 메시지를 받게 할 수 있습니다. Pull 방식에서 전달되는 모든 데이터의 크기는 Subscriber가 결정합니다. Dynamic Pull 방식의 데이터 요청을 통해 Subscriber가 수용할 수 있는 만큼만 데이터를 요청하는 방식이 Back Pressure입니다. Publisher는 Subscriber의 onNext 호출 토탈 횟수가 Subscription의 request 파라미터 횟수를 초과해서는 안됩니다. Reactive Streams의 인터페이스 명세를 구현한 Reactor 구현체는 Subscription의 request 메서드를 통해 Backpressure 기능을 제공하고 있습니다. 4.5. Mono & Flux Reactor에서 사용하는 Publisher 객체에는 Mono와 Flux가 있습니다. 차이점은 발행하는 데이터 갯수입니다. Mono : 0 ~ 1 개의 데이터 스트림입니다. Flux : 0 ~ N 개의 데이터 스트림입니다. 5. Refactoring GitHubContributionCalculator.java @Component
public class GithubContributionCalculator implements PlatformContributionCalculator {

private final WebClient webClient;

public GithubContributionCalculator(WebClient webClient) {
this.webClient = webClient;
}

@Override
public Contribution calculate(String accessToken, String username) {
Map<ContributionCategory, Integer> bucket = getContributionsViaPlatform(accessToken, username, latch);
return new Contribution(bucket);
}

private Map<ContributionCategory, Integer> getContributionsViaPlatform(String accessToken, String username) {
Map<ContributionCategory, Integer> bucket = new EnumMap<>(ContributionCategory.class);
extractStars(accessToken, username, bucket);
extractCount(COMMIT, ""/search/commits?q=committer:%s"", accessToken, username, bucket);
extractCount(PR, ""/search/issues?q=author:%s type:pr"", accessToken, username, bucket);
extractCount(ISSUE, ""/search/issues?q=author:%s type:issue"", accessToken, username, bucket);
extractCount(REPO, ""/search/issues?q=author:%s type:issue"", accessToken, username, bucket);

return bucket;
}

private void extractStars(
String accessToken,
String username,
Map<ContributionCategory, Integer> bucket
) {
String apiUrl = generateUrl(username);
sendGitHubApi(accessToken, apiUrl)
.bodyToMono(ItemDto.class)
.subscribe(result -> bucket.put(STAR, result.sum()));
}

private void extractCount(
ContributionCategory category,
String restUrl,
String accessToken,
String username,
Map<ContributionCategory, Integer> bucket
) {
String apiUrl = generateUrl(restUrl, username);
sendGitHubApi(accessToken, apiUrl)
.bodyToMono(CountDto.class)
.subscribe(result -> bucket.put(category, result.getCount()));
}

private ResponseSpec sendGitHubApi(String accessToken, String apiUrl) {
return webClient.get()
.uri(apiUrl)
.headers(httpHeaders -> {
httpHeaders.setBearerAuth(accessToken);
httpHeaders.set(""Accept"", ""application/vnd.github.cloak-preview"");
})
.accept(MediaType.APPLICATION_JSON)
.retrieve()
.onStatus(HttpStatus::isError, error -> Mono.error(PlatformHttpErrorException::new));
}
}
WebClient는 이벤트에 반응형으로 동작하도록 설계되었습니다. Spring React 프레임워크를 사용하며, React Web 프레임워크인 Spring WebFlux의 Http Client로 사용됩니다. WebClient를 통해 GitHub으로 조회 API를 요청합니다. 응답이 오면 미리 subscribe를 통해 구독해둔대로, Map에 통계 데이터가 저장됩니다. 그러나 Async & Non-Blocking하기 때문에 Map에 값이 확실하게 저장되는지 응답을 기다릴 필요 없이, 연속적으로 5건의 API를 호출한 뒤 다른 작업을 수행하게 됩니다. 5.1. CountDownLatch 리팩터링한 코드를 기반으로 유저의 GitHub 활동 통계 조회 API를 테스트해본 결과, 뜻밖에도 Null Pointer Exception이 발생했습니다. HTTP 응답을 작성하기 위해 Map에서 통계 데이터를 get으로 조회하려고 했으나, 해당 시점이 너무 일러 아직 GitHub으로부터 응답이 오기 전이었기 때문입니다. 응답을 작성하려는 시점에 Map에 원하는 데이터가 전부 존재하는지 확인할 필요가 있습니다. (모놀로틱한 서비스다보니 완벽한 비동기 로직을 사용하기에 약간의 한계가 존재합니다.) 응답을 작성하기 전에 반복문을 돌려, Map의 KeySet 크기가 원하는 사이즈가 될 때까지 Busy-Wait하니 NPE를 해결할 수 있었습니다. 이 때, 팀원 손너잘이 CountDownLatch를 통해 코드를 조금 더 개선했는데요. CountDownLatch란 어떤 스레드가 다른 스레드에서 작업이 완료될 때 까지 기다릴 수 있도록 해주는 클래스입니다. GitHubContributionCalculator.java @Component
public class GithubContributionCalculator implements PlatformContributionCalculator {

private static final int CONTRIBUTION_COUNT = 5;

private final WebClient webClient;

public GithubContributionCalculator(WebClient webClient) {
this.webClient = webClient;
}

@Override
public Contribution calculate(String accessToken, String username) {
try {
CountDownLatch latch = new CountDownLatch(CONTRIBUTION_COUNT);
Map<ContributionCategory, Integer> bucket = getContributionsViaPlatform(accessToken, username, latch);
waitThreads(latch);

return new Contribution(bucket);
} catch (InterruptedException e) {
Thread.currentThread().interrupt();
throw new PlatformInternalThreadException();
}
}

private Map<ContributionCategory, Integer> getContributionsViaPlatform(String accessToken, String username, CountDownLatch latch) {
Map<ContributionCategory, Integer> bucket = new EnumMap<>(ContributionCategory.class);
extractStars(accessToken, username, bucket, latch);
extractCount(COMMIT, ""/search/commits?q=committer:%s"", accessToken, username, bucket, latch);
extractCount(PR, ""/search/issues?q=author:%s type:pr"", accessToken, username, bucket ,latch);
extractCount(ISSUE, ""/search/issues?q=author:%s type:issue"", accessToken, username, bucket, latch);
extractCount(REPO, ""/search/issues?q=author:%s type:issue"", accessToken, username, bucket, latch);

return bucket;
}

private void extractStars(
String accessToken,
String username,
Map<ContributionCategory, Integer> bucket,
CountDownLatch latch
) {
String apiUrl = generateUrl(username);
sendGitHubApi(accessToken, apiUrl)
.bodyToMono(ItemDto.class)
.subscribe(result -> {
bucket.put(STAR, result.sum());
latch.countDown();
});
}

private void extractCount(
ContributionCategory category,
String restUrl,
String accessToken,
String username,
Map<ContributionCategory, Integer> bucket,
CountDownLatch latch
) {
String apiUrl = generateUrl(restUrl, username);
sendGitHubApi(accessToken, apiUrl)
.bodyToMono(CountDto.class)
.subscribe(result -> {
bucket.put(category, result.getCount());
latch.countDown();
});
}

private ResponseSpec sendGitHubApi(String accessToken, String apiUrl) {
return webClient.get()
.uri(apiUrl)
.headers(httpHeaders -> {
httpHeaders.setBearerAuth(accessToken);
httpHeaders.set(""Accept"", ""application/vnd.github.cloak-preview"");
})
.accept(MediaType.APPLICATION_JSON)
.retrieve()
.onStatus(HttpStatus::isError, error -> Mono.error(PlatformHttpErrorException::new));
}

private void waitThreads(CountDownLatch latch) throws InterruptedException {
if (!latch.await(2, TimeUnit.SECONDS)) {
throw new PlatformHttpErrorException();
}
}
}
응답을 작성하기 전에 총 5건의 데이터가 Map에 잘 저장되었는지 확인해야 합니다. 즉, subscribe 내부 Consumer 메서드는 총 5번 호출되어야합니다. 따라서 CountDownLatch의 기본값을 5개로 두고, subscribe 내부 Consumer 메서드가 호출될 때마다 countDown을 통해 Latch 숫자를 감소하도록 합니다. 5건의 Async & Non-Blocking API 호출이후, awiat을 통해 Latch가 0이 될 때까지 기다리게합니다. 그러나 5건의 API 응답을 모두 받아보는데 2초 이상이 걸리는 경우 예외를 발생시키도록 합니다. 5건의 API 응답을 받는데 소요된 시간이 2초 이내라면, await은 더 이상 기다리지 않고 다음 코드를 실행하게 됩니다. 이제 HTTP 응답을 작성하는 시점에 Map에 데이터가 없어 NPE가 발생하는 문제가 발생하지 않습니다. 4.3초 걸리던 서비스 수행 로직이 0.9초로 개선된 것을 확인할 수 있습니다. References 배달의민족 최전방 시스템! ‘가게노출 시스템’을 소개합니다. [Spring]WebClient를 이용한 REST API 호출 [Spring] WebFlux의 개념 / Spring MVC와 간단비교 Spring Webflux + Reactor Spring Web on Reactive Stack Armeria로 Reactive Streams와 놀자! – 1 Webflux 공부하자 1편 [스프링 리액티브] Observer Pattern과 Pub/Sub Pattern Twitter Facebook Google+ # 백엔드 # WebFlux",BE
453,"Spring Data Elasticsearch 설정 및 검색 기능 구현 2021, Oct 18 실습 Repository에서 코드를 확인할 수 있습니다. 1. Elasticsearch Elasticsearch는 Apache Lucene 기반의 Java 오픈소스 분산형 RESTful 검색 및 분석 엔진입니다. 방대한 양의 데이터에 대해 실시간으로 저장과 검색 및 분석 등의 작업을 수행할 수 있습니다. 특히 정형 데이터, 비정형 데이터, 지리 데이터 등 모든 타입의 데이터를 처리할 수 있는데요. Elasticsearch는 JSON 문서(Document)로 데이터를 저장하기 때문입니다. Elasticsearch는 단독 검색을 위해 사용하거나, ELK(Elasticsearch & Logstash & Kibana) 스택을 기반으로 사용합니다. 1.1. ELK 본 글의 주제와는 벗어나지만 ELK에 대해서도 짧게나마 알아봅시다. ELK는 Elasticsearch + Logstash + Kibana를 같이 연동하여 사용한다는 의미입니다. Filebeat 로그를 생성하는 서버에 설치해 로그를 수집합니다. Logstash 서버로 로그를 전송합니다. Logstash 로그 및 트랜잭션 데이터를 수집과 집계 및 파싱하여 Elasticsearch로 전달합니다. 정제 및 전처리를 담당합니다. Elasticsearch Logstash로부터 전달받은 데이터를 저장하고, 검색 및 집계 등의 기능을 제공합니다. Kibana 저장된 로그를 Elasticsearch의 빠른 검색을 통해 가져오며, 이를 시각화 및 모니터링하는 기능을 제공합니다. 1.2. RDB 비교 Elasticsearch는 데이터를 행렬 데이터로 저장하는 것이 아니라, JSON 문서(Document)로 직렬화된 복잡한 자료 구조를 저장하는 방식을 채택하고 있습니다. 따라서 기존 RDB에서 사용하던 용어를 그대로 사용하지 않습니다. 단, 그에 대응하는 적합한 용어들이 존재하니 이 글을 읽는데 어려움없으시길 바라겠습니다. 😅 1.3. Inverted Index Elasticsearch는 특정 문장을 입력받으면, 파싱을 통해 문장을 단어 단위로 분리하여 저장합니다. 또한 대문자를 소문자로 치환하거나 유사어 체크 등의 추가 작업을 통해 텍스트를 저장합니다. Elasticsearch는 역 색인이라고 하는 자료 구조를 사용하는데, 이는 전문 검색에 있어서 빠른 성능을 보장합니다. 책의 전반부에 위치한 일반적인 목차가 Index라면, 책 후반부에 키워드마다 내용을 찾아볼 수 있도록 돕는 목차가 Reverted Index입니다. 역 색인은 각 Document에 등장하는 모든 고유한 단어들을 리스트업하고, 해당 단어들이 등장하는 Document들을 식별합니다. 색인은 최적화된 Document 컬렉션이며, 각 Document는 데이터를 포함하고 있는 Key-Value 쌍으로 이루어진 Field의 컬렉션입니다. Elasticsearch는 모든 Field의 데이터를 인덱싱하는데, 인덱싱된 Field는 각각의 최적화된 자료구조를 사용합니다. 텍스트 형식의 Field는 Inverted Index에 저장되며, 숫자 혹은 지리 관련 Field는 BKD 트리에 저장됩니다. RDB는 데이터 수정·삭제의 편의성과 속도 면에서 강점이 있지만 다양한 조건의 데이터를 검색하고 집계하는 데에는 구조적인 한계가 존재합니다. 특정 단어 검색시 ROW 개수만큼 확인을 반복하기 때문입니다. 반면 단어 기반으로 데이터를 저장하는 Elasticsearch는 특정 단어가 어디에 저장되어 있는지 이미 알고 있어 모든 Document를 검색할 필요가 없습니다. 반면 수정과 삭제는 내부적으로 굉장히 많은 리소스가 소요되는 작업이라, RDBMS를 대체하기 어렵습니다. 1.4. Architecture 하나의 클러스터 내 복수 개의 노드를 사용하는 경우, 저장된 Document는 클러스터 전역으로 분배되기 때문에 어느 노드에서든 즉시 접근이 가능합니다. Cluster 최소 하나 이상의 노드로 이루어진 노드들의 집합을 의미합니다. 서로 다른 클러스터는 데이터의 접근 및 교환을 할 수 없는 독립적인 시스템으로 유지됩니다. 여러 대의 서버가 하나의 클러스터를 구성하거나, 하나의 서버에 여러 개의 클러스터가 존재할 수 있습니다. Node Elasticsearch를 구성하는 하나의 단위 프로세스를 의미합니다. Shard 데이터를 분산해서 저장하는 방법을 의미합니다. Scale-Out을 위해 RDB의 Database에 해당하는 Index를 여러 Shard로 쪼갭니다. 기본적으로 1개가 존재하며, 검색 성능 향상을 위해 클러스터의 Shard 개수를 조정할 수 있습니다. Replica 또 다른 형태의 Shard를 의미합니다. 노드를 손실했을 경우, 데이터의 신뢰성을 위해 Shard를 복제하는 것입니다. 따라서 Replica는 서로 다른 노드에 위치시킬 것을 권장하고 있습니다. 추가적으로, Node는 다양한 역할로 분류할 수 있습니다. 대규모 클러스터에서 로드 밸런싱 역할을 하는 노드. 데이터 변환 등 사전 처리 파이프라인 노드. 색인된 데이터 CRUD 노드. 메타 데이터 등 전체 클러스터를 제어하는 마스터 노드. 인덱스 생성과 삭제. 데이터 입력시 샤딩 할당. 단일 노드로 Elasticsearch를 구동할 수 있지만, 트래픽이 많아진다면 노드별로 서버를 분리하거나 작업 노드에 대해 Scale-Out 및 로드 밸런싱을 함으로써 성능을 향상시킬 수 있습니다 1.5. 특징 Scale out : Shard를 통해 규모가 수평적으로 늘어날 수 있습니다. 고가용성 : Replica를 통해 데이터의 안정성을 보장하고, 단일 장애점을 극복합니다. Schema Free : Json 문서를 통해 데이터를 검색하므로, 스키마의 개념이 없습니다. RESTful : CRUD 작업은 RESTful API를 통해 수행되며, 각각이 HTTP의 PUT / GET / POST / DELETE 메서드에 대응됩니다. 2. Spring Data Elasticsearch Spring Data Elasticsearch 프로젝트는 Elasticsearch 검색 엔진을 사용하는 솔루션 개발을 도와주는 모듈입니다. 다음 작업에 대해 높은 수준의 추상화를 템플릿으로 제공하고 있습니다. Document의 저장과 검색 및 정렬. Document를 Aggregate로 재구성. Spring Data JPA가 Repository 인터페이스에 정의한 메서드 이름을 분석해서 JPQL을 자동으로 생성 및 실행해주는 것처럼, Spring Data Elasticsearch 또한 Repository 인터페이스에 메서드를 정의함으로써 쿼리를 표현할 수 있습니다. 3. Docker를 활용한 Elasticsearch 설치 Shell $ docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.0
$ docker run -d -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" docker.elastic.co/elasticsearch/elasticsearch:7.10.0
다양한 설치 방법이 있지만 그 중 가장 간편한 Docker를 사용했습니다. 해당 예제는 단일 노드로 클러스터를 구성하는데, 클러스터를 멀티 노드로 구성하고 싶다면 docker-compose를 활용하시면 됩니다. 9200 포트는 HTTP 클라이언트와 통신에 사용되며, 9300 포트는 노드들간 통신할 때 사용됩니다. 최신 공식 문서 기준 Spring Data Elasticsearch 4.2.x 버전이 Elasticsearch 7.10.0. 버전과 호환되기 때문에 7.10.0 버전으로 이미지를 받습니다. 다른 버전을 사용하고 싶다면 Srping Data Elasticsearch 프로젝트 버전 및 Elasticsearch 버전 호환 유무를 꼭 확인하도록 합시다. 4. Spring Boot 설정 build.gradle dependencies {
implementation 'org.springframework.boot:spring-boot-starter-data-JPA'
implementation 'org.springframework.boot:spring-boot-starter-web'
implementation 'org.springframework.data:spring-data-elasticsearch:4.2.2'
compileOnly 'org.projectlombok:lombok'
runtimeOnly 'com.h2database:h2'
annotationProcessor 'org.projectlombok:lombok'
testImplementation 'org.springframework.boot:spring-boot-starter-test'
}
현재 사용 중인 Elasticsearch 버전과 호환되게 Spring Data Elasticsearch 4.2.x 버전으로 의존성을 받습니다. 4.1. Elasticsearch Clients 설정 ElasticsearchOperations.java public interface ElasticsearchOperations extends DocumentOperations, SearchOperations {
//...
}
Elasticsearch 관련 작업을 수행할 때 주로 ElasticsearchOperations 인터페이스의 구현체를 사용합니다. 해당 인터페이스는 DocumentOperations 및 SearchOperations 인터페이스를 확장하고 있습니다. AbstractElasticsearchConfiguration.java public abstract class AbstractElasticsearchConfiguration extends ElasticsearchConfigurationSupport {

@Bean
public abstract RestHighLevelClient elasticsearchClient();

@Bean(name = { ""elasticsearchOperations"", ""elasticsearchTemplate"" })
public ElasticsearchOperations elasticsearchOperations(ElasticsearchConverter elasticsearchConverter,
RestHighLevelClient elasticsearchClient) {

ElasticsearchRestTemplate template = new ElasticsearchRestTemplate(elasticsearchClient, elasticsearchConverter);
template.setRefreshPolicy(refreshPolicy());

return template;
}
}
AbstractElasticsearchConfiguration 클래스가 ElasticsearchOperations을 Bean으로 등록하고 있습니다. 구현체는 ElasticsearchRestTemplate이네요. 해당 부분을 개발자가 커스텀하게 Bean 등록을 해주면 됩니다. ElasticSearchConfig.java @Configuration
public class ElasticSearchConfig extends AbstractElasticsearchConfiguration {

@Override
public RestHighLevelClient elasticsearchClient() {
ClientConfiguration clientConfiguration = ClientConfiguration.builder()
.connectedTo(""localhost:9200"")
.build();
return RestClients.create(clientConfiguration).rest();
}
}
Spring Data Elasticsearch는 ElasticsearchClient를 통해 Elasticsearch 노드 혹은 클러스터와 연결됩니다. ElasticsearchClient를 직접 사용할 수 있지만, 대게 더 추상화된 ElasticsearchOperations 혹은 ElasticsearchRepository를 사용합니다. ElasticsearchClient 구현체는 리액티브 지원 여부 등에 따라 종류가 다양한데, RestHighLevelClient가 일반적입니다. ElasticsearchOperations 구현체가 사용하는 RestHighLevelClient만 Bean으로 등록해주면 설정이 완료되었습니다. 4.2. Logging application.properties logging.level.org.springframework.data.elasticsearch.client.WIRE=TRACE
Spring Data Elasticsearch가 제대로 동작하는지 확인하고 싶다면, 위 로거를 등록합니다. 5. 예제 코드 User.java @Document(indexName = ""users"")
@Entity
public class User {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@Embedded
private BasicProfile basicProfile;

protected User() {
}

public User(String name) {
this(name, null);
}

public User(String name, String description) {
this(null, new BasicProfile(name, description));
}

@PersistenceConstructor
public User(Long id, BasicProfile basicProfile) {
this.id = id;
this.basicProfile = basicProfile;
}

// getter 생략
}
BasicProfile.java @Embeddable
public class BasicProfile {

@Column(nullable = false, unique = true)
private String name;

private String description;

protected BasicProfile() {
}

public BasicProfile(String name, String description) {
this.name = name;
this.description = description;
}

// getter 생략
}
과거의 Spring Data Elasticsearch는 Jackson 기반으로 데이터를 직렬화해 JSON으로 맵핑했는데, 현재는 Meta Object Mapping 방식을 사용합니다. JPA에서 @Entity 애너테이션을 통해 특정 클래스가 RDB에 저장될 테이블임을 명시한 것 처럼, @Document 애너테이션으로 해당 클래스가 ES DB에 맵핑될 클래스임을 표기합니다. 또한 @PersistenceConstructor가 부착된 생성자통해 ES DB에 저장된 Document가 Aggregate로 재구성됩니다. 그 외 기타 애너테이션 및 설정들은 공식 문서를 확인하길 바랍니다. ElasticsearchRepository.java @NoRepositoryBean
public interface ElasticsearchRepository<T, ID> extends PagingAndSortingRepository<T, ID> {

Page<T> searchSimilar(T entity, @Nullable String[] fields, Pageable pageable);
}
Spring Data JPA에서 사용자 정의 Repository 인터페이스를 정의할 때 JpaRepository 인터페이스를 확장한 것처럼, ElasticsearchRepository 인터페이스를 확장해 정의하면 됩니다. UserRepository.java public interface UserSearchRepository extends ElasticsearchRepository<User, Long>, CustomUserSearchRepository {

List<User> findByBasicProfile_NameContains(String name);
}
Spring Data JPA처럼 메서드 이름을 기반으로 CRUD 명령 쿼리를 생성할 수 있습니다. ElasticSearchConfig.java @EnableElasticsearchRepositories
@Configuration
public class ElasticSearchConfig extends AbstractElasticsearchConfiguration {
// ...
}
@EnableElasticsearchRepositories 애너테이션을 부착해줍니다. CustomUserSearchRepositoryImpl.java @RequiredArgsConstructor
@Component
public class CustomUserSearchRepositoryImpl implements CustomUserSearchRepository {

private final ElasticsearchOperations elasticsearchOperations;

@Override
public List<User> searchByName(String name, Pageable pageable) {
Criteria criteria = Criteria.where(""basicProfile.name"").contains(name);
Query query = new CriteriaQuery(criteria).setPageable(pageable);
SearchHits<User> search = elasticsearchOperations.search(query, User.class);
return search.stream()
.map(SearchHit::getContent)
.collect(Collectors.toList());
}
}
복잡한 쿼리를 직접 다뤄야 한다면 ElasticsearchOperations Bean을 주입받아 커스텀하게 쿼리를 작성할 수 있습니다. UserController.java @RequiredArgsConstructor
@RequestMapping(""/api"")
@RestController
public class UserController {

private final UserService userService;

@PostMapping(""/users"")
public ResponseEntity<Void> save(@RequestBody UserRequest userRequest) {
UserRequestDto userRequestDto = new UserRequestDto(
userRequest.getName(),
userRequest.getDescription()
);
Long id = userService.save(userRequestDto);
URI uri = URI.create(String.valueOf(id));
return ResponseEntity.created(uri)
.build();
}

@GetMapping(""/users/{name}"")
public ResponseEntity<List<UserResponse>> search(@PathVariable String name, Pageable pageable) {
List<UserResponse> userResponses = userService.searchByName(name, pageable)
.stream()
.map(UserResponse::from)
.collect(Collectors.toList());
return ResponseEntity.ok(userResponses);
}
}
UserService.java @Slf4j
@RequiredArgsConstructor
@Transactional(readOnly = true)
@Service
public class UserService {

private final UserRepository userRepository;
private final UserSearchRepository userSearchRepository;

@Transactional
public Long save(UserRequestDto userRequestDto) {
User user = new User(userRequestDto.getName(), userRequestDto.getDescription());
User savedUser = userRepository.save(user);
userSearchRepository.save(user);
return savedUser.getId();
}

public List<UserResponseDto> searchByName(String name, Pageable pageable) {
// userSearchRepository.findByBasicProfile_NameContains(name) 가능
return userSearchRepository.searchByName(name, pageable)
.stream()
.map(UserResponseDto::from)
.collect(Collectors.toList());
}
}
간단한 User 저장 및 검색 예제 코드를 작성했습니다. 자세한 코드는 글 상단에 위치한 실습 Repository 링크에서 확인 가능합니다. 5.1. 트러블 슈팅 Log The bean 'userRepository', defined in com.example.elasticsearch.user.domain.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration, could not be registered.
A bean with that name has already been defined in com.example.elasticsearch.user.domain.UserRepository defined in @EnableElasticsearchRepositories declared on ElasticsearchRepositoriesRegistrar.EnableElasticsearchRepositoriesConfiguration and overriding is disabled.

Action:

Consider renaming one of the beans or enabling overriding by setting spring.main.allow-bean-definition-overriding=true
Spring Data Elasticsearch 단일 모듈을 사용할 때는 문제가 없는데, 예제 코드처럼 Spring Data JPA와 함께 사용하는 경우 ApplicationContext 로드에 실패합니다. User 클래스를 위해 생성한 JPA용 Repository와 Elasticsearch용 Repository 인터페이스 모두 PagingAndSortingRepository<T, ID>를 확장하고 있습니다. 그 결과 @EnableAutoConfiguration을 기반으로 Bean을 주입하는 과정에서 Bean 중복 문제가 발생합니다. application.properties spring.main.allow-bean-definition-overriding=true
Bean을 오버라이딩할 수 있도록 상기 옵션을 추가해줍니다. 그럼에도 불구하고 다시 실행해보면 에러가 발생하는데요. Trouble Annotations @EnableJpaRepositories
@EnableElasticsearchRepositories
@EnableJpaRepositories 및 @EnableElasticsearchRepositories 등 두 애너테이션은 기본적으로 Repository 관련 클래스를 모두 스캐닝하려고 시도합니다. Log UnsupportedFragmentException: Repository com.example.elasticsearch.user.domain.UserRepository implements org.springframework.data.repository.query.QueryByExampleExecutor but ElasticsearchRepositoryFactory does not support Query by Example!
@EnableElasticsearchRepositories 애너테이션이 JpaRepository 인터페이스를 확장한 JPA 관련 클래스를 스캐닝하면 위와 같은 에러가 발생합니다. Log Caused by: org.springframework.data.mapping.PropertyReferenceException: No property searchSimilar found for type User!

혹은

Caused by: org.springframework.data.mapping.PropertyReferenceException: No property index found for type User!
반대로 @EnableJpaRepositories 애너테이션이 ElasticsearchRepository 인터페이스를 확장한 Elasticsearch 관련 클래스를 스캐닝하면 위와 같은 에러가 발생합니다. ElasticSearchConfig.java @EnableElasticsearchRepositories(basePackageClasses = UserSearchRepository.class)
@Configuration
public class ElasticSearchConfig extends AbstractElasticsearchConfiguration {

//...
}
ElasticsearchApplication.java @EnableJpaRepositories(excludeFilters = @ComponentScan.Filter(
type = FilterType.ASSIGNABLE_TYPE,
classes = UserSearchRepository.class))
@SpringBootApplication
public class ElasticsearchApplication {

public static void main(String[] args) {
SpringApplication.run(ElasticsearchApplication.class, args);
}

}
따라서 @EnableElasticsearchRepositories 애너테이션은 Elasticsearch 관련 Repository 클래스만 스캐닝하도록 합니다. 반대로 @EnableJpaRepositories 애너테이션은 JPA 관련 Repository 클래스만 스캐닝하도록 합니다. @EnableXXX 관련 애너테이션을 잘 설정했다면 필요없는 spring.main.allow-bean-definition-overriding=true 옵션은 제거하셔도 됩니다. 5.2. 동작 확인 어플리케이션을 실행해보고 Elasticsearch 서버로의 데이터 쓰기 및 조회가 원활하게 동작하는지 확인해봅시다. RDB는 SQL 쿼리를 통해 CRUD 작업을 수행한다면, Elasticsearch는 RESTful API 쿼리를 통해 CRUD 작업을 수행합니다. Spring Data JPA가 자동으로 SQL 쿼리를 생성해 DB 서버로 요청을 보내는 것처럼, Spring Data Elasticsearch 또한 RESTful API 쿼리를 생성해 ES DB 서버로 요청을 보냅니다. jinhong이라는 문자열이 포함된 모든 User 검색에 성공했습니다. RESTful한 GET 요청을 통해 ES DB에 저장된 Document를 조회한 모습입니다. COUNT(*) SQL 쿼리처럼 몇 명의 유저가 저장되어있는지 카운팅할 수 있습니다. Query POST /users/_delete_by_query

{
""query"": {
""range"": {
""id"": {
""gte"" : 1
}
}
}
}
User ID가 1보다 크거나 같은 경우 삭제하겠다는 쿼리를 요청 본문에 담아 삭제를 진행할 수 있습니다. SQL 쿼리가 다양한 것처럼 Elasticsearch 또한 다양한 RESTful API 쿼리가 존재하니, 자세한 내용은 공식 문서를 참고해주세요. 6. Trade-Off 행 기반으로 데이터를 저장하는 RDB에서 특정 키워드가 포함된 데이터를 조회할 때 LIKE 쿼리를 주로 사용합니다. 문제는 LIKE '%jinhong%'과 같이 jinhong 키워드가 들어간 데이터를 조회할 때, % 기호를 앞에 사용하는 경우 조회 성능 향상을 위한 인덱스 사용이 불가능합니다. 2대의 조회 Slave DB(Replication)를 사용하는 웹 어플리케이션에서 특정 이름 키워드가 포함된 유저 정보를 조회하는 부하 테스트를 진행한 결과입니다. 평균 TPS가 38.9입니다. 키워드 검색 요청 상황에서 RDB를 ES 검색 엔진으로 대체하면 성능 이점을 얻을 수 있습니다. ES를 도입한 다음 부하 테스트를 동일 환경에서 진행했습니다. ES의 경우 단 1대의 서버만으로도 평균 TPS가 82.2를 기록했습니다! 이는 2대의 서버를 사용하는 기존의 RDB 방식보다 성능이 2배 이상을 상회합니다. 물론 Elasticsearch라는 별도의 DB를 사용함으로써 고려해야할 관리 포인트가 많아진다는 단점이 존재합니다. 기존 RDB에 저장된 데이터를 ES로 이관해야 합니다. 업데이트 발생시 RDB와의 데이터 정합성을 제대로 유지해야합니다. ES는 루씬 쓰기 성능이 나쁜 만큼, 업데이트 방식에 대한 추가적인 의사결정이 필요합니다. RDB - ES 간의 데이터 싱크를 실시간으로 맞출 것인지, 혹은 배치 방식을 사용할 것인지 등. 실시간으로 데이터 싱크를 맞추면 ES 서버 트래픽이 증가하고 쓰기 작업을 수행하느라 많은 리소스를 사용하게 됩니다. Bulk API 기반의 배치 방식으로 데이터 싱크를 맞추면 Document의 반복 업데이트를 줄일 수 있으나, 데이터 싱크에 지연이 발생합니다. 고가용성을 위한 단일 장애점 극복을 위해 ES 클러스터링 및 샤딩 등 아키텍쳐에 대한 고려가 필요합니다. References Elasticsearch Guide Docs Spring Data Elasticsearch - Reference Documentation ELK Stack을 이용한 로그 관제 시스템 만들기 🙈[Elasticsearch] 기본 개념잡기🐵 No property index found for type User 엘라스틱서치(Elasticsearch)에서 관계형 데이터 모델링하기 Twitter Facebook Google+ # 백엔드 # ElasticSearch",BE
454,"내장 Redis 설정기 2021, Oct 19 안녕하세요 :) 마크입니다! 목차 목차 들어가며 테스트 환경 구축 의존성 테스트 도메인 생성 내장 Redis port 수동 설정 내장 Redis port 자동 설정 사용시 주의할 점 - 중요 메모리 확인 netstat 확인 M1 마치며 참고 들어가며 토큰과 캐싱 개발 초기엔 내장 Redis가 존재하는지 모르고, RedisRepository 역할을 하는 객체를 직접 자바 코드로 구현하여 테스트를 진행했다. 즉, Fake 객체를 만들어서 테스트하는 방법을 택했다. 이렇게 테스트 하는 것의 가장 큰 문제는 실제 Redis 환경이 아니란 점이다.. 그래서 Embedded H2처럼 Embedded Redis도 있다 찾아봤고, 적용시키는 시점에서 모르는 것이 많아 삽질을 통해 많은 것을 배울 수 있었고, 이참에 정리하게 되었다. 본 글은 이동욱님의 - [Redis] SpringBoot Data Redis 로컬/통합 테스트 환경 구축하기와 아라한사님께서 번역하신 스프링 데이터 레디스 를 참고하여 작성된 글입니다. 테스트 환경 구축 의존성 먼저 의존성을 주입해준다. implementation group: 'org.springframework.boot', name: 'spring-boot-starter-data-redis', version: '2.5.4'
implementation (group: 'it.ozimov', name: 'embedded-redis', version: '0.7.3') {
exclude group: 'org.slf4j', module: 'slf4j-simple'
}
Spring Data Redis Spring Data JPA처럼 Redis를 JPA Repository 사용하듯 인터페이스를 제공하는 모듈. embedded-redis 기존 it.ozimov.embedded-redis안에 slf4j-simple이 내장되어 있어서, 필자는 slf4j에 대한 충돌이 발생하였다. 그래서 이를 제외시켰다. 테스트 도메인 생성 테스트를 위한 도메인을 간단히 구현해준다. Post.java @AllArgsConstructor
@NoArgsConstructor
@Getter
@RedisHash(""post"")
public class Post {

@Id @Indexed
private Long id;
private String title;
private String content;
}
Repository도 간단히 구현해준다. PostRedisRepository public interface PostRedisRepository extends CrudRepository<Post, Long> {
}
이제 Redis를 연결하기 위한 Config파일을 생성해준다. (기존의 DB에 연결하기 위해서 사용했던 Datasource와 비슷한 역할을 한다.) RedisConfig @Configuration
@EnableRedisRepositories
public class RedisConfig {

@Value(""${spring.redis.host}"")
private String redisHost;

@Value(""${spring.redis.port}"")
private int redisPort;

@Bean
public RedisConnectionFactory redisConnectionFactory() {
return new LettuceConnectionFactory(redisHost, redisPort);
}

@Bean
public RedisTemplate<?, ?> redisTemplate() {
RedisTemplate<byte[], byte[]> redisTemplate = new RedisTemplate<>();
redisTemplate.setConnectionFactory(redisConnectionFactory());
return redisTemplate;
}
}
RedisConnectionFactory를 이용하여 내장 혹은 외부 Redis에 연결한다. RedisTemplate은 RedisConnection에서 넘겨준 byte 값을 객체 직렬화한다. 마지막으로 설정 파일을 추가해준다. application-test.yml spring:
redis:
host: localhost
port: 6379
test 환경에서만 실행할 것이기 때문에 위와 같이 해두었다. 만약 다른 환경에서도 필요하다면 profile 설정을 해주면 된다. 내장 Redis port 수동 설정 이제 Redis를 사용하기 위한 설정은 모두 되었다. 본격적으로 Embedded-Redis Server를 실행만 해주면 된다. 기본적으로 Embedded-Redis Server를 실행시키면 localhost(127.0.0.1)에 실행된다. 이제 Embedded-Redis Server 설정을 해준다. EmbeddedRedisConfig @Configuration
@Profile(""!prod"")
public class EmbeddedRedisConfig {

@Value(""${spring.redis.port}"")
private int redisPort;

private RedisServer redisServer;

@PostConstruct
public void redisServer() throws IOException {
redisServer = new RedisServer(redisPort);
redisServer.start();
}

@PreDestroy
public void stopRedis() {
if (redisServer != null && redisServer.isActive()) {
redisServer.stop();
}
}
}
설정 파일에서 받아온 port를 이용하여 내장 Redis 서버를 작동시킨다. 이때 RedisConfig에서 바라보는 host, port랑 내장 Redis 서버가 켜진 port랑 다르면 에러가 발생한다. 필자는 이 부분 때문에 삽질을 굉장히 오래했다. 이런xx 빈 스코프 @PostConstruct: 객체의 초기화 부분 -> 객체가 생성된 후 별도의 초기화 작업을 위해 실행하는 메서드를 선언한다. @PreDestroy: 마지막 소멸 단계 -> 빈 컨테이너에서 빈을 제거하기 전에 해야할 작업을 이곳에 정의한다. 간단히 테스트 코드를 작성하고 테스트해본다. PostRedisRepositoryTest @Import({EmbeddedRedisConfig.class}) // @DataRedisTest를 사용하면 Config를 읽지 못하므로 추가
@DataRedisTest
@DirtiesContext(classMode = ClassMode.BEFORE_EACH_TEST_METHOD) // 반복 테스트할 때 매번 새로운 컨텍스트를 띄우기 위함
@ActiveProfiles(""test"")
class PostRedisRepositoryTest {

@Autowired
private PostRedisRepository postRedisRepository;

@AfterEach
void tearDown() {
postRedisRepository.deleteAll();
}

@RepeatedTest(value = 20)
void save_and_find() {
// given
Post post = new Post(1L, ""post"", ""post content"");

// when
postRedisRepository.save(post);

// then
Post findPost = postRedisRepository.findById(post.getId())
.orElseThrow();
assertThat(post.getId()).isEqualTo(findPost.getId());
assertThat(post.getTitle()).isEqualTo(findPost.getTitle());
assertThat(post.getContent()).isEqualTo(findPost.getContent());
}

@RepeatedTest(value = 20)
void update_and_find() {
// given
Post post = new Post(1L, ""post"", ""post content"");
postRedisRepository.save(post);

// when
Post savedPost = postRedisRepository.findById(1L)
.orElseThrow();
savedPost.setTitle(""updated post"");
postRedisRepository.save(savedPost);

// then
Post updatedPost = postRedisRepository.findById(1L)
.orElseThrow();
assertThat(updatedPost.getTitle()).isEqualTo(""updated post"");
}
}
20번을 돌려도 문제없이 조회, 수정등의 테스트가 통과하는 것을 볼 수 있다. 내장 Redis port 자동 설정 자동으로 port를 찾아 설정해준다는 의미로 동적이라고 하였습니다! 위에서 학습 테스트한 방식의 문제점은 통합 테스트시 여러 내장 Redis를 작동시킬 일이 있으면 port가 충돌한다는 것이다. 그러므로, 이미 사용중인 port라면 다른 port를 사용할 필요가 있다. 그렇다면 특정 port가 사용중인지 아닌지 확인하는 방법은 무엇일까? 정답은 쉘을 사용하여 netstat명령어를 실행시켜 확인하는 것이다. netstat -nat | grep LISTEN | grep {port} 자바에선 현재 실행중인 환경과의 인터페이스인 Runtime 객체를 제공한다. ShellTest public class ShellTest {

@Test
void shellTest() throws IOException {
// port
int port = 22;

// netstat
String command = String.format(""netstat -nat | grep LISTEN | grep %d"", port);
String[] shell = {""/bin/sh"", ""-c"", command};
Process process = Runtime.getRuntime().exec(shell);

// InputStream을 통해 읽기
String line;
StringBuilder pidInfo = new StringBuilder();
try (BufferedReader input = new BufferedReader(new InputStreamReader(process.getInputStream()))) {
while ((line = input.readLine()) != null) {
pidInfo.append(line);
}
}

System.out.println(pidInfo.toString());
}
}
위 코드를 실행해서 만약 연결된 port가 있다면 아래와 같이 pidInfo가 뜬다. 반면에, 아무런 port도 연결되어 있지 않다면 아무 문자열로 뜨지 않는다. (isEmpty()) 이를 사용해서 port를 찾고 연결하는 과정을 자동화 시키면 된다. 수정된 EmbeddedRedisConfig @Configuration
@Profile(""!prod"")
public class EmbeddedRedisConfig {

private static final String BIN_SH = ""/bin/sh"";
private static final String BIN_SH_OPTION = ""-c"";
private static final String COMMAND = ""netstat -nat | grep LISTEN|grep %d"";

@Value(""${spring.redis.port}"")
private int redisPort;

private RedisServer redisServer;

// 아래처럼 빈을 꼭 여기서 등록해줘야 한다.
@Bean
public RedisConnectionFactory redisConnectionFactory() {
return new LettuceConnectionFactory(""127.0.0.1"", redisPort);
}

@PostConstruct
public void redisServer() throws IOException {
int port = isRedisRunning() ? findAvailablePort() : redisPort;
redisServer = new RedisServer(port);
redisServer.start();
}

@PreDestroy
public void stopRedis() {
if (redisServer != null && redisServer.isActive()) {
redisServer.stop();
}
}

private boolean isRedisRunning() throws IOException {
return isRunning(executeGrepProcessCommand(redisPort));
}

private boolean isRunning(Process process) {
String line;
StringBuilder pidInfo = new StringBuilder();

try (BufferedReader input = new BufferedReader(new InputStreamReader(process.getInputStream()))) {

while ((line = input.readLine()) != null) {
pidInfo.append(line);
}

} catch (Exception e) {
}

return !pidInfo.toString().isEmpty();
}

private Process executeGrepProcessCommand(int port) throws IOException {
String command = String.format(COMMAND, port);
String[] shell = {BIN_SH, BIN_SH_OPTION, command};
return Runtime.getRuntime().exec(shell);
}

public int findAvailablePort() throws IOException {

for (int port = 10000; port <= 65535; port++) {
Process process = executeGrepProcessCommand(port);
if (!isRunning(process)) {
return port;
}
}

throw new IllegalArgumentException(""Not Found Available port: 10000 ~ 65535"");
}
}
어렵게 생각할 필요없다. 그저 기존의 설정 파일에 설정된 port가 이미 사용중이라면 10000부터 차례대로 port를 설정하여 가져오는 것이다. 여기서 가장 중요한 개념은 RedisConfig에서 바라보는 port와 내장 Redis 서버의 port가 같아야한다는 점이다! 그러므로, RedisConfig의 코드도 수정해줘야한다. RedisConfig @Configuration
@EnableRedisRepositories
public class RedisConfig {

@Value(""${spring.redis.host}"")
private String redisHost;

@Value(""${spring.redis.port}"")
private int redisPort;

@Bean
@ConditionalOnMissingBean(RedisConnectionFactory.class) // 수정된 부분!
public RedisConnectionFactory redisConnectionFactory() {
return new LettuceConnectionFactory(redisHost, redisPort);
}

@Bean
public RedisTemplate<?, ?> redisTemplate() {
RedisTemplate<byte[], byte[]> redisTemplate = new RedisTemplate<>();
redisTemplate.setConnectionFactory(redisConnectionFactory());
return redisTemplate;
}
}
그렇다면 어떻게 내장 Redis 서버를 사용하는 환경과 외장 Redis 서버를 사용하는 환경을 분리할까? 바로 @ConditionalOnMissingBean을 사용하는 것이다. 이 애노테이션은 기존에 같은 빈이 등록되어 있으면 해당 메서드의 빈은 등록하지 않는 기능을 제공한다. 마지막으로 기존의 테스트를 돌려보면 정상 통과한다. 이외에도 통합 테스트시 여러 대의 내장 Redis 서버를 켜도 문제 없이 실행된다. 사용시 주의할 점 - 중요 내장 Redis를 사용하며 겪었던 경험과 주의할 점을 이곳에 남겨둔다. (부들부들 😠) 메모리 확인 내장 Redis Server도 메모리에 올라간다. 그리고 테스트시 많은 내장 Redis Server를 키다보면, 메모리 부족이 발생할 수 있다. 이럴 경우 Redis 설정에서 메모리 설정을 해주면 된다. @PostConstruct
public void redisServer() throws IOException {
int redisPort = isRedisRunning() ? findAvailablePort() : port;
redisServer = new RedisServer(redisPort);
redisServer = RedisServer.builder()
.port(redisPort)
.setting(""maxmemory 128M"") //maxheap 128M
.build();
redisServer.start();
}
Redis에 저장되는 데이터의 용량이 128M가 넘어가면, 기존의 데이터를 덮어씌우는 방식으로 일정 메모리 이상 차지하지 못하게 한다. netstat 확인 만약 두번째 방법인 내장 Redis port 자동 설정를 사용한다면, 해당 코드가 어느 환경에서 도는지 확인해야한다. 필자는 프로젝트를 하며 내장 Redis를 구현하여 사용하던중, 젠킨스에서만 계속해서 예외가 발생했다. 4시간동안의 삽질 끝에 찾은 문제는 도커 컨테이너안에 netstat 명령어가 not found였기 때문이다… 해당 컨테이너안에 net-tools를 설치했더니 잘 동작한다.. 내장 Redis 서버를 사용하려면 우선 포트를 조심하고, 두 번째는 해당 프로그램이 돌아가는 환경에 net-tools가 존재하는지 확인하자. M1 M1 환경에서 Redis 내장 서버를 시동시키면 아래와 같은 메시지가 뜬다. Caused by: java.lang.RuntimeException: Can't start redis server. Check logs for details.
원인은 내장 Redis 라이브러리에 mac_arm64용 바이너리가 준비되어 있지 않아서 그렇다. 우선 Redis 소스 코드를 다운받아 컴파일하면, Redis Server 컴파일 파일이 생긴다. 해당 파일을 프로젝트의 resources 디렉터리에 복사하고, ClassPathResource 로 불러오시면 된다. public void redisServer() throws IOException, URISyntaxException {
int redisPort = isRedisRunning() ? findAvailablePort() : port;
redisServer = new RedisServer(redisPort);

if (isArmMac()) {
redisServer = new RedisServer(Objects.requireNonNull(getRedisFileForArcMac()),
redisPort);
}
if (!isArmMac()) {
redisServer = new RedisServer(redisPort);
}

redisServer.start();
}

private boolean isArmMac() {
return Objects.equals(System.getProperty(""os.arch""), ""aarch64"") &&
Objects.equals(System.getProperty(""os.name""), ""Mac OS X"");
}

private File getRedisFileForArcMac() {
try {
return new ClassPathResource(""binary/redis/redis-server-6.2.5-mac-arm64"").getFile();
} catch (Exception e) {
throw new EmbeddedRedisServerException();
}
}
M1 Embedded Redis 문제 해결 PR - by 다니에서 전체 코드를 확인할 수 있다. 마치며 처음엔 내장 Redis Server를 어떻게 구축해야하나 막막했었다… 그래도 이동욱님의 글를 보고 많은 도움을 받았다. 다만 생각보단 추상적인 부분이 많아 삽질을 많이 하게되었다. 실제 프로젝트에 적용시켰을 때 많은 문제가 야기되었고, 이번 글을 쓰게 된 계기가 되었다. 쨋든! 굉장히 좋은 지식을 얻어간다. 현재는 토큰과 캐시 데이터를 저장하는 용도로만 Redis를 사용하지만, 메시지 큐와 같은 기능을 사용할 때 Redis를 사용하기에 내장 Redis 서버가 유용하게 사용될 듯하다. 참고 https://jojoldu.tistory.com/297 https://www.baeldung.com/spring-embedded-redis http://arahansa.github.io/docs_spring/redis.html https://velog.io/@hakjong/ARM-Mac-M1-%EC%97%90%EC%84%9C-embedded-redis-%EC%82%AC%EC%9A%A9 Twitter Facebook Google+ # 백엔드 # Redis # Embedded",BE
455,"Spring Boot와 Redis 기반의 캐싱 - 적용기 2021, Oct 19 안녕하세요! 마크입니다 :) 목차 목차 들어가며 캐시 전략 Cache란? Look Aside Cache 스프링 캐시 코어 스프링은 어떻게 캐싱을 처리하지? Cache와 CacheManager 결국 핵심은 두 가지다 프로젝트에 캐싱 구현하기 Redis 설정 캐시 설정 캐시 적용 결과 다음 편은.. 들어가며 최근 깃-들다는 성능 진단을 통해 성능 개선 작업을 수행중이다. 그 과정에서 비로그인 홈피드 조회와 같이 단순하고 빈번한 동일 요청이 반복되는 요청이 많다는 것을 발견했다. 그리고 이러한 요청은 쿼리가 비교적 복잡하며, 쿼리가 많이 날라간다. 이번 캐싱 시리즈는 이러한 요청을 캐싱 기능을 통해 해결하는 과정을 담았다. 이번 시리즈는 총 두 편으로 구성된다. Spring Boot와 Redis 기반의 캐싱 - 적용기 Spring Boot와 Redis 기반의 캐싱 - 성능 개선 이번 글은 시리즈중 첫번째인 Spring Boot와 Redis 기반의 캐싱 적용기이다. 캐시 전략 이번 챕터에선 캐시란 무엇이며, 캐싱 적용에 어떤 전략을 사용했는지 서술한다. Cache란? Cache는 나중에 요청을 결과를 미리 저장해두었다가 빠르게 서비스를 해주는 것을 의미 ex. Factorial -> 10! = 10 * 9! (9!를 캐싱해두면 연산이 훨씬 빨라짐) ex. DP 운영체제도 CPU가 디스크에 접근하는 속도가 느리므로, 중간에 캐시를 많이 둬서 이를 빠르게 사용하도록 한다. 파레토 법칙 20:80 법칙으로, 우리 사회에서 일어나는 현상의 80%는 20%의 원인으로 인하여 발생한다. (캐싱을 잘 설명해주는 유비쿼터스) 즉, 전체 요청의 80%는 20%의 사용자이다. (캐싱을 사용해야 하는 이유) Look Aside Cache 캐시에도 다양한 전략이 존재한다. 대표적으론 Look Aside Cache와 Write Back이 존재한다. Look Aside Cache: 캐시 저장소에 데이터가 있으면, 캐시에서 가져오고, 없으면 메인 DB에서 값을 가져오는 대표적인 캐시 전략. Wrtie Back: 캐시 저장소에 특정 시간동안 데이터를 모아서, 배치 처리를 통해 메인 DB에 저장하는 전략. 현재 시도하려는 성능 개선은 쓰기 보다는 조회 성능을 개선하려고 하기 때문에, Look Aside Cache가 적합하다고 판단하였다. Look Aside Cache를 조금 더 자세히 살펴보자면 아래와 같다. Web Server는 데이터가 존재하는지 Cache를 먼저 확인한다. Cache에 데이터가 있으면 Cache에서 값을 가져온다. Cache에 데이터가 없다면 DB에서 데이터를 가져와서 Cache에 저장하고 값을 가져온다. Write하는 방식 방법1. 애플리케이션이 새로운 데이터 쓰기 혹은 업데이트할 때 캐시와 DB 모두에 같은 작업을 실행하는 방법. 방법2. 애플리케이션의 모든 쓰기 작업은 DB에만 적용되고, 기존의 캐시 데이터를 무효화시키는 방법. 스프링 캐시 코어 이제 본격적으로 스프링과 Redis를 이용하여 캐시를 적용해보려고 한다. 이번 챕터에선 스프링 진영에선 캐시 기능을 어떻게 제공하는지 살펴본다. 스프링은 어떻게 캐싱을 처리하지? 핵심을 말하자면, 스프링은 캐싱을 적용하는 코드를 추상화하여 자바 메서드에 적용시킬 수 있도록 하였다. 예를 들어, MySQL과 같은 DB로부터 조회 쿼리를 실행하는 애플리케이션 메서드가 존재한다고 가정해보자. 그럼 해당 메서드에 캐싱 애노테이션을 붙이면 AOP를 통해, 캐시 처리를 하는 것이다. 이때 만약 캐시 DB (ex. Redis)에 조회하고자하는 데이터가 있다면 해당 타겟 메서드의 로직 (실제 DB에 조회 쿼리)은 실행되지 않는다. 캐시해둔 결과를 Proxy에서 반환하기 때문이다. 물론, 캐시 DB에 조회하고자하는 데이터가 없다면, 실제 DB에 조회 쿼리를 날려 데이터가 가져와서 캐시 DB에 저장후 데이터를 반환한다. 위와 같이 스프링 캐시 코어는 Look aside Cache을 디폴트로 지원한다. 정말 편한 것은 캐싱 애노테이션만 붙이면 위와 같이 캐싱 전략은 AOP를 통해 자동으로 적용된다는 것이다. 개발자는 그저 핵심 로직 (실제 DB로부터 데이터 조회)만을 작성하면 된다. 물론 캐싱 DB의 데이터를 실제 DB와 동기화시키기 위한 기능도 제공한다. 즉, 캐싱 DB에 저장되는 데이터를 업데이트하거나 삭제하는 기능도 제공한다. 이는 데이터의 변경이 발생했을 때 굉장히 유용하게 사용되며, 이 또한 애노테이션을 붙이면 쉽게 적용시킬 수 있다. 쉽게 얘기해서 get-if-not-found-then-proceed-and-put-eventually code blocks라고 이해하면 쉽다. 스프링은 프레임워크답게 개발자들이 핵심 로직만 작성하면 쉽게 부가적인 기능 (ex. 트랜잭션, 캐싱)등을 적용시킬 수 있도록 추상화시켜두었다. 다시 한번, 스프링을 사용하면서 관심사의 분리, 객체지향 원칙에 대한 이해를 높일 수 있었다. Cache와 CacheManager 스프링이 캐싱 기능을 잘 추상화시켰고, 그저 @EnableCaching이라는 애노테이션을 선언만해주면, 캐시 관련된 애노테이션을 스캔하여 캐싱 로직을 수행한다. 사실 가장 중요한 것은 캐시 데이터를 어디에 저장하느냐이다. 다시 말해, 스프링은 캐시 관련된 로직을 제공하지만, 캐시 데이터를 어디에 저장할지는 개발자가 지정해줘야한다. 그리고 이 부분을 스프링은 아래 두 가지 인터페이스를 이용해 추상화해두었다. org.springframework.cache.Cache: 캐시 데이터에 대한 생명주기를 관리하는 객체. org.springframework.cache.CacheManager: 캐시를 관리해주는 매니저. Cache 객체를 관리한다. 더 자세한 아키텍처 부분은 여기를 참고하자. 스프링은 아래와 같은 캐시 DB의 대한 구현체를 제공한다. ConcurrentMapCacheManager: Java의ConcurrentHashMap을 사용해 구현한 캐시를 사용하는 캐시매니저 SimpleCacheManager: 기본적으로 제공하는 캐시가 없어 사용할 캐시를 직접 등록하여 사용하기 위한 캐시매니저 EhCacheCacheManager: 자바에서 유명한 캐시 프레임워크 중 하나인 EhCache를 지원하는 캐시 매니저 CompositeCacheManager: 1개 이상의 캐시 매니저를 사용하도록 지원해주는 혼합 캐시 매니저 CaffeineCacheManager: Java 8로 Guava 캐시를 재작성한 Caffeine 캐시를 사용하는 캐시 매니저 JCacheCacheManager: JSR-107 기반의 캐시를 사용하는 캐시 매니저 RedisCacheManager: 캐시 저장소로 유명한 Redis를 지원하는 캐시 매니저. 프로젝트에는 Redis 환경이 구축되어 있으므로, RedisCacheManager를 이용하여 구현하였다. 결국 핵심은 두 가지다 스프링에서 캐싱처리를 적용시키기위해 신경써야할 부분은 사실 딱 두가지이다. Caching declaration: Identify the methods that need to be cached and their policy. 애노테이션을 통해 어떤 메서드가 캐싱이 필요한지, 또한 애노테이션을 통해 어떤 정책을 사용할지 설정해주면된다. Cache Configuration: The backing cache where the data is stored and from which it is read. 캐시한 데이터를 어디에 저장하고 읽어올지 설정해주면 된다. 프로젝트에 캐싱 구현하기 이제 캐시의 개념과 전력, 그리고 스프링에서 어떻게 캐싱을 구현하려는지 알았으니 프로젝트에 적용시킨 과정을 소개한다. Redis 설정 일반적인 스프링에서의 Redis 설정이다. @EnableRedisRepositories
@Configuration
public class RedisConfiguration {

private final String host;
private final String password;
private final int port;

public RedisConfiguration(
@Value(""${security.redis.host}"") String host,
@Value(""${security.redis.password}"") String password,
@Value(""${security.redis.port}"") int port
) {
this.host = host;
this.password = password;
this.port = port;
}

@Bean
@ConditionalOnMissingBean(RedisConnectionFactory.class)
public RedisConnectionFactory redisConnectionFactory() {
RedisStandaloneConfiguration configuration =
new RedisStandaloneConfiguration(host, port);
configuration.setPassword(password);
return new LettuceConnectionFactory(configuration);
}

@Bean
public RedisTemplate<?, ?> redisTemplate() {
RedisTemplate<byte[], byte[]> redisTemplate = new RedisTemplate<>();
redisTemplate.setConnectionFactory(redisConnectionFactory());
return redisTemplate;
}
}
캐시 설정 RedisCacheConfiguration @EnableCaching
@Configuration
@Profile(""!test"")
public class RedisCachingConfiguration {

private final RedisConnectionFactory redisConnectionFactory;
private final ObjectMapper objectMapper;

public RedisCachingConfiguration(
RedisConnectionFactory redisConnectionFactory,
ObjectMapper objectMapper
) {
this.redisConnectionFactory = redisConnectionFactory;
this.objectMapper = objectMapper;
}

public CacheManager redisCacheManager() {
CollectionType collectionType = objectMapper.getTypeFactory()
.constructCollectionType(ArrayList.class, PostResponseDto.class);

RedisCacheConfiguration redisCachingConfiguration = RedisCacheConfiguration
.defaultCacheConfig()
.serializeKeysWith(
RedisSerializationContext.SerializationPair.fromSerializer(
new StringRedisSerializer()
)
)
.serializeValuesWith(
RedisSerializationContext.SerializationPair.fromSerializer(
new Jackson2JsonRedisSerializer<>(
collectionType
)
)
)
.entryTtl(Duration.ofMinutes(30));

return RedisCacheManager
.RedisCacheManagerBuilder
.fromConnectionFactory(redisConnectionFactory)
.cacheDefaults(redisCachingConfiguration)
.build();
}
}
RedisConnectionFactory : 캐시 데이터 저장소로 Redis를 결정하였으므로, Redis 서버에 접속하기 위한 ConnectionFactory가 필요하다. RedisConnectionFactory와 관련된 더 자세한 내용은 여기를 참고하면 된다. CacheManager : 어떤 Cache 저장소에 데이터를 저장할지 설정해주는 객체. 생각보다 별거 없다. 하지만.. 위 설정에서 삽질한 부분이 존재한다. Redis는 보통 외부에 저장소를 따로 위치해 사용하며, 네트워크 통신을 위해 byte array (바이트 배열)로 변환해야 한다. (OuputStream으로 내보내기 위함인 듯 하다.) 또한, WAS 관점에서 Redis에 저장된 데이터도 단지 byte일 뿐이다. 그러기 때문에 불러올 때도 바이트 배열을 객체로 변환해줘야한다. 쉽게 말해, 객체와 JSON 간의 직렬화/역직렬화 설정을 해주어야한다. 비로그인 홈피드 조회의 경우, Response DTO가 생각보다 복잡하여 이를 처리하는데 삽집을 했다. 날짜 형식 직렬화/역직렬화 문제: Spring Cache에 호환되는 Jackson2Json의 경우 LocalDateTime을 직렬화/역직렬화하지 못하는 이슈가 발생했다. 해결: 여러가지 방법을 찾아 보았고, @JsonSerialize를 사용하여 해결하였다. 내부 DTO와 리스트를 직렬화할 때 참조값을 직렬화하는 문제 문제: 아무 설정해주지 않고 Response DTO를 직렬화하면, DTO안의 값이 아닌 참조 값이 직렬화되는 문제 해결: Jackson2Json에서 제공하는 CollectionType를 커스텀하여 설정. 캐시 적용 캐시 적용은 어렵지 않다. 그저 캐시를 적용하고 싶은 메서드에 상황에 맞는 애노테이션을 붙여주면 된다. @Cacheable: Triggers cache population. @CacheEvict: Triggers cache eviction. @CachePut: Updates the cache without interfering with the method execution. @Caching: Regroups multiple cache operations to be applied on a method. @CacheConfig: Shares some common cache-related settings at class-level. 학습 테스트를 통해 정리한 글은 여기를 참고하면 된다. 이제 본격적으로 프로젝트에 적용해본다. 우선 조회 캐싱 처리다. PostFeedService @Cacheable(
key = ""#homeFeedRequestDto.page"",
value = ""homeFeed"",
condition = ""#homeFeedRequestDto.guest == true"",
unless = ""#result == null || #result.empty""
)
public List<PostResponseDto> homeFeed(HomeFeedRequestDto homeFeedRequestDto) {
Pageable pageable = getPagination(homeFeedRequestDto);
if (homeFeedRequestDto.isGuest()) {
return PostDtoAssembler.assembleFrom(null, postRepository.findAllPosts(pageable));
}
User requestUser = findUserByName(homeFeedRequestDto.getRequestUserName());
List<Post> result = postRepository.findAllAssociatedPostsByUser(requestUser, pageable);
return PostDtoAssembler.assembleFrom(requestUser, result);
}
@Cacheable: 조회 캐시 설정 애노테이션 key: 저장소에 저장될 때의 key. (프로젝트에는 page 번호를 사용했다.) value: 저장소에 저장될 때의 그룹 명. (프로젝트에는 homeFeed로 설정했다.) condition: SpEL을 사용하여 캐싱 조건 설정. (프로젝트에는 게스트일 때만 설정했다.) unless: DB로부터 받아온 값에 따라 캐싱 조건 설정. (프로젝트에는 DB에서 받은 값이 없으면 캐시되지 않도록 설정했다.) 다음은 게시물 업데이트 혹은 삭제시 캐시 저장소를 초기화하는 처리다. PostService @CacheEvict(
value = ""homeFeed"",
allEntries = true
)
@Transactional
public PostImageUrlResponseDto write(PostRequestDto postRequestDto) {
Post post = createPost(postRequestDto);
Post savedPost = postRepository.save(post);

return PostImageUrlResponseDto.builder()
.id(savedPost.getId())
.imageUrls(savedPost.getImageUrls())
.build();
}
@CacheEvict: 캐시 저장소에 저장된 데이터를 지우는 애노테이션 value: 저장소에 저장될 때의 그룹 명. (프로젝트에는 homeFeed로 설정했다.) allEntries: true 설정시 저장소에 저장된 특정 그룹의 데이터 삭제 테스트를 작성해보자. 테스트를 하기 위해선 쿼리 카운팅이 구현되어 있어야 한다. PostFeedCacheIntegrationTest @DisplayName(""캐싱 - 비로그인 홈피드 조회시 두 번째 조회부터는 캐시 저장소에서 가져온다. (현재 페이지당 쿼리는 6번)"")
@Test
void readHomeFeed_Guest_LatestPosts() {
// given
createMockPosts();
HomeFeedRequestDto homeFeedRequestDto = createMockGuestHomeFeedRequest();
entityManager.clear();

// when
queryCounter.startCount(); // 카운트 시작
postFeedService.homeFeed(homeFeedRequestDto); // 첫번째 조회 (이때 캐싱)

List<PostResponseDto> postResponseDtos = postFeedService.homeFeed(homeFeedRequestDto); // 두번째 조회 (캐싱 저장소에서 데이터 가져옴.)

// then
assertThat(postResponseDtos)
.extracting(""authorName"", ""githubRepoUrl"", ""liked"")
.containsExactly(
tuple(""dani"", ""java-racingcar"", null),
tuple(""ginger"", ""jwp-chess"", null)
);
assertThat(queryCounter.getCount().getValue()).isEqualTo(6L);
}
동일한 조회를 두 번 보내서 메인 DB에 쿼리가 얼마나 날라갔는지 확인하는 방식으로 테스트를 진행하였다. 업데이트 혹은 삭제 시 초기화되는 테스트는 PR을 확인! 글이 너무 길어져 생략했습니다! 결과 캐시 저장소에 저장되는 형식은 key : value = page 번호 : 해당 page홈피드 조회 결과다. 다음 편은.. 다음 편은 캐싱을 적용시켜 어느정도에 성능 개션이 있었으며, 왜 캐싱을 사용하는 것이 성능 개선에 도움이 되는지 다룬다! To be Continue… Twitter Facebook Google+ # 백엔드 # Cache # Spring # Redis # 구현",BE
456,"M1(ARM)에서 Embedded Redis를 사용하는 방법 2021, Oct 20 안녕하세요, 다니입니다. 🌻 이슈 1 문제 상황 우아한테크코스 팀 프로젝트를 진행하며 Redis를 도입했다. 테스트 환경에서는 Embedded Redis가 동작하게 구성했는데, M1에서는 해당 DB가 실행되지 않는 현상이 발생했다. 백엔드 팀원 대부분이 M1을 사용하고 있어 이를 해결하는 게 우선시 됐다. 관련 내용을 찾아보다가 Embedded Redis 라이브러리에서 mac_arm64용 바이너리가 준비되어 있지 않다. 또, 소스 코드에도 MAC_OS_X_arm64가 없다. 이와 같은 사실을 알게 됐다. 참고한 글에 따르면, Embedded Redis 라이브러리를 수정해서 사용할 수도 있다고 한다. 하지만, 공식 문서에서는 사용자가 직접 바이너리를 지정해서 이용하는 걸 추천한다고 설명했다. 그래서 RedisServer 객체를 활용하는 방식을 선택했다. 해결 방법 먼저 Redis 소스 코드를 다운받고 컴파일한다. 아래 과정을 거치면, redis-6.2.5/src 경로에 바이너리 파일들이 빌드되어 준비된다. // 소스 코드 다운로드
% wget https://download.redis.io/releases/redis-6.2.5.tar.gz

// 압축 해제
% tar xzf redis-6.2.5.tar.gz

// redis-6.2.5 디렉토리 이동
% cd redis-6.2.5

// 파일 링크 및 설치 등
% make
다음으로 redis-server 바이너리 파일을 실행한다. % src/redis-server

39720:C 20 Oct 2021 14:28:50.565 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
39720:C 20 Oct 2021 14:28:50.565 # Redis version=6.2.5, bits=64, commit=d9e6d08d, modified=0, pid=39720, just started
39720:C 20 Oct 2021 14:28:50.565 # Warning: no config file specified, using the default config. In order to specify a config file use src/redis-server /path/to/redis.conf
39720:M 20 Oct 2021 14:28:50.566 * monotonic clock: POSIX clock_gettime
_._
_.-``__ ''-._
_.-`` `. `_. ''-._ Redis 6.2.5 (d9e6d08d/0) 64 bit
.-`` .-```. ```/ _.,_ ''-._
( ' , .-` | `, ) Running in standalone mode
|`-._`-...-` __...-.``-._|'` _.-'| Port: 6379
| `-._ `._ / _.-' | PID: 39720
`-._ `-._ `-./ _.-' _.-'
|`-._`-._ `-.__.-' _.-'_.-'|
| `-._`-._ _.-'_.-' | https://redis.io
`-._ `-._`-.__.-'_.-' _.-'
|`-._`-._ `-.__.-' _.-'_.-'|
| `-._`-._ _.-'_.-' |
`-._ `-._`-.__.-'_.-' _.-'
`-._ `-.__.-' _.-'
`-._ _.-'
`-.__.-'

39720:M 20 Oct 2021 14:28:50.569 # Server initialized
39720:M 20 Oct 2021 14:28:50.569 * Ready to accept connections
앞에서 redis-server가 잘 실행됐다면, 해당 파일의 이름을 변경하여 프로젝트에 추가한다. 이때, resources 디렉토리 하위에 복사해야 한다. 마지막으로 EmbeddedRedisConfiguration 클래스에 아래 코드를 추가한다. 여기까지 하면, M1에서도 Embedded Redis가 잘 작동된다. @PostConstruct
public void redisServer() throws IOException, URISyntaxException {
int redisPort = isRedisRunning() ? findAvailablePort() : port;

if (isArmMac()) {
redisServer = new RedisServer(
Objects.requireNonNull(getRedisFileForArcMac()),
redisPort
);
}
if (!isArmMac()) {
redisServer = new RedisServer(redisPort);
}

redisServer.start();
}

private boolean isArmMac() {
return Objects.equals(System.getProperty(""os.arch""), ""aarch64"") &&
Objects.equals(System.getProperty(""os.name""), ""Mac OS X"");
}

private File getRedisFileForArcMac() {
try {
return new ClassPathResource(""binary/redis/redis-server-6.2.5-mac-arm64"")
.getFile();
} catch (Exception e) {
throw new EmbeddedRedisServerException();
}
}
이슈 2 문제 상황 그런데, 나는 위의 방법을 적용해도 테스트가 계속 실패했다. 어떤 부분을 놓쳤는지 원인을 분석했는데, JDK 버전이 호환되지 않아 터진 것이었다. 기존에는 default 버전을 쓰고 있었는데, Azul Zulu Community(M1용) 버전으로 바꿔야 했다. 해결 방법 해당 버전으로 변경하고 실행해보니까, 이번에는 정말로 문제 없이 잘 작동했다. 테스트 실행 속도가 이전보다 빨라진 건 덤이었다! References ARM Mac (M1) 에서 embedded-redis 사용하기 Redis - Download Apple Silicon M1 용 어플 호환성+JDK Twitter Facebook Google+ # 백엔드 # Database # Embedded Redis # M1 # ARM",BE
468,"홈피드 조회 성능 테스트 경험기_1 2021, Oct 15 💡 Intro 진행 중인 프로젝트에서 구현한 웹 어플리케이션이 어느 정도의 부하를 견딜 수 있는지에 대한 성능테스트를 진행했다. 프로젝트는 개발자를 타켓으로 한 깃헙 레포지토리를 연동한 게시물을 업로드하여 개발자들이 자신의 작업을 공유하고 다른 이들의 프로젝트를 캐줄얼하게 엿볼 수 있는 SNS형 웹 어플리케이션이다. 웹 어플리케이션에 들어가자마자 최신순으로 정렬된 게시물 피드를 볼 수 있다. (비로그인/로그인 모두 가능) 홈피드 게시물 조회 성능테스트를 진행, 병목 지점을 분석하고 개선하는 과정을 따라가보자. 🌩 사전 작업 테스트 더미 데이터 입력 테스트를 진행하기 위해서는 실제 운영환경과 최대한 유사한 환경에서 테스트하는 것이 중요하다. 운영환경과 유사한 환경이라고 하면 크게 1) 인프라 구조 2) 데이터 두 가지가 있다. 먼저 대량의 더미 데이터를 입력하도록 한다. (팀원 케빈이 수고해주었다 !! 👍) * 테스트 데이터 : 게시물 100만 / 유저 20만
* 태그 10만 (1개당 게시물 10개)
* 댓글 100만 (게시물당 1개)
*
* 테스트 용이성을 위해 유저 1명 이름은 tester로 명명해 저장
* 테스트 용이성을 위해 태그 3개 이름은 java, javascript, spring로 명명해 저장
MariaDB 쿼리 캐시 끄기 왜 쿼리 캐시를 껐을까? 실제 어플리케이션에서는 query cache 설정이 켜져있음에도 불구하고 cache 설정을 끈 이유는 실제 환경에서는 많은 유저들이 여러 태그를 검색하여 매번 다양한 쿼리가 실행되지만 테스트 환경에서는 3개의 태그를 랜덤으로 실행하기 때문에 캐시 적중률이 실제 환경보다 높다. 따라서 db 쿼리캐시를 꺼서 최대한 실제 환경과 맞춰주도록 한다. 참고로 MySQL 8.0 부터는 쿼리 캐시 기능이 꺼져있다고 한다. 또한 여전히 os 측에서 하는 memory 캐시 영향이 있지만 제어하기 어려운 부분이므로 우선 넘어가도록 한다. 쿼리 캐시 확인 MariaDB config 파일에서 cache size를 0으로 설정한다. 이후 sudo service mysqld restart 로 DB를 재구동시킨다. 변경 후 적용 확인 MariaDB slow query 로그 설정하기 오래 걸리는 쿼리에 대한 로그를 남겨 특정 쿼리로 인한 병목이 있는지 확인할 수 있도록 설정한다. 단위는 1초 이상 걸리는 쿼리에 대한 로그를 남기는 것으로 했다. Slow query 적용 중인지 확인 Slow 쿼리 설정 적용 하기 slow_query_log = 1 부터 long_query_time 까지 적용 적용 후 sudo service mysqld restart 로 재시작 Slow query 설정 후 확인 제대로 적용되었는지 확인하기 위해서 5초 이상 걸리는 쿼리를 실행하고 로그파일 경로의 mariadb-slow.log에 대항 쿼리에 대한 로그가 남았는지 확인해보자. 🌩 테스트 진행하기 테스트 환경 테스트를 위해 구축한 테스트 환경은 다음과 같다. WAS 2대가 각각 AWS EC2 Medium 사양으로 실행중이다. AWS EC2 Medium 사양으로 Reverse Proxy가 있으며 Load balancer 역할을 하면 ssl 적용이 되어 있다. 데이터 베이스는 AWS EC2 Medium에 MariaDB로 3대가 연결되어 있다. Master DB 1개, Slave DB 2대로 replication이 적용되어 있다. 테스트 툴은 K6로 진행한다. AWS EC2 Medium 에 K6 테스트 서버를 구축했다. 왜 K6일까? 사실 팀 차원에서 하는 테스트 툴은 Ngrinder 이다. 하지만 AWS 권한 제한으로 인해 controller와 agent를 별도의 EC2로 분리하지 못했다. (그것 때문인지는 모르겠지만 간혹 랜덤하게 K6와 동일한 테스트를 돌렸을 때 결과가 매우 다르게 나올때도 있었다…) ngrinder는 반드시 분리하도록 권장하기 때문에 혹시 모를 영향을 최소화 하기 위해서 나는 K6에서 진행하였다. 또한 K6는 문서가 굉장히 깔끔하게 잘 되어 있어 스크립트를 짜거나 테스트 설정을 하는 것이 입문자에게 편하다는 장점이 있었다. 테스트 스크립트 및 설정 K6는 자바스크립트로 테스트 스크립트를 짠다. 부하 테스트는 약 10분간 148 명의 vuser로 진행했다. 본래 30분 이상을 하기를 권장하지만 시간 관계상 10분만 진행하고 빠르게 결과를 분석하기로 했다. 스크립트 비회원으로 홈 피드 조회 API 요청을 보낸다. 우선 pagination은 0 - 20 고정이다. (추후 랜덤 페이지 테스트를 진행해야한다.) 응답코드가 200 인지 확인한다. import http from 'k6/http';
import { URL } from 'https://jslib.k6.io/url/1.0.0/index.js';
import { check } from 'k6';
import { randomIntBetween } from ""https://jslib.k6.io/k6-utils/1.1.0/index.js"";

export let options = {
vus: 148,
duration: '600s',
};

export default function () {

const url = new URL('https://test-pick-git.o-r.kr/api/posts');

url.searchParams.append('page', 0);
url.searchParams.append('limit', 10);

const res = http.get(url.toString());

check(res, {
'is status 200': (r) => r.status === 200,
});
}
성능 테스트를 진행하면서 서버의 상태를 관리하기 위해 각각 WAS 2대, DB 2대에 대한 상태를 출력하고 모니터링 했다. vmstat 1 -Sm 와 top 명령어를 통해 프로세스의 상태, CPU 상태, 스왑 발생 여부, load average 등을 확인했다. 🌩 테스트 진행하기 첫번째 테스트 - WAS 오류 WAS 및 DB 모니터링: 왼쪽 위 부터 시계 방향으로 was1 → was2 → slaveDB2 → slaveDB1 WAS2에 대한 CPU idle 비율이 100% 이므로 해당 WAS가 동작하지 않은 것을 알아내었다. 확인해보니 어플리케이션이 종료되어 있었다. 테스트 진행시간이 5분정도 경과되었을 때 was2에 어플리케이션을 띄웠고 테스트는 그대로 계속 진행했다. 테스트 결과 DB의 경우 OS메모리 캐싱이 되므로 DISK I/O는 발생하지 않았다. 다만 비효율적인 쿼리에 의해 CPU 과부하가 걸리는 것을 확인할 수 있었다. 맨 왼쪽 칼럼 r(실행 대기 프로세스 수) 수치가 10 정도로 매우 높다. 본래 r은 CPU 코어 갯수여야 서버가 잘 돌아가고 있다고 판단한다. (현재 ec2 CPU 코어 개수 2개) 요청 당 실행 시간(http_req_duration) 13.33 초로 매우 긴 시간이 소요되기에 개선해야 할 점이 명확히 보였다. 두번째 테스트 WAS 및 DB 모니터링: 왼쪽 위 부터 시계 방향으로 was1 → was2 → slaveDB2 → slaveDB1 앞 테스트와 동일하게 WAS의 CPU나 I/O 상황은 대체적으로 양호하고 DB 서버에 CPU 과부하가 걸리는 것을 확인할 수 있다. 테스트 결과 WAS가 2대였음에도 불구하고 error rate이 줄어든 것 밖에 나아진 부분은 없었다. 요청 실행 시간이나 테스트 갯수 tps 등의 수치가 위와 동일했다. 이것을 통해 알 수 있는 것은 WAS의 성능이 아니라 DB에 의한 성능저하라는 것이다. 더 명확하게 알아보기 위해 slow query 로그를 확인해 보았다. 로그를 확인해보니 태그를 검색하고 검색 결과인 게시물을 조회하는 쿼리가 1.5 초 정도 소요되는 것을 확인할 수 있었다. 다음 포스트에서 병목이 생기는 DB 쿼리를 진단하고 개선한 후 결과에 대해서 다룬다. 백엔드 코다입니다 🙌 Twitter Facebook Google+ # 백엔드 # Database # 성능테스트",BE
469,"홈피드 조회 성능 테스트 경험기_2 2021, Oct 15 💡 Intro 이전 포스트에서 진행한 프로젝트에서 홈피드 게시물 조회 성능 테스트에 대한 결과를 보고 개선대상을 파악하고 개선한다. 개선 후 테스트를 재진행하여 결과를 비교한다. 🌩 쿼리 진단 이전 포스트에서 진행한 성능 테스트를 통해 DB 쿼리 쪽 병목이 있다는 것을 알아냈다. 구체적으로 쿼리를 자세히 살펴보면서 어떤 문제가 있는지 확인해보자. 홈피드 게시물을 반환할 때 발생하는 slow query 현재는 포스트 조회하는 쿼리가 최대값으로는 3.62 초가 소요된다. 쿼리의 실행계획을 확인해서 문제점을 파악해보니 100만건의 데이터를 거의 다 훑으면서 filesort를 하고 있었다. 게시물을 최신순으로 정렬하여 상위 10개를 가지고 오는 Pagination을 적용하고 있기 때문이다. 🌩 개선하기 createt_At 칼럼에 인덱스를 추가하여 데이터가 정렬되도록 한다. 인덱스를 건 후 실행계획을 확인해보니 filesort가 제거되었고 훑는 row 수가 대폭 줄어들었다. 🌩 개선 후 성능 테스트 인덱스 추가 후 테스트 테스트 중 서버 모니터링 본래 DB에 실행 대기 중인 프로세스(맨 왼쪽 r 칼럼)가 많았다. 약 20 이라는 수치를 보였으며 DB 서버의 CPU의 idle 상태도 항상 0%였다. 인덱스를 걸어서 쿼리 수행이 훨씬 빨라지면서 DB 인스턴스에 CPU 부하가 훨씬 줄었다. 또한 여전히 Disk I/O는 발생하지 않는다. 대신 WAS 쪽 CPU에 부하가 발생했다. r 수치가 11-15 사이를 왔다갔다 했으며 CPU idle 비율이 2% ~ 15%를 왔다갔다 했다. 왜 갑자기 이런 수치가 보여졌을까? 성능 테스트를 할 때 실운영 환경과의 차이가 있기 때문이다. 성능 테스트를 할 때는 한 vuser의 요청에 대한 응답이 오기 전까지 다음 요청을 보내지 않는다. 따라서 요청에 대한 응답이 느리면 그 다음 요청을 못보내기 때문에 이전 테스트 결과와 다르게 더 많은 요청을 보냈고, WAS의 CPU 부하가 발생했다. 인덱스 추가 후 성능 테스트 결과 진행된 테스트 개수: 6694 → 944360 요청 응답 시간: 13.41s → 83.57ms TPS: 10.94/s → 1573.72/s 성능이 대폭 향상된 것을 확인할 수 있다. 🌩 요약 기존 홈피드를 조회할 때 13초 가량의 시간이 소요 되었다. 분석해보니 디스크 I/O 부하는 없고 DB쪽 CPU 부하가 있으므로 쿼리 효율성의 문제라고 판단하였다. order by createdAt 쿼리의 부분이 풀스캔 + filesort를 하고 있음을 발견했다. (쿼리 실행 시간 3초 이상) createdAt 칼럼에 인덱스를 걸어서 풀스캔 → limit 개수 만큼만 스캔하도록 설정했다.. 결과적으로 응답 시간이 약 13초에서 85ms 정도로 단축되었다. 🌩 느낀점 테스트 코드를 통해서는 확인할 수 없는 허점을 발견할 수 있었던 것이 새로웠다. 성능 테스트를 하면서 가장 크게 느낀 것은 돌아가는 어플리케이션을 구현하는 것이 아니라 사용자에게 서비스될 수 있는 어플리케이션을 만들어야 한다는 것이다. 그렇게 하기 위해서는 경험하지 않으면 알 수 없는 많은 부분들을 고려해야한다. 기존에는 코드 퀄리티 자체와 원하는 결과를 내는 것에 집중했다면, 이제는 제한된 리소스 내에게 원하는 목표치까지 서비스할 수 있는 어플리케이션을 구현하는것이 중요하다는 시각이 트인 좋은 기회였다. 성능 테스트를 하면서 막힌 부분은 서버를 모니터링, 커넥션 개수 파악, 데이터베이스 쿼리 최적화, 네트워크 통신 부분이다. 많이 배웠던 CS 기초가 웹 어플리케이션을 구현하면서 많이 등장하지 않아서 왜 중요한지 체감하지 못하고 있있는데 이런 대용량 (대용량이라고 하기엔 훨씬 부족하지만..) 서비스를 만들면서 다시 마주하게 될 줄 몰랐다. 역시 기초가 튼튼한 것이 가장 중요한 것 같다. 성능 테스트를 하면서 부족한 CS 기초 (운영체제, 네트워크, 데이터베이스)를 병행해서 공부했다. 띄엄띄엄 알던 지식들이 하나로 연결되는 경험이었다. 백엔드 코다입니다 🙌 Twitter Facebook Google+ # 백엔드 # Database # 성능테스트",BE
470,"서비스 성능 개선 - MySQL Optimizer 실행 계획 분석을 기반으로 2021, Oct 18 1. 들어가며 안녕하세요, 케빈입니다. 현재 운영 중인 Pick-Git 서비스는 Spring Data JPA를 활용 중입니다. 개발 초기에는 서비스를 빠르게 개발하는 것이 급선무다 보니, 쿼리의 성능을 크게 고려하지 않았습니다. 기껏해야 트랜잭션 내부에서 실행되는 쿼리 로그를 보고 N + 1 문제가 있는지 등을 체크하는 정도에 불과했습니다. N + 1 문제는 Fetch Join 및 Batch Fetch Size 옵션 설정을 통해 어느정도 해소할 수 있었는데요. 어느 정도 필수적인 서비스 기능들이 완성되고부터는 서비스의 안정적인 운영에 관심이 많아졌습니다. nGrinder 및 K6 등을 통해 부하 테스트를 진행하면서 서비스의 트래픽 가용성을 확인할 수 있었는데요. DB Scale-Up이 불가능한 상황이라 DB Replication을 통한 Scale-Out을 통해 성능을 대폭 개선할 수 있었습니다. 그러나 여전히 트래픽 부하가 심해지면 다음과 같은 문제들이 여전히 존재했습니다. DB 서버의 CPU Utilization이 100%에 육박합니다. TPS가 급격하게 하락합니다. HikariCP Deadlock이 종종 발생합니다. SNS 서비스 특성상 조회 트랜잭션이 빈번하게 발생하기 때문에, 일부 조회 요청은 Redis를 도입해 성능을 향상시킬 수 있겠지만 이 또한 미봉책에 불과합니다. 근본적으로 쿼리 성능을 향상시키는 것이 중요합니다. Optimizer 실행 게획 분석을 기반으로 쿼리 튜닝을 진행함으로써 서비스 성능을 개선한 사례를 공유하고자 합니다. 2. 사전 작업 운영 환경과 최대한 동일하게 구성한 테스트 환경을 구축하고, 테스트용 DB에 대량의 더미 데이터를 삽입했습니다. 테스트 데이터 셋의 구성은 다음과 같습니다. 게시물 100만건 댓글 100만건 유저 20만건 태그 10만건 WAS 로드 밸런싱 및 DB Replication 적용 이후 팀원 코다가 다음 항목들에 대해 MariaDB 옵션을 설정했습니다. Query Cache OFF Duration이 1초 이상 걸리는 Slow Query 로깅 MySQL 8.0부터는 Query Cache 기능이 사라졌습니다. 실제 운영 DB의 경우 Query Cache 기능을 사용하고 있지만, 더 상세한 성능 측정을 위해 테스트 DB는 Query Cache 옵션을 끄도록 했습니다. 3. 홈 피드 조회 PostRepository.java @Query(""select p from Post p left join fetch p.user order by p.createdAt desc"")
List<Post> findAllPosts(Pageable pageable);
홈 피드 조회 비회원 유저의 홈 피드 게시물 조회 JPQL 쿼리입니다. 팀원 코다가 홈피드 조회 성능 테스트 경험기_2 글에서 홈 피드 조회에 대한 쿼리 튜닝 경험을 이미 공유한 상태인데요. 이번 절에서는 팀원 코다가 정리한 자료를 짧게 요약해보겠습니다. (코다 감사!) SQL select
post0_.id as id1_8_0_,
user1_.id as id1_14_1_,
post0_.content as content2_8_0_,
post0_.created_at as created_3_8_0_,
post0_.github_repo_url as github_r4_8_0_,
post0_.updated_at as updated_5_8_0_,
post0_.user_id as user_id6_8_0_,
user1_.description as descript2_14_1_,
user1_.image as image3_14_1_,
user1_.name as name4_14_1_,
user1_.company as company5_14_1_,
user1_.github_url as github_u6_14_1_,
user1_.location as location7_14_1_,
user1_.twitter as twitter8_14_1_,
user1_.website as website9_14_1_
from
post post0_
left outer join
user user1_
on post0_.user_id=user1_.id
order by
post0_.created_at desc limit 0, 10;
옵티마이저 실행 계획을 확인한 결과, 게시물 100만건 Table Full Scan하고 Filesort를 진행하고 있었습니다. 게시물을 최신순으로 정렬하고 상위 10개를 가져오기 때문입니다. 슬로우 쿼리 로그에는 쿼리 수행 시간이 1.5초에서 3초까지 소요되었습니다. 부하 테스트를 진행할 때 vmstat으로 WAS 및 DB 서버 상태를 검사한 결과입니다. 상단의 2개는 WAS이며 하단의 2개가 조회를 위한 DB Slave 서버입니다. DB 서버의 CPU Utilization(us) 및 실행 대기 프로세스의 수 (r)이 매우 높게 측정되는 것을 볼 수 있습니다. DB에 병목이 심한데 Scale-Up을 진행할 수 없는 상황인만큼 쿼리 최적화가 시급합니다. 팀원 코다가 created_at 컬럼에 인덱스를 걸고 다시 실행 계획을 확인한 결과, 이번에는 Table Full Scan없이 Index Full Scan을 통해 게시물을 조회합니다. Filesort도 사라졌으며 쿼리의 Duration 또한 3초에서 0.0094초로 대폭 하락했습니다. 과거 게시물이 35만건일 때 VUser 600명이 10분간 홈 피드를 조회하는 부하 테스트를 진행한 적이 있었는데요. 이 당시 TPS가 평균 30을 기록했습니다. 현재는 게시물이 100만건이니 인덱스를 걸지 않는다면 이보다 성능이 더 나쁘게 측정될 것입니다. 이번에는 게시물이 100만건일 때 VUser 500명이 10분간 홈 피드를 조회하는 부하 테스트를 진행했습니다. TPS가 평균 300대로 매우 높게 측정되는 것을 확인할 수 있었습니다. 테스트 DB 서버의 대기 중인 프로세스의 수 및 CPU Utilization 또한 많이 개선되었습니다. 3.1. Pagination의 늪 성능이 개선되어서 팀원 모두들 흐뭇(?)했었는데, 팀원 코다가 다음과 같은 이슈를 제기했습니다. 조회 쿼리 실행할 때 Page 크기가 특정 범위를 초과하면 다시 Full Table Scan하네요? 일단 다시 MySQL Workbench를 실행해서 실행 계획을 확인해봅시다. SQL select
post0_.id as id1_8_0_,
user1_.id as id1_14_1_,
post0_.content as content2_8_0_,
post0_.created_at as created_3_8_0_,
post0_.github_repo_url as github_r4_8_0_,
post0_.updated_at as updated_5_8_0_,
post0_.user_id as user_id6_8_0_,
user1_.description as descript2_14_1_,
user1_.image as image3_14_1_,
user1_.name as name4_14_1_,
user1_.company as company5_14_1_,
user1_.github_url as github_u6_14_1_,
user1_.location as location7_14_1_,
user1_.twitter as twitter8_14_1_,
user1_.website as website9_14_1_
from
post post0_
left outer join
user user1_
on post0_.user_id=user1_.id
order by
post0_.created_at desc limit 5000, 10;
OFFSET에 해당하는 부분을 5000으로 높이니 인덱스를 타지 않고 Table Full Scan, Filesort를 사용하고 있습니다. 아울러 쿼리 수행 시간이 다시 나빠지기 시작했습니다. OFFSET을 500000으로 높인 결과입니다. 즉, OFFSET 크기와 조회 쿼리 성능이 반비례함을 알 수 있습니다. TestRunner.groovy @Test
public void test() {
long randomId = Math.abs(random.nextInt() % 99999) + 1
HTTPResponse response = request.GET(""https://{test-api-address}/api/posts?page="" + randomId + ""&limit=10"", params)

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}
기존의 테스트 스크립트는 page=10 쿼리 파라미터를 고정해서 사용했었습니다. 이번에는 높은 범위의 난수를 생성해서 API 요청을 보냈습니다. 테스트를 시작한지 1분쯤 되니 테스트가 중단되었습니다. 에러율이 50%를 초과했기 때문인데요. TPS가 극악에 달할 정도로 낮은 것을 확인했습니다. 에러 로그를 보니 조회 쿼리를 수행하는데 너무 많은 시간이 소요되어 대기 중이던 트랜잭션 워커 쓰레드가 Connection을 얻지 못해 예외를 발생시키고 있습니다. HikariCP Deadlock 문제가 발생하는 중입니다. 기존에 page=10 일때는 인덱스를 적용하지 않아도 쿼리를 수행하는데 1초 ~ 3초 정도 소요되었습니다. 그러나 page=600,000 혹은 page=900,000과 같은 같이 OFFSET이 높은 경우 수행 시간 10초 이상의 쿼리들이 빈번했습니다. 최대 18초가 소요된 쿼리도 보이네요. 3.2. 문제 원인 SQL select * from post order by id limit 150000, 10;
해당 쿼리는 150,010개의 행을 ID로 정렬한 다음 그 중 처음 10개 행을 반환하겠다는 의미입니다. 실질적으로 10개의 레코드가 필요하지만, 그 과정에서 150,000개에 달하는 개수의 레코드를 카운팅하게됩니다. 일반적으로 테이블은 레코드를 정렬해주는 인덱스가 있다면 Filesort를 사용하지 않습니다. 그런데 정렬된 데이터를 조회하는데 쿼리가 많게는 10초 이상이 걸린다는 것은 확실히 문제가 있어 보입니다. 인덱스는 논리적인 개념에서 테이블의 일부분이지만 물리적으로는 테이블과 별도로 존재하는 개체입니다. 대표적으로 두 가지 정보를 가지고 있습니다. Index Key : 인덱스가 생성된 컬럼들입니다. Table Pointer : 특정 레코드가 나타내는 행을 고유하게 지칭하는 어떠한 값을 의미합니다. InnoDB의 경우 PK 값입니다. 인덱스는 B-Tree 자료구조에 저장되며 이를 바탕으로 ref 및 range 검색을 통해 데이터를 빠르게 조회합니다. 그러나 인덱스 자체는 테이블의 모든 정보를 담고 있지 않습니다. 순서를 유지하며 실제 테이블 값을 찾기 위해서, 인덱스와 테이블이 연결되어야 합니다. 즉, DB 엔진은 행 포인터를 기반으로 각 인덱스 레코드에 상응하는 테이블 레코드를 찾아 인덱싱이 되지 않은 모든 데이터를 반환해야합니다. 테이블 레코드를 상응하는 인덱스 레코드와 연결하는 행위를 Row Lookup이라고 합니다. 위 사진에서 인덱스와 테이블을 연결하는 구불구불한 화살표들입니다. 인덱스 레코드와 테이블 레코드는 메모리 상 및 디스크 상 모두 서로 떨어져 개별적으로 존재합니다. Row Lookup은 Page나 Cache가 점점 더 적중하지 못하고 Miss하는 상황을 유발합니다. 또한 디스크는 순차 접근을 통해 탐색을 할 것이고 그 결과 많은 비용이 발생합니다. 위 사진에서 모든 연결들을 순회하는데 많은 시간이 소요됩니다. 3.3. 해결 그런데 LIMIT 및 OFFSET 등을 사용할 때 이러한 Row Lookup 작업이 정말 필요할까요? 만약 LIMIT 8, 2 쿼리라면 인덱스를 사용해 앞선 8개의 값은 무시하고 남은 2개만 반환하면 될 일입니다. 이러한 방법을 Late Row Lookup이라고 합니다. DB 엔진은 정말 다른 방법이 없을 때 Row Lookup을 진행합니다. 만약 인덱스 필드들을 기반으로 행을 필터링하는 방법이 존재한다면, 실제 MySQL Table을 Lookup하기 전 수행됩니다. SQL select * from post where id > 150000 order by id limit 10;
이처럼 인덱스를 사용할 수 있는 WHERE 구문을 통해 먼저 데이터를 필터링하면 150,000개의 Row는 무시하고 LIMIT에 걸린 10개의 Row에 대해서만 Row Lookup이 발생합니다. 따라서 Pagination을 처리할 때 적절히 WHERE 조건을 사용해주면, 상기의 문제를 극복할 수 있습니다. 현재 운영 중인 서비스 중 비회원의 홈 피드 조회는 게시물을 최신순으로 보여줍니다. 즉, 날짜를 기반으로 내림차순 정렬하는데요. 따라서 프론트엔드 측에서 다음 페이지를 갱신할 때, 이전 페이지에서 가장 하단에 위치한 게시물의 날짜를 백엔드로 전달하고 이를 WHERE 조건으로 함께 사용합니다. SQL select
post0_.id as id1_8_0_,
user1_.id as id1_14_1_,
post0_.content as content2_8_0_,
post0_.created_at as created_3_8_0_,
post0_.github_repo_url as github_r4_8_0_,
post0_.updated_at as updated_5_8_0_,
post0_.user_id as user_id6_8_0_,
user1_.description as descript2_14_1_,
user1_.image as image3_14_1_,
user1_.name as name4_14_1_,
user1_.company as company5_14_1_,
user1_.github_url as github_u6_14_1_,
user1_.location as location7_14_1_,
user1_.twitter as twitter8_14_1_,
user1_.website as website9_14_1_
from
post post0_
left outer join
user user1_
on post0_.user_id=user1_.id
where
post0_.created_at < ${'foo-date'}
order by
post0_.created_at desc limit 10;
조회 쿼리 성능이 다시 향상되었으며, Full Table Scan 및 Filesort를 더이상 사용하지 않습니다. 그런데 날짜를 WHERE 조건에 사용하는 것이 다소 불편하다고 느꼈습니다. 대부분의 경우 ID PK 데이터 순서와 created_at 데이터 순서가 동일합니다. 따라서 프론트엔드 측에서 다음 페이지를 갱신할 때, 이전 페이지에서 가장 하단에 위치한 게시물의 날짜가 아닌 ID를 전달해서 쿼리에 사용할 수 있습니다. SQL select
post0_.id as id1_8_0_,
user1_.id as id1_14_1_,
post0_.content as content2_8_0_,
post0_.created_at as created_3_8_0_,
post0_.github_repo_url as github_r4_8_0_,
post0_.updated_at as updated_5_8_0_,
post0_.user_id as user_id6_8_0_,
user1_.description as descript2_14_1_,
user1_.image as image3_14_1_,
user1_.name as name4_14_1_,
user1_.company as company5_14_1_,
user1_.github_url as github_u6_14_1_,
user1_.location as location7_14_1_,
user1_.twitter as twitter8_14_1_,
user1_.website as website9_14_1_
from
post post0_
left outer join
user user1_
on post0_.user_id=user1_.id
where
post0_.id < ${'foo-post-id'}
order by
post0_.created_at desc limit 10;
WHERE 조건을 사용하지 않던 때보다는 쿼리 성능이 향상 되었으나, WHERE 조건에 날짜를 사용하는 방식보다는 현저히 느립니다. WHERE 조건에 날짜를 사용하는 경우 INDEX Range Scan을 사용하지만, ID를 사용하니 INDEX Full Scan을 사용하고 있습니다. 현재 WHERE 조건과 ORDER 조건에서 사용 중인 인덱스가 다르기 때문입니다. SQL 문장이 WHERE 절과 ORDER BY 절을 가진다면, WHERE 조건은 A 인덱스를 사용하고 ORDER BY는 B 인덱스를 사용하는 것은 불가능합니다. WHERE 절과 ORDER BY 절이 같이 사용된 쿼리 문장은 다음 중 한 가지 방법으로 인덱스를 사용합니다. WHERE 절과 ORDER 절 모두 동시에 같은 인덱스를 사용하기. WHERE 절의 비교 조건에 사용하는 칼럼과 ORDER BY 절의 정렬 대상 칼럼이 모두 하나의 인덱스에 연속해서 포함되어 있을 때 이 방식으로 인덱스를 사용할 수 있습니다. WHERE 절의 인덱스만 사용하기. ORDER 절의 인덱스만 사용하기. 현재 Index Full Scan을 사용하지 않기 위해서는 WHERE + ORDER 컬럼을 모두 포함하는 INDEX (ID, CREATED_AT)를 새로 만들거나 어느 한 쪽의 인덱스를 사용하도록 WHERE 절이나 ORDER 절의 컬럼을 수정해야합니다. 편의를 위해 ORDER 절도 ID를 사용하도록 수정해보겠습니다. SQL select
post0_.id as id1_8_0_,
user1_.id as id1_14_1_,
post0_.content as content2_8_0_,
post0_.created_at as created_3_8_0_,
post0_.github_repo_url as github_r4_8_0_,
post0_.updated_at as updated_5_8_0_,
post0_.user_id as user_id6_8_0_,
user1_.description as descript2_14_1_,
user1_.image as image3_14_1_,
user1_.name as name4_14_1_,
user1_.company as company5_14_1_,
user1_.github_url as github_u6_14_1_,
user1_.location as location7_14_1_,
user1_.twitter as twitter8_14_1_,
user1_.website as website9_14_1_
from
post post0_
left outer join
user user1_
on post0_.user_id=user1_.id
where
post0_.id < ${'foo-post-id'}
order by
post0_.id desc limit 10;
WHERE 절과 ORDER 절 모두 ID를 사용하니 다시금 Index Range Scan을 하며 성능이 향상되었습니다. nGrinder를 통해 부하 테스트를 진행했습니다. 난수 발생을 통해 page=600000과 같은 극단적인 요청을 지속적으로 발생시켰지만, 평균 TPS가 300대로 유지되는 등 Pagination 조회 성능이 크게 하락하지 않는 것을 확인할 수 있습니다. 😊😊 4. 유저 검색 UserRepository.java @Query(""select u from User u where u.basicProfile.name like %:username%"")
List<User> searchByUsernameLike(@Param(""username"") String username, Pageable pageable);
SQL select
user0_.id as id1_14_,
user0_.description as descript2_14_,
user0_.image as image3_14_,
user0_.name as name4_14_,
user0_.company as company5_14_,
user0_.github_url as github_u6_14_,
user0_.location as location7_14_,
user0_.twitter as twitter8_14_,
user0_.website as website9_14_
from
user user0_
where
user0_.name like ? limit ?
현재 사용 중인 유저 검색 방법은 여러 문제가 존재합니다. 앞서 언급한 Pagination OFFSET 문제가 존재하기 때문에 WHERE에 적절한 조건이 추가되어야 합니다. 유저를 이름으로 조회할 때 LIKE 연산자를 이용하는데, % 기호를 앞에 넣어 사용하면 유저 이름 인덱스를 사용하지 못합니다. 따라서 해당 쿼리를 실행하면 Table Full Scan이 발생합니다. 회원이 20만건일 때 VUser 500명이 10분간 회원 이름을 검색하는 부하 테스트를 진행했습니다. 평균 TPS는 38.9입니다. 테스트를 진행할 때 DB 서버의 상태를 측정해보니 대기 중인 프로세스의 수 및 CPU Utilization이 매우 높습니다. 현재 테스트는 심지어 이전 절처럼 Pagination OFFSET을 높게 준 것이 아니라 page=0으로 매우 낮게 주었습니다. 즉, 1번 문제는 쉽게 처치할 수 있으나 궁극적인 원인은 2번 문제입니다. 팀 회의를 통해 도출한 대안은 다음과 같습니다. 인덱스를 타기 위해 LIKE :username%과 같이 앞에서 사용하는 % 기호를 삭제하기. 쿼리 튜닝을 포기하고 검색 요청의 경우 Elastic Search를 통해 처리하기. 1번 방법은 클라이언트가 입력한 문자열로 시작하는 유저 이름만을 검색하기에 사용자 경험 측면에서 부적절하다고 판단했습니다. 따라서 2번 방법을 채택하기로 결정했습니다. 물론 Elastic Search라는 별도의 DB를 사용함으로써 고려해야할 관리 포인트가 많아진다는 단점이 존재합니다. RDB와의 데이터 정합성을 제대로 유지해야합니다. ES는 읽기 성능이 우수하지만 쓰기 성능이 나쁜 만큼, 업데이트 방식에 대한 추가적인 학습이 필요합니다. 실시간으로 처리할 것인지 배치 방식으로 처리할 것인지 등. 고가용성을 위한 단일 장애점 극복을 위해 ES 클러스터링 및 샤딩 등 아키텍쳐에 대한 고려도 필요합니다. 그러나 지금으로서는 ES를 도입하는 것이 가장 최선이라 여겨졌습니다. ES를 적용한 다음 부하 테스트를 진행했습니다. RDB를 사용할 때와 테스트 전제 조건은 동일합니다. RDB의 경우 DB Replication을 적용해 2대의 Slave DB가 조회를 처리했음에도 불구하고 평균 TPS가 38.9였습니다. 그러나 ES의 경우 단 1대의 서버만으로도 평균 TPS가 82.2를 기록했습니다! 이는 RDB를 사용하는 기존의 방식보다 성능이 2배 이상을 상회합니다. 향후 ElasticSearch 또한 클러스터링 및 샤딩 등을 적용하면 더 높은 성능 및 고가용성을 얻을 수 있겠다고 느꼈습니다. 😊 5. 마치며 성능 개선과 클린코드는 끝을 바라보고 하는게 아니에요. Optimizer 실행 계획 분석을 통해 팀원들과 의사결정을 진행하고, 쿼리 튜닝 및 별도의 DB 도입을 통해 서비스의 일부 기능들의 성능을 개선했습니다. 그럼에도 불구하고 아직 갈 길이 멀다고 느껴집니다. 여전히 개선이 필요한 쿼리들이 많기 때문입니다. SQL select
post0_.id as id1_8_0_,
user1_.id as id1_14_1_,
post0_.content as content2_8_0_,
post0_.created_at as created_3_8_0_,
post0_.github_repo_url as github_r4_8_0_,
post0_.updated_at as updated_5_8_0_,
post0_.user_id as user_id6_8_0_,
user1_.description as descript2_14_1_,
user1_.image as image3_14_1_,
user1_.name as name4_14_1_,
user1_.company as company5_14_1_,
user1_.github_url as github_u6_14_1_,
user1_.location as location7_14_1_,
user1_.twitter as twitter8_14_1_,
user1_.website as website9_14_1_
from
post post0_
inner join
user user1_
on post0_.user_id=user1_.id
where
user1_.id in (
select
user3_.id
from
follow follow2_
inner join
user user3_
on follow2_.target_id=user3_.id
and (
follow2_.source_id=?
)
)
or user1_.id=?
order by
post0_.created_at desc limit ?
로그인 회원의 경우, 홈 피드를 조회할 때 자기 자신의 게시물 및 팔로잉 중인 유저의 게시물만 조회하는 비즈니스 로직을 가지고 있습니다. 이 부분 또한 페이지네이션이 걸려 있습니다. OR 절 때문에 인덱스를 타지 못해 Table Full Scan이 발생할 뿐만 아니라, 임시 테이블 생성 및 Filesort를 사용합니다. 그 결과 쿼리 수행 속도가 배우 느려집니다. OR 절만 삭제해도 인덱스를 사용해 성능이 대폭 향상되는데, 해당 부분을 어떻게 개선할지 팀 차원에서 지속적으로 논의 중입니다. 🧐 SQL select
distinct post1_.id as id1_8_,
post1_.content as content2_8_,
post1_.created_at as created_3_8_,
post1_.github_repo_url as github_r4_8_,
post1_.updated_at as updated_5_8_,
post1_.user_id as user_id6_8_
from
post_tag posttag0_
inner join
post post1_
on posttag0_.post_id=post1_.id cross
join
tag tag2_
where
posttag0_.tag_id=tag2_.id
and (
tag2_.name in (
?
)
) limit ?
게시물을 복수 개의 태그를 기반으로 조회하는 쿼리입니다. 게시물과 태그는 N:N 관계를 맺고 있으며, 태그 이름은 유니크 인덱스가 걸려있습니다. Index Range Scan을 사용하고 커버링 인덱스가 적용되고 있습니다. 평균 TPS는 300 이상으로 성능 또한 현재로서는 나름 준수한 편입니다. 그런데 현재 SQL 쿼리에서 Cross Join이 발생하고 있습니다. 성능에 시급한 이슈는 없지만 추후 이러한 부분을 제거할 수 있을지 등 또한 학습 차원에서 팀원들과 논의 중에 있습니다. (다들 쿼리 튜닝이 처음인지라…) 긴 글 읽어주셔서 감사합니다. References 홈피드 조회 성능 테스트 경험기_1 홈피드 조회 성능 테스트 경험기_2 MySQL ORDER BY / LIMIT performance: late row lookups Real MySQL [7-8] 쿼리 작성 및 최적화 - WHERE, ORDER BY 인덱스 사용 Twitter Facebook Google+ # 백엔드 # MySQL # ElasticSearch",BE
474,"다이내믹 프록시를 이용한 쿼리 카운팅 적용기 (feat. 캐싱 테스트) 2021, Oct 08 목차 목차 들어가며 사전 지식 JDBC와 DataSource 동작 과정 다이내믹 프록시 쿼리 카운팅 구현 사용 예시 캐싱 테스트 N + 1 문제 분석 마치며 안녕하세요! 마크입니다 :) 들어가며 주변에서 특정 비즈니스를 수행하면서 몇 개의 쿼리가 날라가는지 직접 확인하는 장면을 많이 봤다. 적어도 필자는 그랬다. 문제는 테스트할 때 위와같이 쿼리가 굉장히 많이 날라가며, 어떤 코드가 몇 개의 쿼리를 날렸는지 파악하기 힘들다. 또한, N + 1 문제와 캐싱 테스트는 쿼리가 얼마나 날라갔는지가 굉장히 중요한 지표가 된다. 필자는 캐싱 테스트를 위해 쿼리 카운터를 구현하게 되었다. 이번 글은 이렇게 특정 시점부터 특정 시점까지 몇 개의 쿼리가 날라가고, 어떤 쿼리가 날라갔는지 쉽게 확인할 수 있는 커스텀 방법을 소개하고자 한다. 핵심을 말하자면, 다이내믹 프록시를 이용하여 DataSource의 getConnection부분을 프록시로 감싸는 방법이다. 사전 지식 본격적으로 시작하기전에 우선 사전 지식 두 가지가 있다. JDBC 동작 과정 다이내믹 프록시 JDBC와 DataSource 동작 과정 🤔 JDBC와 DataSource 동작 과정은 왜 알아야하는가? 출처: https://terasolunaorg.github.io/guideline/5.1.0.RELEASE/en/ArchitectureInDetail/DataAccessJpa.html JDBCTemplate, Hibernate등 여러 가지 라이브러리 혹은 프레임워크 모두 JDBC를 사용하여 DB와 통신을 한다. 그리고 DataSource를 통해 DB 커넥션을 위한 정보와 커넥션을 제공받는다. 결론적으로 JDBC와 DataSource를 통해 커스텀해야 쿼리 카운팅을 할 수 있다. 🤔 도대체 어떻게 한다는 것인가? 코드를 통해 어느 부분을 커스텀하면 쿼리 카운팅이 가능한지 살펴보자. DataSourceConfig @Configuration
public class DataSourceConfig {

@Bean
public UserDao userDao() {
return new UserDao(dataSource());
}

@Bean
public DataSource dataSource() {
DriverManagerDataSource dataSource = new DriverManagerDataSource();
dataSource.setDriverClassName(""org.h2.Driver"");
dataSource.setUrl(""jdbc:h2:tcp://localhost/~/toby"");
dataSource.setUsername(""sa"");
dataSource.setPassword("""");
return dataSource;
}
}
UserDao public class UserDao {
private final DataSource dataSource;

public UserDao(DataSource dataSource) {
this.dataSource = dataSource;
}

public void save(User user) throws SQLException {
Connection conn = null;
PreparedStatement ps = null;
try {
// DataSource로부터 Connection을 가져옴.
conn = dataSource.getConnection();

// Connection으로부터 Statement를 가져와 SQL 쿼리를 실행한다.
ps = conn.prepareStatement(""insert into users(id, name, password) values(?, ?, ?)"");
ps.setString(1, user.getId());
ps.setString(2, user.getName());
ps.setString(3, user.getPassword());

ps.executeUpdate();
} catch (SQLException e){
...
} finally {
...
}
}
...
}
전형적인 난감한 UserDao 예시이다. 🤔 위 코드에서 쿼리 카운팅을 해볼 커스텀할 부분을 찾았는가? // DataSource로부터 Connection을 가져옴.
conn = dataSource.getConnection();

// Connection으로부터 Statement를 가져와 SQL 쿼리를 실행한다.
ps = conn.prepareStatement(""insert into users(id, name, password) values(?, ?, ?)"");
바로 SQL 쿼리가 매개변수로 주어지는 conn.preparedStatement 메서드 이다. 즉, DataSource에서 반환하는 Connection을 프록시로 감싸서 preparedStatement가 호출될 때 해당 매개변수를 카운팅하면 된다. 아직 어려운가? 걱정말라. 밑에서 커스텀하면서 이해가 될 것이다. 다이내믹 프록시 위에서 언급했듯이 Connection을 프록시로 감싸야 쿼리 카운팅이 가능하다. 🤔 다이내믹 프록시 동작 원리 일반적인 프록시와 다르게, 다이내믹 프록시는 모든 요청을 InvocationHandler에 위임한다. 즉, sayHello, sayHi, sayThankYou등 메서드를 호출하면 리플렉션을 통해 Method와 args로 변환되어 InvocationHandler의 invoke()에 넘겨진다. 부가로직은 기존의 프록시처럼 InvocationHandler에 정의해두면 된다. 구체적으로 보면 다이내믹 프록시 객체가 클라이언트의 모든 요청을 리플렉션 정보로 변환해서 InvocationHandler 구현 객체의 invoke()메서드로 넘긴다. public Object invoke(Object proxy, Method method, Object[] args) 만들어진 다이내믹 프록시는 메서드 요청을 리플렉션을 이용해 메타 데이터를 뽑아내고, Method와 매개변수와 함께 InvocationHandler.invoke에게 메시지 요청한다. 더 자세한 내용은 다이내믹 프록시 정리 글을 참고. 🤔 일반적인 프록시도 있는데 왜 다이내믹 프록시를 사용해야하는가? 필자는 처음에 의문이었다. 하지만 구현하면서 의문점을 해결할 수 있었다. 프록시의 문제점중 하나는 부가기능이 필요없는 메서드도 구현해서 타깃으로 위임하는 코드를 일일이 만들어줘야한다는 것이다. 심지어 타깃 인터페이스의 메서드가 추가되거나 변경될 때마다 함께 수정해줘야 한다. 현재 프록시를 통해 커스텀해야하는 인터페이스는 Connection이다. Docs - Connection를 가서 메서드 개수를 보면 알겠지만, 10개가 훌쩍 넘는다. 반면에, 커스텀에 사용되는 메서드는 prepareStatement메서드 뿐이다. 즉, 관련없는 10개가 넘는 메서드를 모두 위임해주는 코드를 작성해줘야한다. 이런 수고스러움을 해결하기 위해서 다이내믹 프록시를 이용한다. 쿼리 카운팅 구현 이제 본격적으로 쿼리 카운팅을 적용시켜 N + 1 문제를 해결본다. 프록시를 적용하는 것을 큰그림으로 그려보면 다음과 같다. 적용전 적용후 카운트 데이터 객체 구현 Count @Getter
public class Count {

private long value;

public Count(long value) {
this.value = value;
}

public Count countOne() {
return new Count(++value);
}
}
QueryCounter @Getter
public class QueryCounter {

private Count count;
private boolean countable;

public QueryCounter() {
countable = false;
count = new Count(0L);
}

public void startCount() {
countable = true;
count = new Count(0L);
}

public void countOne() {
if (!isCountable()) {
throw new RuntimeException(""[Error] 아직 카운트를 시작하지 않았습니다."");
}
count = count.countOne();
}

public void endCount() {
countable = false;
}
}
QueryCounter 테스트 코드 Connection 프록시 구현 ProxyConnectionHandler public class ProxyConnectionHandler implements InvocationHandler {

private final Connection connection;
private final QueryCounter queryCounter;

public ProxyConnectionHandler(Connection connection, QueryCounter queryCounter) {
this.connection = connection;
this.queryCounter = queryCounter;
}

// 쿼리 카운팅 (부가 기능 구현)
@Override
public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
if (queryCounter.isCountable()) {
if (method.getName().equals(""prepareStatement"")) {
 return getConnectionWithCountQuery(method, args); // 핵심 로직 호출 및 반환
}
}
return method.invoke(connection, args); // 핵심 로직 호출 및 반환
}

// 카운트
private Object getConnectionWithCountQuery(Method method, Object[] args)
throws InvocationTargetException, IllegalAccessException {
PreparedStatement preparedStatement = (PreparedStatement) method.invoke(connection, args);

for (Object statement : args) {
if (isQueryStatement(statement)) {
 System.out.println(""## Query : "" + (String) statement); // 추후에 로깅으로 수정 예정
 queryCounter.countOne();
 break;
}
}
return preparedStatement;
}

// preparedStatement가 호출될 때 해당 매개변수가 String 형식이며, select으로 시작하는 쿼리인지 체크.
private boolean isQueryStatement(Object statement) {
if (statement.getClass().isAssignableFrom(String.class)) { // 매개변수가 String인지 확인
String sql = (String) statement;
return sql.startsWith(""select"");
}
return false;
}
}
어려워 보이지만, 사실 간단한 코드이다. 다이내믹 프록시를 이용하여 invoke에 넘어오는 호출 메서드 정보를 바탕으로 PreparedStatement메서드면 해당 메서드의 매개변수를 뽑아내서 카운팅하는 것이다. ProxyConnectionHandler - 소스 코드 DataSource 프록시 구현 CountDataSource public class CountDataSource implements DataSource {

private final QueryCounter queryCounter;
private final DataSource targetDataSource;

public CountDataSource(QueryCounter queryCounter, DataSource targetDataSource) {
this.queryCounter = queryCounter;
this.targetDataSource = targetDataSource;
}

@Override
public Connection getConnection() throws SQLException {
Connection connection = targetDataSource.getConnection();
return (Connection) Proxy.newProxyInstance(
connection.getClass().getClassLoader(),
connection.getClass().getInterfaces(),
new ProxyConnectionHandler(connection, queryCounter)
);
}

... 타깃으로 위임하는 코드
}
이제 Connection에 프록시를 설정해주기 위해서 기존의 DataSource의 프록시 역할을 하는 CountDataSource 생성해주었다. CountDataSource - 소스 코드 getConnection제외하고는 모든 메서드에 위임하는 코드를 삽입해줘야한다. 프록시의 단점이기도 하며, 이 부분도 다이내믹 프록시를 사용해도 좋다. 하지만 필자는 잦은 리플렉션은 성능상 좋지 않고, 메서드의 개수가 적으므로, 이 부분은 일반적인 프록시 패턴을 사용했다. DataSource 설정 DataSourceConfig @Configuration
public class DataSourceConfig {

@Bean
public QueryCounter queryCounter() {
return new QueryCounter();
}

@Bean
public DataSource dataSource() {
DataSource dataSource = DataSourceBuilder.create()
.driverClassName(""org.h2.Driver"")
.url(""jdbc:h2:mem:~/test;MODE=MySQL;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE"")
.username(""SA"")
.password("""").build();
return new CountDataSource(queryCounter(), dataSource);
}
}
QueryCount를 통해 몇 번 쿼리가 날라갔는지 확인해야하기에, 빈으로 등록하여 여러 테스트 코드에서 주입받아 사용할 수 있도록 한다. DataSource는 위에서 만든 프록시 객체 (CountDataSource)를 반환하도록 설정한다. 가능한 쉽게 코드를 설명할 수 있도록, 쿼리를 로깅하는 기능을 제외했다. 사용법 queryCounter.startCount(); // 카운트 시작
xxxrepository.findById();
xxxrepository.findById();
queryCounter.getCount(); // 개수 확인
queryCounter.endCount(); // 카운트 종료 및 리셋
사용법은 위와 같이 간단하다. 카운트 시작 쿼리 실행 카운트 결과 확인 카운트 종료 및 리셋 4번에서 다시 1번으로 가는 것도 가능하다. 사용 예시 쿼리 카운팅의 예시를 살펴본다. 캐싱 테스트 캐싱은 테스트하기 생각보다 까다롭다. 하지만, DB에 쿼리가 날라 갔는지만 확인가능하다면 쉽게 테스트할 수 있다. 실제 깃-들다에서는 캐싱을 테스트하기 위해서, QueryCounter를 사용한다. @DisplayName(""캐싱 - 비로그인 홈피드 조회시 두 번째 조회부터는 캐시 저장소에서 가져온다. (현재 페이지당 쿼리는 6번)"")
@Test
void readHomeFeed_Guest_LatestPosts() {
// given
... 게시물 작성 코드 ...

// when
queryCounter.startCount(); // 쿼리 카운트 시작
postFeedService.homeFeed(homeFeedRequestDto); // 첫번째 조회 (캐싱)

// 두번째 조회 (이때 DB에 쿼리를 날리지 않는다.)
List<PostResponseDto> postResponseDtos = postFeedService.homeFeed(homeFeedRequestDto);

// then
assertThat(queryCounter.getCount().getValue()).isEqualTo(6L); // 쿼리 카운팅을 통한 테스트
}
N + 1 문제 분석 이제 본격적으로 구현한 QueryCounter로 N + 1문제를 살펴봐보자. N + 1 문제 해결 전 이제 fetch join으로 N + 1을 해결해보고 다시 테스트를 돌려보면 아래와 같이 쿼리 개수가 1로 바뀐 것을 볼 수 있다. N + 1 문제 해결 후 위와 같이 쿼리 카운팅을 사용하면 N + 1 문제에 대한 분석이 훨씬 쉽다. 마치며 캐싱에 대한 테스트를 위해 쿼리 카운터가 필요하여 구현하다보니 이렇게까지 정리하게 되었다. 이해하고 정리하는데 꽤 힘들었지만, 그래도 그 과정에서 프록시와 다이내믹 프록시, JDBC, Hibernate등의 이해도가 깊어져서 좋은 경험이었다! 이후에도 이와 같이 객체지향을 사용한 커스텀을 더욱 많이 해보리~ 아! Thanks to 나봄 (I’m Spring) 그는 G.O.D Twitter Facebook Google+ # 백엔드 # 다이내믹프록시 # 쿼리카운팅 # 캐싱테스트",BE
493,"테스트코드 최적화 여행기 (4) 2021, Oct 07 안녕하세요 깃들다의 손너잘 입니다. 이 글을 시작하기 전에 저희 프로젝트의 도메인에 대한 설명이 필요할 것 같은데요… 그냥 인스타그램이라고 생각하시면 편합니다! 그러면 글 시작하겠습니다! 드디어 제가 적용한 마지막 테스트 최적화 방법에 대한 글을 작성하게 되었네요..! 마지막으로 제가 적용시킨 방법은 인수테스트의 조회용 테스트와 그 외 테스트를 분리하여 진행하는것입니다. 이게 무슨소리인가..? 하지요..? 일단, 이에 대한 자세한 설명을 하기 전에 인수테스트의 어느 부분이 문제였는지를 설명해 보겠습니다. 전체 테스트 시간을 봤을 때 가장 오랜 시간을 차지하는 테스트는 인수테스트 입니다. 실제로 HTTP 요청을 전송하여 테스트를 진행하기 때문에 그만큼의 딜레이가 발생하기 때문입니다. 따라서 인수테스트의 속도를 최대한 빠르게 만들면, 전체 테스트 시간도 비약적으로 줄어들것이라는 생각을 하게 되었습니다. 테스트 최적화와 관련된 이런저런 글도 읽고 유툽도 봤는데, 대부분 WebMvcTest로 대체하여 사용하거나 Mock을 사용하는 방식이 가장 제너럴 한 것 같았습니다. 하지만 저는 인수테스트에 RestAssured를 사용하는 팀의 컨벤션을 변경하고 싶지 않았습니다. 즉 RestAssured를 그대로 사용하면서 테스트시간을 최대한 줄여보고 싶었습니다. 그래서 전체 인수테스트를 분석해본 결과, 테스트의 시간을 크게 높이는 원인이 테스트 FIxture를 통해 환경을 구성하는 부분이라는 것을 알았습니다. 테스트를 진행하기 위해서는 먼저 Fixture를 정의하고 이를 이용해 테스트 환경을 만듦니다. 인수테스트에서는 실제로 HTTP 메서드를 통해 POST 요청을 보내는 방식으로 각 테스트 환경을 만들어내죠. 따라서 각 테스트마다 테스트 환경을 구성하기 위한 HTTP을 보내게 됩니다. 만약 테스트 환경이 복잡하고 그 환경을 이용하는 테스트가 많이 존재한다면, 각 테스트 마다 환경을 재구성하는데 많은 시간이 소요됩니다. 제가 테스트 시간을 줄일 수 있다고 생각한 부분이 바로 이부분 입니다. 스프링 테스트가 Application Context를 재활용하는것 처럼 테스트 환경도 재활용 할 수 있다면 어떨까요? 테스트 환경의 재사용 테스트 환경을 재사용 하기 위해서는 몇가지 제약사항이 필요합니다. 바로 상태가 변경되면 안된다는점이었습니다. 만일 특정 테스트로 인해서 테스트 환경의 상태가 변경된다면 다른 테스트에 영향이 일어나고 결국 테스트는 독립적이어야 한다 라는 원칙에 영향을 끼치게 됩니다. 하지만 한가지, 환경에 영향을 끼치지 않는 테스트가 있습니다. 바로 조회 관련 테스트 입니다. 조회 테스트는 HTTP GET 메소드를 사용합니다. 그리고 GET은 서버의 데이터에 아무런 영향을 끼치지 않는 안전한 메소드 임이 자명한 사실이죠. 그리고, 인수테스트에는 조회와 관련된 테스트가 상당히 존재한다는 사실도 알았습니다. 그래서 아래 그림과 같은 테스트환경을 생각해봤습니다. READ DB는 읽기만 가능한 DB이고, CUD(Create, Update, Delete) DB는 상태를 변경할 수 있는 DB입니다. 기본적으로 테스트를 진행하는데 보통 인메모리DB인 h2를 많이 사용합니다. 그리고 매 테스트마다 db에 데이터를 넣고, 테스트를 진행하고, 데이터를 지우는 방식으로 테스트를 진행합니다. 하지만 테스트를 위한 전체 데이터가 이미 저장된, 상태가 변하지 않는 DB가 있다면 어떨까요? 조회와 관련된 테스트는 해당 DB를 통해서 테스트를 진행할 수 있을겁니다. 위 그림의 READ DB가 이를 나타냅니다. 그러면 조회 관련 테스트에 한해서는 테스트 환경을 매번 만들 필요가 없을 것 입니다. 그래서 위와 같은 환경을 생각하며 테스트 코드를 리팩토링 하기 시작했습니다. 테스트 코드 분리 제가 이 작업을 진행하면서 가장 먼저 진행한 작업은 조회 관련 테스트를 추출하는 것 이었습니다. 위와같이 조회와 관련된 테스트를 모두 추출하였고 하나의 패키지로 묶어주었습니다. 이 작업을 진행한 이유는 다음과 같습니다. 각 조회 테스트에서 필요한 Fixture와 테스트 환경 파악 2개의 DB 환경을 구성하기 전에 실제로 위 가설이 실제로 효과가 있는지 확인해 보기 위함 위와 같은 이유로 조회 관련 테스트를 분리하고, DB의 초기화 로직을 주석처리 시켜주었습니다. 이를 통해 각 테스트의 DB상태가 유지될 수 있도록 했습니다(전 단계에서 테스트의 Configuration을 통일시켜놨기 때문에 동일한 Application Context에서 테스트가 진행되기 때문에 가능합니다.) 그리고 리팩토링은 위 패키지를 대상으로만 진행하였고 테스트의 통과 여부와 성능 측정도 위 패키지를 대상으로만 진행했습니다(어쩌피 이 최적화의 목적은 조회 관련 Acceptance Test테스트에 존재하니까요!). 지켜야 할 것 이 방법을 적용하기전에 스스로 제약사항을 몇가지 걸었습니다. 그것은, 인수테스트의 성질을 유지하자라는 것 입니다. 인수테스트는 시나리오 테스트 입니다. 인수테스트는 실제 사용자의 시나리오를 테스트합니다. 따라서 DB의 상태를 DML을 통해 직접 구성하지 않습니다. 만일 시나리오에 로그인을 해야한다는 조건이 있다면, 첫 요청에 한에서는 실제로 HTTP 요청을 전송하여 Fixture를 셋팅합니다. 만일 테스트 A에서 HTTP 요청으로 사용자 로그인을 진행하여 Access Token을 받아왔고, 테스트 B에서 로그인요청이 시나리오에 포함되어 있어 그 토큰을 에서 사용한다면, 테스트 B에 대한 시나리오를 제대로 수행했다고 말할 수 있을까요? 저는 그렇다고 결론을 내렸습니다. 이미 저장된 token을 사용했고 HTTP 요청을 날리지 않으니 아니라고 말할수도 있지만, 테스트 B에서 사용한 token은 동일한 서버로 부터 HTTP 요청으로 발급받은 token이기 때문에 이는 로그인이라는 시나리오를 휼륭이 수행한 상태라고 생각할 수 있습니다. 인수테스트는 문서의 목적이 있습니다. 인수테스트는 테스트를 읽을 수 있어야 한다고 생각합니다. 따라서 테스트 A에서 로그인을 수행하고 토큰을 이미 가지고 있다 하더라도, 테스트 B에서 로그인 시나리오를 진행할 때 로그인을 한다는 시나리오를 코드로서 표현할 수 있어야 한다고 생각합니다. 앞으로 진행되는 리팩토링은 위의 제약사항을 지키면서 작성하였습니다. 로그인 로직 재사용 가장 먼저 로그인과 관련된 리팩토링을 진행했습니다. 로그인 또한 API 호출이 필요하므로 이미 로그인 된 유저를 재사용 가능하게하면 각 테스트에서 로그인 관련 HTTP 호출을 제거하고 테스트 속도를 올릴 수 있을것이라 생각했습니다. 먼저 TUser라는 테스트용 유저 데이터를 만들었습니다. 유저는 깃들다 백엔드 팀원 5명의 이름으로 하였고 token를 필드로 가지고 있을 수 있도록 했습니다. 그리고 로그인요청 시 token이 없다면 새로운 HTTP 로그인 요청을 날리고, token이 있다면 token을 재사용합니다. 코드에서는 위와 같이 표현됩니다. 테스트 코드에 존재하는 모든 로그인 로직을 위 로직으로 변경한뒤 테스트를 다시 실행시켜봤습니다. 대략 14~15초가 소요되던 테스트가 대략 12~13초로 줄어들었습니다. 반복되는 로그인 부분을 제거함으로서 약 2초정도의 테스트 시간 단축이 가능해졌습니다. User의 Follow, Following 관계 구성 다음으로 진행한 부분은, 유저간의 Follow, Following 관계를 만드는 것 입니다. Follow, Following 관련 테스트를 하기 위해서 매 테스트 마다 유저를 새로 만들고, Follow, Following 관계를 새로 구성하는 코드가 테스트에 중복되어 있었습니다. 이 부분을 제거하기 위해서는 모든 테스트에서 공통으로 사용하는 Follow, Following 관계도가 필요했고 이는 아래와 같습니다. 총 5명의 유저와 위와 같은 팔로잉 관계를 맺도록 했습니다. 팔로우 관계는 코드로 위와같이 표현되며, 이 역시 이미 팔로우, 팔로워 관계면 실제 요청은 날리지 않는 식으로 만들었습니다. 이 작업을 완료한 뒤 테스트를 돌려보니 9~10초 사이의 시간이 소요되는것을 확인했습니다. 팔로우, 팔로잉 관계를 만들어 주는 코드가 제거됨에 따라 테스트 속도가 아주 만족스럽게 증가하였습니다! POST, LIKE, COMMENT 마지막으로 POST, LIKE, COMMENT를 정의해 보겠습니다. POST, LIKE, COMMENT또한 각 도메인의 검색, 조회등 조회에 대한 테스트를 할 때 마다 매번 Fixture를 생성하고 DB에 저장하는 요청을 보내는 요청이 중복됩니다. 3가지 도메인에 대한 작업을 동시에 진행한 이유는 특정 도메인에 대한 테스트가 아닌, 전체 인수테스트 시간을 측정하고싶어서입니다. LIKE와 COMMENT의 경우, POST와 모두 연관되어 있습니다. 따라서 만일 POST만 공통 요청 제거 작업을 진행하면 제거작업이 진행되지 않은 LIKE, COMMENT관련 테스트에 영향이 가면서 관련 테스트가 실패하게 됩니다. 따라서 POST와 함께 LIEK, COMMENT에 대한 정의를 한번에 모두 진행했습니다. 정의된 환경는 위와 같습니다. POST에 대해 LIKE와 COMMENT가 어떻게 달려있는지, 누가 POST를 생성했는지를 나타내고 있습니다. 위 그림을 정의하기 위한 TPost라는 Post Fixture를 정의하고 각 Post의 상태를 저장할 수 있도록 했습니다. 위에서 작성한 TUser처럼, 최초 Post 생성, 좋아요 표시, 댓글 달기 요청시에는 실제 HTTP 요청을 통해 데이터를 셋팅하고 DB의 상태가 어떻게 되어있는지는 객체에 표시하게 하였습니다. 그리고 동일한 요청이 들어올 경우 객체의 상태를 확인하여 이미 셋팅되어 있다면 HTTP요청을 하지 않도록 구성했습니다. 코드는 이런식으로 정의됩니다. 모든 환경는 위의 환경 그림에 의거하여 정의됩니다. DB의 상태가 항상 그림의 상태와 동일해야함을 약속했기 때문이죠. 엄청난 시간단축이 발생했습니다. 3초 후반에서 4초 초반대를 지속적으로 기록합니다. 처음 14~15초였던 테스트 시간에 비하면 정말 비약적인 발전을 이뤄냈습니다!. 이번 글에서는 인수테스트의 조회용 테스트와 그외 테스트를 분리하면 테스트의 시간이 줄어든다는것을 확인해 봤습니다. 마지막 다음 글에서는 실제로 2개의 DB환경을 구성하고 조회 테스트와 그 외의 테스트마다 서로 다른 DB를 사용하도록 하는 방법에 대하여 작성해 보겠습니다. 긴 글 읽어주셔서 감사합니다. Twitter Facebook Google+ # 백엔드 # test # 최적화",BE
494,"테스트코드 최적화 여행기 (3) 2021, Oct 07 우리는 처음에 Context를 재사용 하기 위해서 Dirties Context를 제거했습니다. 그리고 그를 통해 비약적인 테스트 속도 개선을 이뤄냈습니다. 그래서 저는 이제 모든 테스트가 동일한 Context를 돌려 사용한다고 생각했고, 안심하고 있었습니다. 그런데 전체 테스트를 진행했을때 이상한점을 찾았습니다. 테스트가 잘 진행되다가 중간중간에 한번씩 뚝,뚝, 걸리는 부분이 있는겁니다. 그래서 그 뚝 뚝 걸리는 테스트를 확인해 봤더니 왠걸… 새로운 Context를 Load하고 있었습니다. 왜 이런일이 발생하는가 소스를 비교하고 실험해본 결과, 스프링이 테스트에서 컨텍스트를 관리하는 방법과 관련이 있었습니다. 스프링은 기본적으로 테스트를 진행할 때 Context를 캐싱하여 여러 테스트에서 돌려 사용할 수 있도록 합니다(토비의 스프링3.1 vol.1). DirtiesContext를 제거한것도 이것과 관련이 있죠. 이때, 캐싱의 조건(?)를 생각해봐야합니다. 스프링 테스트은 설정단위로 Application Context을 캐싱합니다. 예시를 통해서 확인해보겠습니다. @DataJpaTest
public class DataJpaTestTest {

@Autowired
PostRepository postRepository;

@Test
void name() {
}
}
@SpringBootTest
public class SprintBootTestTest {

@Autowired
PostRepository postRepository;

@Test
void name() {
}
}
DataJpaTest를 4개, SpringBootTest를 4개 만들었습니다. 이를 실행시키면 총 몇개의 Application Context가 만들어질까요? 정답은 2개입니다. @DataJpaTest 는 DataJpaTest에 필요한 필수 Bean만 Context에 Load하고 SpringBootTest의 경우는 모든 Bean을 Load하게 됩니다. 즉, 둘의 Configuration 다르죠. 따라서 총 컨택스트는 2번 생성됩니다(동시에 2개가 생성되는게 아닙니다.) 이 이야기를 왜 했는가 하니, 깃들다의 테스트코드들의 중간에 잠깐씩 멈추던게 바로 위 문제때문이었습니다. 어떤 Integration test는 DataJpaTest로 작성되어 있고, 또 다른 어떤 Integration test는 다른 Integration test에서는 사용하지 않는 Configuration을 Import받고있는 등 Configuration이 통일성이 없는 부분이 있던거죠. 이러한 문제는 AcceptanceTest에도 동일하게 발생했습니다. 처음에는 테스트 상단에 있는 어노테이션들을 일치시키면 동일한 환경이 구성되면서 이 문제가 해결될것이라고 생각했습니다. 그러나 이는 착각이었습니다. 위의 상황뿐만 아니라, @MockBean 을 사용해도 컨텍스트를 새로 만들고, 목킹된 객체의 개수, 종류에 따라서도 컨텍스트를 새로 만들어냅니다. 또한 @import 에 따라서도 컨텍스트를 새로 만듦니다. 지금 생각해보면 당연한건데 처음에는 진짜 어질어질했습니다.. 지금까지 이런걸 생각하지 않고 테스트 코드를 짰는데 얼마나 많은 성능적 손해를 본건지.. 깃들다의 경우 OAuth를 통한 로그인을 구현하고 있기 때문에 로그인을 위해 @MockBean을 통해 로그인 관련 컴포넌트를 목킹시켜 사용하고 있었습니다. 따라서 위에서 말했던 모든 상황마다 새로운 Context를 만들어내면서 속도가 어마무시하게 느려졌습니다. 테스트를 느리게 만들었던 아이들 중 몇명… 이 문제를 확인하고 테스트의 Configuration을 통일시켜주는 작업을 진행했습니다. 정말 사소한것 하나로도 새로운 Context가 생성되는걸 보면서 한숨이 푹푹 나왔습니다… 심지어 WebMvcTest의 경우에도 매 테스트 클래스마다 컨텍스트를 따로 다 만들어내는것을 확인했습니다. Mock되는 객체들이 서로 다르니 어쩔 수 없겠지요.. 일단 최대한 묶을 수 있는 테스트는 묶어서 Configuration을 최대한 맞춰주었습니다. WebMcvTest의 경우는 Controller Test에 필요한 모든 Mock과 Controller class 파일들을 abstract class에 몰아넣고 모든 ControllerTest(MockMvcTest)가 이를 상속받도록 했습니다. 어쩌피 컨트롤러딴 테스트이기 때문에 서로 각자의 Service에 관여할일이 없어 모든 Service를 목킹 하여도 괜찮다고 생각했기 때문입니다. 이러한 방식으로 AcceptanceTest, DataJpaTest, WebMvcTest, IntegrationTest를 각각 하나의 Context를 이용해서 테스트 가능하도록 리팩토링 해 주었습니다. 결론적으로, 이 방식을 적용했을때나 안했을때나 Intellij에 찍히는 테스트 시간을 큰 차이가 없습니다. 하지만 이는 Intellij가 중간에 새롭게 실행되는 Application Context Load시간을 테스트 시간에 누적하지 않아서 발생한 문제입니다. 실제로 적용해 보시면 속도차이가 많이 발생한다는 사실을 확인할 수 있을 것 입니다. 이 글 시리즈의 제일 마지막에 GIF로 테스트 동작하는것을 녹화하여 올려놓았는데, 그것을 확인해보면 중간에 테스트가 잠깐씩 멈추는게 현저히 적은것을 확인할 수 있습니다(이 글에서 설명했듯, AcceptanceTest, DataJpaTest, WebMvcTest, IntegrationTest마다 Application Context를 새로 Load하기 위해총 4번 멈춥니다). 그리고 테스트가 전체적으로 매끄럽게 돌아가고요. 그런 결과가 가능했던 이유가 이번 글에서 설명한 이 설정 덕분이었습니다. 어떤 차이인지 궁금하시다면 잠시 테스트최적화여행기 5편의 결과를 보고 오시면 좋을 것 같습니다 🙂 지금까지 저희 팀이 얼마나 많은Configuration을 가진 테스트를 진행한건지.. 한번 테스트를 돌릴때 마다 얼마나 많은 Application Context가 생성된건지.. 제대로 신경써주지 못한 테스트코드에게 미안한 마음이 들었습니다. 마무리를 어떻게 지어야 할지 모르겠네요… 뭐, Spring 테스트의 Context 생성 조건을 잘 이해한다면 이런식으로도 테스트 시간을 줄일 수 있다는 사실을 알게 되어서 재미있었습니다. 그러면 다음에는 조회 관련 테스트를 분리하여 테스트 성능을 향상시키는 방법에 대해서 글을 작성해 보겠습니다. 읽어주셔서 감사합니다. Twitter Facebook Google+ # 백엔드 # test # 최적화",BE
495,"테스트코드 최적화 여행기 (5) 2021, Oct 07 안녕하세요 깃들다팀의 손너잘입니다. 드디어 마지막 글이네요! 저번 글에서는 인수테스트의 조회용 테스트와 그외 테스트를 분리하여 테스트 속도의 최적화가 가능함을 확인해 봤습니다. 이번글에서는 실제로 테스트에 이를 적용해 보도록 하겠습니다. 다중 데이터 소스와 DB 선택 전략 다중 데이터소스를 사용하는 자세한 방법에 대해서는 [링크]를 참고하여 주세요! 이전 글에서 저는 위와같은 테스트 환경을 구상했습니다. 이를 구현하기 위해서 2개의 h2를 띄우도록 하였습니다. 위와 같은 방식으로 read, wirte DataSouce bean을 생성하였습니다(이제보니 write db라는 말이 이상하네요.. 위 그림의 CUD DB가 write db라고 생각해 주시면 감사하겠습니다 🙂) 그리고 상황에 따라 특정 DataSource를 사용하도록 하기 위해서 LazyConnectionDataSourceProxy 을 활용했습니다. LazyConnectionDataSourceProxy 은 JDBC Connection을 가지고 오는 시점까지 DataSource의 사용을 지연시킵니다. 그렇다면, JDBC Connection을 가지고 오는 순간 어느 DataSource를 사용할것인지에 대한 선택방법만 정의해 주면 됩니다. 이는 AbstractRoutingDataSource 를 상속받은 클래스를 이용해 구성할 수 있습니다. 위와 같은 방식이죠, determinCurrentLookupKey에 DataSource 선택 전략을 작성해 주시면 되는데요, 저는 DataSourceSelecor라는 객체를 만들어서 이를 정의했습니다. toWrite()를 호출하면 selected필드를 write로, toRead()를 호출하면 selected 필드를 read로 변경하도록 하였고 getSelected()를 통해 상태를 가지고 올 수 있도록 하였습니다. 문자열을 return하는데 어떻게 DataSource가 선택되는지 궁금하실텐데 이에 대한 자세한 내용은 처음 알려드린 링크를 참조해 주세요! 이때, DataSourceSelector가 Component로 지정되어 있기 때문에 다른 객체에서 이 객체를 DI받아 자신이 사용할 DataSource를 선택할 수 있게 됩니다. 저같은 경우는 AcceptanceTest 추상화 클래스에 DataSourceSelector를 DI받고 위임 메서드를 만들어 인수 테스트 클래스에서 toRead(), toWrite()를 지정할 수 있게 하였습니다. 이를 통해 조회 관련 테스트의 경우 toRead(); 메서드를 호출하여 read DB를 사용할 수 있도록 하였습니다. 또한 DB는 WriteDB가 기본으로 선택되도록 하였습니다. DataBase clear로직은 dataSourceSelector가 Write 상태일때만 동작하게 하였으며, 모든 인수 테스트의 @AfterEach 에 toWrite()를 실행하도록 하여 각 테스트가 종료될 떄 마다 WriteDB를 사용하도록 다시 셋팅시켰습니다. 이렇게 한 이유는, 테스트의 실행 순서는 일정하지 않기 때문에 어느 시점에는 조회 관련 테스트가 돌아가고 어느 시점에서는 업데이트 관련 테스트가 돌아갈 수 있기 때문입니다. 따라서 조회 테스트시에만 readDB를 사용할것이다! 라고 명시시키고 다른 테스트에서는 기본적으로 WriteDB를 이용하도록 한 것 입니다. Table 셋팅 이제 조회 관련 테스트와 그 외 테스트가 서로 다른 DB를 사용할 수 있게 되었습니다. 여기서 문제가 발생하는데요, 테스트를 사용할때는 테이블을 자동으로 생성하기 위해 보통 Hibernate가 제공해주는 ddl-auto기능을 많이 사용하게 됩니다. 문제는 이 기능을 사용하게 되면 제가 기본으로 잡아놓은 WriteDB에만 테이블이 생성되고 ReadDB에는 테이블이 생성되지 않아 조회 관련 테스트가 다 실패하게 됩니다. 테이블이 자동 생성되는 시점은 EntityManagerFactory 가 build되는 시점입니다. EntityManagerFactory 를 빌드할때 ddl-auto 관련 속성을 명시하면 내부에서 Dialect가 쿼리를 만들어 DataSource를 통해 테이블을 생성하게 됩니다. 이때, DataSource에 제가 기본으로 설정한 DataSource가 WriteDB였고, EntityManagerFacotry 내부에서 Table을 정의하는 부분을 제가 컨트롤 할 수 없는것에 문제가 있었습니다. 때문에 ReadDB에 테이블을 정의할 방법을 생각하게 되었습니다. 그래서 결국 만든것이 SchemaGenerator라는 녀석입니다. 당연히 Hibernate 구현체 종속적으로 구현되어 있습니다. 간단히 말하면, 프로젝트의 Entity들을 전부 파싱해서 DDL을 만들어서 String으로 반환시켜주는 역할을 합니다. 이 소스에 대한 설명은 [링크]를 참조해 주세요! 이를 통해 DDL을 만들어줄 수 있었기 때문에 더이상 ddl-auto를 사용하지 않고 수동을 테이블을 정의할 수 있게 되었습니다. 그래서 과감하게 application-test.yml에서 ddl 자동생성 부분을 none으로 변경해 주었습니다. 이제 테이블을 셋팅 시켜보겠습니다. 제일 첫번째 글에서 table이름들을 추출했던 부분입니다(DatabaseCleaner 객체를 기억 하시나요 ㅎ). 이 컴포넌트는 InitializeingBean 를 상속하고 있기 때문에 ApplicationContext가 처음 초기화되는 시점에 afterPropertiesSet() 을 실행시키게 됩니다. 여기서 각 DB에 table을 create시켜줄 것 입니다. 이떄, ddl-auto: create 와 동일한 효과를 주기 위해서 처음에는 drop table을 진행하고 그다음 create table을 진행하도록 하였습니다. read db는 컨텍스트가 변경되더라도 상태를 유지할 수 있게 하도록 drop을 시키지 않았습니다. toRead()와 toWrite()가 무슨 함순지는 알고 계시니 어떤식으로 로직이 돌아가는지 이해 하실거라 생각합니다! readDB쪽에 SQLSyntaxErrorException 을 잡아놓은 이유는, 사실 저의 귀차니즘에 가깝지만… db에 테이블이 이미 존재하면 create()과정을 진행하지 않도록 하는 로직을 짜야하는데… 귀찮아서 이미 존재하는 테이블을 create하면 에러난다는 점을 이용해서…..(ㅈㅅ) 어찌되었든 이런식으로 두 데이터베이스에 모두 테이블을 셋팅시킬 수 있었습니다!! 결과!!! 드디어 대망의 테스트 속도 결과입니다!!! (두구두구두구두구) 비교군은, DirtiesContext를 제거한 테스트와 비교하였습니다. DirtiesContext가 존재하는 경우는 너무 느려서 제외했습니다… 뭐… 말이 필요할까요.. 엄청난 차이가 발생합니다..! 테스트 코드의 개수가 다른 이유는 각 기법을 적용하는 사이에 테스트코드의 정리가 일어나서입니다ㅜ 지표상으로는 7초의 차이가 발생했지만, GIF를 보시면 알수 있듯 실제로는 더 많은 속도차이가 발생합니다! 단점 그러면 이 3번째 방법은 무조건적으로 좋을까요? 아닙니다. 이 방법을 적용하는데에는 몇가지 단점이 존재합니다. 조회 관련 테스트를 작성하는 개발자가 DB의 상태를 알고 있어야 한다 조회 관련 테스트의 경우는 미리 정의된 데이터들을 이용하여 테스트를 진행합니다. 따라서 이전 글에서 그린 그림과 같이 Fixture들이 어떤 식으로 정의되어 있는지 이해한 상태에서 코드를 작성해야하는 어려움이 있습니다.(하지만 적절한 문서화를 통해 해소할 수 있겠죠?) 새로운 도메인이 추가되었을 때 조회 테스트를 위한 환경을 조심히 구성해야 한다. 새로운 요구사항이 생기고, 그에 따라 새로운 도메인이 추가되거나 삭제될 경우 다른 조회테스트에 영향이 갈지 안갈지를 잘 생각해서 테스트 환경을 새로 구성해야 합니다. 그렇다면 이 방법을 적용하는건 실질적으로 불가능 할까요? 저는 아니라고 생각합니다. 요구사항이 많이 일어나고 변화가 많은 프로젝트라면 적용하기 힘들겠지만 리팩토링을 진행해야하는 레거시 프로젝트나 작은 규모의 프로젝트에서 적용되면 큰 효과를 얻을 수 있을 것이라 생각됩니다. 빠른 테스트는 리팩토링에 다가가는 첫 걸음 이니까요🙂 이를 통해 깃-들다의 테스트 코드 최적화 여행기를 모두 마무리 하였습니다. 조금이라도 얻어가는게 있었으면 하는 마음이네요.. ㅎㅎ 긴 글 읽어주셔서 감사합니다. 깃들다팀의 손너잘이었습니다. Twitter Facebook Google+ # 백엔드 # test # 최적화",BE
496,"OAuth 2.0 흐름 2021, Oct 07 안녕하세요, 다니입니다. 🌻 Workflow 사용자는 Pick-Git에서 Github Login에 필요한 자원에 접근합니다. Github Login URL(client_id, redirect_url, scope 포함)을 응답합니다. 사용자는 로그인 버튼을 클릭하여 Github Login 페이지로 이동합니다. Github은 사용자에게 Login URL에 명시한 scope에 대한 정보 제공 동의 허용 여부를 물어봅니다. 사용자가 동의하고 로그인에 성공합니다. Github은 사용자에게 Authorization Code를 발급합니다. 사용자는 Pick-Git에게 Authorization Code를 전달합니다. Pick-Git은 Github에게 client_id, client_secret, Authorization Code를 전송합니다. Github은 전달받은 데이터를 검증하고, Pick-Git에게 Github Access Token을 발급합니다. Pick-Git은 Github Access Token을 이용하여 Github에게 해당 사용자 프로필 정보를 요청합니다. Github은 해당 Access Token을 검증하고, Pick-Git에게 사용자 프로필 정보를 응답합니다. Pick-Git은 사용자 이름을 payload로 Pick-Git Access Token을 생성하고, 이를 { Pick-Git Access Token : Github Access Token } 형식으로 저장하고, 사용자에게 Pick-Git Access Token을 전달합니다. References OAuth2 동작 흐름에 대해서 알아보자 OAuth 2.0 What the Heck is OAuth? (썸네일) Twitter Facebook Google+ # 백엔드 # OAuth 2.0 # Flow",BE
497,"프록시를 이용한 Http Method 포함 인터셉터 설정기 2021, Oct 04 목차 목차 개요 커스텀을 하게 된 이유 스프링MVC는 어떻게 인터셉터를 매칭시키는 것인가? 프록시 프록시를 이용한 인터셉터 URL Matcher 커스텀 끝으로 코드 들어가며 안녕(하세요) 마크입니다 :) 이번 글은 프록시를 이용한 Http Method 포함 인터셉터 매칭 설정기이다. 우선 인터셉터란 스프링 MVC에서 제공하는 서블릿 필터와 비슷한 역할을 하는 녀석이다. 핸들러(Controller)를 실행하기 전, 후 (아직 랜더링 전) 그리고 완료 (랜더링까지 끝난 이후) 시점에 부가 작업을 하고 싶은 경우에 사용할 수 있다. 특정 핸들러에 횡단 관심사를 분리시킬 수 있어 유용하게 사용된다. 하지만 interceptor register는 특정 핸들러에 특정 인터셉터를 매칭시키는 방법은 아래와 같이 URL로 하는 것이 유일하다. 왜 URL로만 설정가능할까? URL + HTTP Method를 통해 설정하면 얼마나 좋아? 라는 생각에 커스텀을 해보고자 이번 글을 작성하게 됐다. 이번 글은 HTTP Method를 포함하도록 한 이유를 언급하고, 프록시(정확히는 데코레이터)를 활용하여 인터셉터 URL Matcher 방법을 다룬다. 시간이 금이다 하시는 분들은 프록시를 이용한 인터셉터 URL Matcher 커스텀만 봐도 될 듯하다. ㅎㅎ 커스텀을 하게 된 이유 필자는 현재 우아한 테크코스 3기 과정을 진행하고 있다. 그리고 팀 프로젝트로 Github Repo 기반 개발 장려 SNS를 개발하고 있다. 필자가 처음 맡은 파트는 로그인과 OAuth이었고, 스프링 시큐리티를 사용하지 않고 Interceptor와 HandlerMethodArgumentResolver를 이용했다. 시큐리티를 사용하지 않은 이유는 두 가지다. 처음 배우는데 시간이 소모된다. 시큐리티는 추상화가 잘 되어있지만, 그만큼 로그인이나 OAuth 관련된 깊이있는 학습이 어려울 것 같았다. Interceptor는 토큰 검증, HandlerMethodArgumentResolver는 토큰 내용 추출하는 역할을 담당한다. 문제는 특정 API의 GET요청은 로그인/비로그인이 상관 없지만, POST요청을 로그인인 상태여야 했다. 예를 들어, GET /api/posts: 로그인/비로그인 상관 없음. POST /api/posts: 꼭 로그인이여야 함. 같은 URL이지만, Method를 기반으로 인터셉터를 분기시켜야했다. GET요청은 IgnoreAuthenticationInterceptor (토큰 검증 여부 상관 X) POST요청은 AuthenticationInterceptor (토큰 검증 안되면 401 던짐) 기존의 인터셉터는 URL만으로 설정이 되기 때문에 아래와 같이 인터셉터안에 Method를 구분하는 코드가 추가되어야했다. 문제는 이와 같은 API가 점점 많아진다는 것이었다. 많아지면 많아질수록 두 인터셉터안에서 Http Method기반으로 분기되는 코드를 직접 넣어줘야했다. // 실제 이전에 사용했던 코드
@Override
public boolean preHandle(HttpServletRequest request, HttpServletResponse response,
Object handler) throws Exception {
// 문제의 Http Method 설정 코드
if (!isGetRequest(request)) {
throw new InvalidTokenException();
}
if (!isPutRequest(request)) {
....
}

// ... Interceptor 코드 (토큰 검증 로직)
return true;
}
즉, 인터셉터는 토큰 검증 역할만을 담당하는 친구인데, Http Method 매칭 역할도 맡게 된 것이다. 그리하여 이를 분리시키는 커스텀을 하게 되었다. 스프링MVC는 어떻게 인터셉터를 매칭시키는 것인가? 우선 커스텀을 하려면 기존 로직에 대한 이해가 필요하다. 스프링MVC는 다음과 같이 매번 요청때마다 PathMathcer의 구현체인 AntPathMatcher를 사용하여 인터셉터를 매칭한다. AntPathMatcher 더 정확히 말하면 요청이 들어올 때마다 DispatcherServlet의 HandlerMapping에서 미리 InterceptorRegistry에 설정한 인터셉터를 가져와서 HandlerExecutionChain에 인터셉터를 add하는 형식이다. InterceptorRegistry의 인터셉터는 스프링이 켜지면서 WebMvcConfigurationSupport의 의해서 ApplicationContext에 등록되는 형식인 듯 하다. InterceptorRegistry에 미리 설정한 인터셉터 설정 코드 HandlerMapping의 HandlerExecutionChain에서 인터셉터 add하는 코드 그리고 DispatcherSevlet에서 Chaining형식으로 실행된다. (true면 다음 인터셉터, false면 체이닝 종료) 그렇다면 Http Method를 어떻게 추가해야할까?? 처음 고안한 방법은 인터셉터 설정을 할 때 사용되는 InterceptorRegistration를 오버라이딩하거나 새로 만드는 것이었다. 하지만, 위 코드는 스프링MVC의 코드이며, 리팩토링하는 방법을 찾을 수 없었다.. 팀원들과 여러 토론끝에 찾아낸 방식은 스프링 코드를 건드리지 않고 프록시를 이용한 방법이다. 프록시 프록시를 적용하기 전에 프록시에 대한 사전 지식을 알아보자. 🤔 프록시 프록시 자신이 클라이언트가 사용하려고 하는 실제 대상인 것처럼 위장해서 클라이언트의 요청을 받아주는 것을 의미한다. 대리자, 대리인 타깃 (실체) 프록시를 통해 최종적으로 요청을 위임받아 처리하는 실제 객체를 타깃이라 한다. (핵심 비즈니스) 프록시의 기능 타깃과 같은 메서드를 구현하고 있다가 메서드가 호출되면 타깃 객체로 위임한다. (위임) 지정된 요청에 대해서는 부가기능을 수행한다. (부가기능 수행) 프록시 특징 위임 부가기능 외의 나머지 모든 기능은 원래 핵심기능을 가진 클래스로 위임해줘야 한다. (부가기능에서 핵심로직에게 위임) 부가 기능이 핵심 기능을 사용하는 구조 핵심기능은 부가기능을 가진 클래스의 존재 자체를 모른다. 따라서 부가기능이 핵심기능을 사용하는 구조가 된다. 모두 핵심로직(타깃) 인터페이스를 구현해야한다. 클라이언트 입장에선 인터페이스만 보고 사용하기 때문에 핵심 기능을 가진 객체를 사용할 것이라고 기대한다. 하지만 사실은 부가기능을 통해 핵심기능을 이용하는 것. 그러기위해선 부가로직도 핵심로직 인터페이스를 구현해줘야한다. 이렇게해야 부가기능과 핵심로직의 순서를 자유자재로 변경해줄 수 있다. 프록시가 타깃을 제어할 수 있는 위치다. 프록시 사용목적 클라이언트가 타깃에 접근하는 방법을 제어하기 위함 (프록시 패턴) 타깃에 부가적인 기능을 부여해주기 위함 (데코레이터 패턴) 이번 커스텀할 때는 데코레이터 방식을 사용했다. 프록시를 이용한 인터셉터 URL Matcher 커스텀 그렇다면 프록시를 이용해 어떻게 Http Method도 포함하도록 설정할까?? 기존의 스프링 코드를 최대한 건드리지 않는 방향으로 생각한 방법은 API Matcher 역할을 하는 프록시 인터셉터를 만들어주는 것이다. 기존의 매칭 방식과 커스텀한 매칭 방식을 그림을 통해 비교해보자. 기존의 매칭 방식 (URL) 커스텀한 매칭 방식 (URL + Method) 쉽게 얘기해서 POST /api/posts로 설정해두면 GET으로 요청오는 것을 PathMatchInterceptor에서 걸러주는 것. PathMatchInterceptor: 프록시 객체 AuthenticationInterceptor: 타깃 객체 이제 코드를 통해 보자. // PathMatchInterceptor
public class PathMatchInterceptor implements HandlerInterceptor {

private HandlerInterceptor handlerInterceptor; // 타깃 인터셉터
private HashMap<String, List<HttpMethod>> includeRegistry;
private HashMap<String, List<HttpMethod>> excludeRegistry;
private PathMatcher pathMatcher; // URL용 AntMatcher

public PathMatchInterceptor(HandlerInterceptor handlerInterceptor) {
this.handlerInterceptor = handlerInterceptor;
this.pathMatcher = new AntPathMatcher();
this.includeRegistry = new HashMap<>();
this.excludeRegistry = new HashMap<>();
}

public PathMatchInterceptor addPathPatterns(String pattern, HttpMethod... methods) {
return addPathPatterns(pattern, Arrays.asList(methods));
}

public PathMatchInterceptor addPathPatterns(String pattern, List<HttpMethod> methods) {
this.includeRegistry.putIfAbsent(pattern, methods);
return this;
}

public PathMatchInterceptor excludePatterns(String pattern, List<HttpMethod> methods) {
this.excludeRegistry.putIfAbsent(pattern, methods);
return this;
}

public PathMatchInterceptor excludePatterns(String pattern, HttpMethod... methods) {
return excludePatterns(pattern, Arrays.asList(methods));
}

@Override
public boolean preHandle(HttpServletRequest request, HttpServletResponse response,
Object handler) throws Exception {
String requestUrl = request.getRequestURI();
String requestMethod = request.getMethod();

// exclude 확인
for (String url : excludeRegistry.keySet()) {
if (pathMatcher.match(url, requestUrl) && isMethodMatch(requestMethod, url, excludeRegistry)) {
return true; // 타깃 실행하지 않고 다음 체이닝으로 이동
}
}

// include 확인
for (String url : includeRegistry.keySet()) {
if (pathMatcher.match(url, requestUrl) && isMethodMatch(requestMethod, url, excludeRegistry)) {
return handlerInterceptor.preHandle(request, response, handler); // 타깃 인터셉터 실행
}
}

return true;
}

private boolean isMethodMatch(String requestMethod, String url, HashMap<String, List<HttpMethod>> registry) {
return registry.get(url)
.stream()
.map(Enum::name)
.anyMatch(method -> method.equals(requestMethod));
}
}
위 코드에서 알 수 있듯이 exclude의 우선순위를 더 높도록 했다. // Interceptor 설정
@Override
public void addInterceptors(InterceptorRegistry registry) {
HandlerInterceptor authenticationInterceptor = new PathMatchInterceptor(authenticationInterceptor())
.addPathPatterns(""/api/posts"", HttpMethod.POST);

HandlerInterceptor ignoreAuthenticationInterceptor = new PathMatchInterceptor(ignoreAuthenticationInterceptor())
.addPathPatterns(""/api/posts"", HttpMethod.GET)

registry.addInterceptor(authenticationInterceptor)
.addPathPatterns(""/**"");

registry.addInterceptor(ignoreAuthenticationInterceptor)
.addPathPatterns(""/**"");
}
전체적인 동작 원리는 다음과 같다. API Matcher 역할을 하는 PathMatchInterceptor를 만들어준다. 이 인터셉터는 모든 요청(/**)에 대해서 체이닝이 걸리도록 설정해준다. PathMatcherInterceptor의 타깃으로 실제 사용될 인터셉터를 설정해준다. (생성자를 통해) 타깃 인터셉터가 체이닝 될 include, exclude URL과 Http Method를 설정해준다. 사용자 요청시 PathMatcherInterceptor를 거쳐 실제 인터셉터가 실행해도 되는지 URL + Method 기반으로 판단한다. 끝으로 객체지향 공부를하며 배웠던 내용을 활용해볼 수 있어서 좋은 경험이었다. 아직까진 큰 문제없이 커스텀한 코드를 사용중이다! 문제가 있다면 추후에 다시 포스팅 할 예정. 아마 더 좋은 방법이 존재하리라 생각든다. 추후에 더 리팩토링 할 예정!! 코드 PathMatchInterceptor OAuth 설정 코드 Twitter Facebook Google+ # 백엔드 # Spring # Interceptor # Proxy",BE
498,"NGINX 로드 밸런싱 (무료 헬스체크를 곁들인) 2021, Oct 04 목차 목차 개요 준비물 NginX 컴파일 설치 with Module NginX 설정 로드 밸런싱 및 헬스 체크 설정 NginX 실행 참고 들어가며 NginX는 대표적으로 리버스 프록시와 로드 밸런서 역할을 담당할 수 있는 웹 서버이다. 로드 밸런싱을 구현하면 꼭 같이 구성해야하는 기능이 있다. 바로 ‘헬스 체크’다. ‘헬스 체크’란 간단히 말하면 로드 밸런싱 서비스를 제공하는 다수의 서버의 상태를 점검하기 위한 기술이다. 쉽게 말해 WAS들의 건강을 체크하는 기술이다. NginX는 헬스 체크 기능을 간편히 설정할 수 있도록 해준다. 하지만 이 설정을 하기위해선 NginX Plus를 사용해야 하는데.. 바로! 유료버전이다… 여기를 가면 유료버전이라고 나와있다. 대체하는 방법이 있나 서칭해보니 무료로 사용가능한 모듈이 존재했다. 이 글은 NginX에 무료로 사용가능한 헬스 체크 모듈 (nginx_upstream_check_module)를 설치하고 설정하는 방법에 대해서 다룬다. 환경 Ubuntu 18.14 LTS NginX 1.14.2 준비물 로드 밸런싱과 헬스 체크 기능을 테스트하기위해 NginX서버와 두 개의 WAS를 구성해보자. NginX 1대 WAS 2대 본 글은 AWS EC2 기준으로 작성되었습니다. (모든 EC2의 한 VPC안에 존재시키며, Private IP를 통해 통신합니다.) 또한, 각각의 WAS에는 Spring기반의 프로젝트는 실행해둔다. (아무 프로젝트를 실행하면 된다.) 필자는 Spring Petclinic를 사용했다. NginX 컴파일 설치 with Module NginX를 쉽게 설치하는 방법으론 자동인스톨(ex apt)와 Docker 컨테이너가 있다. 참고 하지만 위와 같은 방법은 특정 모듈을 같이 설치하지 못한다. 다시 말해, 특정 모듈은 컴파일 설치를 통해서만 설치 가능하다. 헬스 체크 모듈을 사용하기 위해선 NginX를 컴파일 설치해야하는 것이다. 공식 모듈 DOCS는 여기를 참고하세요. 1. 설치 파일 다운로드 헬스 체크 모듈을 깃허브에서 clone해온다. $ git clone https://github.com/yaoweibin/nginx_upstream_check_module.git
NginX 다운로드 사이트에서 컴파일 버전도 다운받아준다. (본 글에선 1.14.2) 그리고 압축을 풀어준다. $ wget 'http://nginx.org/download/nginx-1.14.2.tar.gz'
$ tar -xzvf nginx-1.14.2.tar.gz
2. NginX 모듈 path 압축 푼 NginX디렉토리에 들어가서 아래와 같이 헬스 체크 모듈을 patch 해준다. (NginX버전에 맞는 *.patch파일을 설정해주면 된다.) $ cd nginx-1.14.2/
$ patch -p1 < {$module source directory}/check_1.14.0+.patch
patch 결과 3. 설정하기 위한 의존성 설치 이제 설치하기 위한 configure를 해야한다. 이때 필요한 의존성이 있다. PCRE -> apt install libpcre3-dev NginX는 Perl5에서 사용하는 정규표현식 라이브러리인 PCRE를 사용한다. openssl -> apt install openssl, sudo apt install libssl-dev https 모듈인 HttpSslModule을 사용하기 위해서 설치해줘야 한다. zlib -> apt install zlib1g, apt install zlib1g-dev ngx_http_gzip_module 모듈을 사용하기 위해서 설치하는 zlib 라이브러리. 컴파일 -> apt install make 경우에 따라 gcc와 g++를 설치해야할 수도 있다. 4. 컴파일 설치 전 configure NginX 소스 디렉토리로 이동하여 configure 명령을 실행한다. configure의 설정은 공식 문서에서 쉽게 파악할 수 있다. $ ./configure --add-module={$module source directory} --with-http_ssl_module
설정 결과 5. 컴파일 설치 이제 마지막으로 컴파일 설치를 진행해주면 된다. $ make
$ make install
이제 /usr/local/nginx를 가면 설치된 nginx를 찾을 수 있다. conf: 설정 파일 html: index와 500에러 관련 html (정적) logs: 로그 디렉터리 (기본) sbin: nginx 실행 파일 디렉토리 NginX 설정 이제 헬스 체크 모듈을 포함한 NginX를 설치하였으니, 아래와 같이 설정해주면 된다. worker_processes auto;

events {
worker_connections 1024;
}

http {
include ./mime.types;

log_format main '$remote_addr - $remote_user [$time_local] ""$request"" '
 '$status $body_bytes_sent ""$http_referer"" '
 '""$http_user_agent"" ""$http_x_forwarded_for""';

access_log ./logs/access.log main;

upstream app {
server 192.168.1.126:8000;
server 192.168.1.110:8000;

check interval=3000 rise=2 fall=5 timeout=4000 type=http;
check_http_send ""HEAD / HTTP/1.0

"";
check_http_expect_alive http_2xx http_3xx;
}

server {
listen 80;

location / {
proxy_pass http://app;
}

location /status {
check_status;

access_log off;
}
}
}
로드 밸런싱 및 헬스 체크 설정 upstream app {
server 192.168.1.126:8000;
server 192.168.1.110:8000;

check interval=3000 rise=2 fall=5 timeout=4000 type=http;
check_http_send ""HEAD / HTTP/1.0

"";
check_http_expect_alive http_2xx http_3xx;
}
app이란 이름으로 서버 클러스터 설정. 두 대의 WAS로 부하 분산 이외에도 로드 밸런싱 설정 공식 문서에서 부하 분산 알고리즘등을 설정해줄 수 있다. 헬스 체크 3초 단위로 체크, 두 번 성공하면 정상, 5번 실패하면 비정상으로 판단한다. (타임 아웃 4초) http 프로토콜 사용 서버에 HEAD 메서드, HTTP 1.0을 사용하여 보냄. HTTP 응답 코드가 2xx이거나, 3xx라면 정상 상태로 판단. NginX 실행 NginX 실행 공식 문서 $ ./sbin/nginx -t
$ ./sbin/nginx -c ./conf/nginx.conf
-t를 통해 설정 파일을 테스트한다. -c를 통해 설정 파일 위치를 지정해주고 실행해주면 끝! /status?format=json으로 보내면 json형식으로 반환해준다. 마치며 개인적으론 처음엔 막막했지만.. 그래도 적용시키는 과정에서 리눅스와 NginX에 대해 배울 점이 많아서 좋았습니다! 이제 운영 환경에서 로드 밸런싱에 연결된 WAS들이 잘 운영되고 있는지 한 눈에 확인할 수 있게 되었습니다 :) 참고 https://github.com/yaoweibin/nginx_upstream_check_module https://opentutorials.org/module/384/4511 https://m.blog.naver.com/sehyunfa/221707853050 Twitter Facebook Google+ # 백엔드 # NginX # HealthCheck # LoadBalancing",BE
499,"나만 몰랐었던 fetch join에 별칭을 쓰지 않는 이유 2021, Oct 06 Intro JPA의 fetch join 사용시 별칭을 쓰면 안되는 이유가 무엇인지 알아본다. 프로젝트애서 fetch join 시 별칭 사용에 대해서 고민해본다. fetch join 별칭은 왜 안될까 ? fetch join에서 별칭이 안되는 이유는 데이터의 일관성이 깨지기 때문이다. 예를 들어서 다음과 같은 코드는 fetch join 대상에 조건문이 들어가서 일관성이 깨진 경우이다. @DisplayName(""fetch join 대상에 조건문을 걸었을 때 데이터가 불일치하다."")
@Test
void findTeamWithSpecificNameMember() {
// given

// 데이터 삽입
Team teamA = new Team(""teamA"");

Member memberA1 = new Member(""memberA1"");
Member memberA2 = new Member(""memberA2"");
Member memberA3 = new Member(""memberA3"");

List<Member> teamAMembers = List.of(memberA1, memberA2, memberA3);

teamA.setMembers(teamAMembers);
teamRepository.save(teamA);

testEntityManager.flush();
testEntityManager.clear();

// 데이터 조회
Team teamA = teamRepository.findById(1L).orElseThrow();
int teamAMemberSize = teamA.getMembers().size();
testEntityManager.clear();

// when
Team teamAWithMemberName = teamRepository.findTeamWithSomeMemberByName(""memberA1"");

// then
/* 본래 teamA에 3명의 멤버가 들어가있지만 fetch join 대상에 where문이 들어가면서 데이터 불일치가 일어났다.
* collection 에는 관련 데이터가 모두 들어가있기를 기대하는데 그렇지 않다.
* 따라서 fetch join 대상에 필터링 조건을 거는 것을 지양한다.
*/
assertThat(teamA.getName()).isEqualTo(teamAWithMemberName.getName());
assertThat(teamAMemberSize).isNotEqualTo(teamAWithMemberName.getMembers().size());
}
TeamA에 대한 member collection 은 본래 3개이다. 그리고 fetch join을 하면 연관된 데이터가 모두 들어올 것이라고 가정한다. 하지만 위와 같이 fetch join 대상에 별칭을 주어 where 필터링 조건을 사용하면 실제로 TeamA에 연관된 멤버는 3명이지만 memberA1만 연관 데이터로 들어온다. DB의 상태에 대한 일관성이 깨진다. 하지만 예외는 있다 일관성을 해치지지 않는 한에서 성능에 도움이 된다면 예외적으로 사용해도 된다. (아마도 하이버네이트가 별칭을 허용하는 이유…) 예를 들어 다음과 같은 쿼리는 일관성을 해치지 않는다. select m from Member m join fetch m.team where t.name = :teamName
하지만 위의 쿼리가 left join fetch로 되면 일관성이 깨진다. (Team이 null이 아닌 Member에 대해서 null 값이 들어가기 때문이다.) 때문에 매우 조심스럽게 사용해야한다. 우리 프로젝트에 있는 별칭은?! 깃들다 프로젝트에도 fetch join 대상에 별칭을 사용하는 부분이 있다. 다음 포스트에 어떤 상황이었는지 배경 설명이 자세하게 되어있다. @Query(""select distinct p from Post p left join fetch p.likes.likes l left join fetch l.user where p.id = :postId"")
Optional<Post> findPostWithLikeUsers(@Param(""postId"") Long postId);
Post 안에는 해당 게시물을 좋아요한 유저들 정보를 담은 Like 리스트가 담겨있다. @Entity
public class Post {

//....

@OneToMany(
mappedBy = ""post"",
fetch = FetchType.LAZY,
cascade = CascadeType.PERSIST,
orphanRemoval = true
)
private List<Like> likes;

//...
}

public class Like {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""post_id"")
private Post post;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""user_id"")
private User user;

//...
}
위 쿼리를 살펴보면 별칭이 p.likes.likes l에 사용된다. where 조건문에는 fetch join 대상을 필터링 하지 않는다. 따라서 데이터 일관성을 헤치지 않는다. fetch join을 할 때 주의해야하는 부분은 collection을 여러개 fetch join 할 경우이다. 위 같은 경우는 post -> like 관계는 OneToMany라서 한번까지 fetch join 할 수 있다. like -> user는 ManyToOne 관계 이므로 추가 fetch join을 할 수 있었다. 마무리 처음에 버그를 마주하고 fetch join 대상에 별칭을 두는 것이 찝찝했지만 왜 안되는지 모르는 상태로 (나만) 넘어갔다. 검토해보니 fetch join 대상이 아니었으며 여러 collection을 fetch join 하는 상황도 아니었다. 하지만 이런 예외적인 경우는 자세히 알아보고 주의해서 사용해야 할 것 같다. 또 왜인지 모르고 그냥 안쓰지는 말자. 백엔드 코다입니다 🙌 Twitter Facebook Google+ # 백엔드 # Database # Jpa",BE
500,"테스트코드 최적화 여행기 (2) 2021, Oct 07 안녕하세요 깃들다팀의 손너잘 입니다. 테스트 최적화의 두번째 글을 작성하게 되었습니다. 이전 글에서 작성하였듯, 첫번째로 할 것은 DirtiesContext의 제거입니다. 많은 프로젝트에서 매번 테스트의 환경을 초기화 시키기 위해 DirtiesContext를 사용합니다. 하지만 DirtiesContext는 스프링 테스트가 매 테스트마다 Application Context를 다시 Load하도록 합니다. 이는 스프링 테스트가 제공해주는 Application Context의 캐싱의 이점을 전혀 가져가지 못합니다. 깃들다팀 또한 처음 프로젝트를 시작했을 떄 테스트의 환경을 초기화해주기 위해(정확히는 인메모리 DB초기화를 위함이지만요) DirtiesContext를 사용했습니다. 심지어 BEFORE_EACH_TEST_METHOD 를 사용하였죠… 프로젝트 초반에는 큰 문제가 없었으나 프로젝트가 점점 커질수록 테스트코드가 많아지고 문제가 발생했습니다. 무려 테스트를 한번 돌리는데 3분이 넘는 시간이 소요되는 문제였습니다.. 수치상으로는 3분이 찍혔지만 중간중간에 Application Context가 재시작 되는 시간이 누산되지 않아서 실제 시간은 거의 4분~5분이 소요됐습니다. DirtiesContext 제거 이를 해결하기 위해, 가장 먼저 DirtiesContext를 제거하여 Application Context를 재활용 할 수 있도록 하였습니다. 모든 AcceptanceTest의 DirtiesContext를 제거하고 공통으로 사용하는 어노테이션들을 추상클래스에 몰아넣어 테스트 클래스들이 이를 상속받도록 하였습니다. 하지만 여기서 문제가 발생합니다. Application Context가 공유되다보니 DB의 초기화작업이 필요하게 된 것 입니다. 참 많은 고민을 했습니다. @Sql 을 사용해 볼까?도 생각 해봤고, ApplicationTest 추상 클래스에 @BeforeEach에 제거 쿼리를 작성하여 초기화 할까도 생각했지만 이러한 방법에는 여러가지 단점이 있었습니다. 테이블의 외례키 제약조건에 따라서 테이블의 삭제 순서를 정해줘야 한다 Entity의 변경사항을 자동으로 반영하지 못하고 변경이 발생할떄 마다 삭제 쿼리를 매번 수정해줘야한다. 이러한 고민을 하던 중 우아한테크코스의 브라운 코치님에게 힌트를 얻어 아래와 같은 클래스를 작성했습니다. @Component
@ActiveProfiles(""test"")
public class DatabaseCleaner implements InitializingBean {

@PersistenceContext
private EntityManager entityManager;

private List<String> tableNames;

@Override
public void afterPropertiesSet() {
entityManager.unwrap(Session.class)
.doWork(this::extractTableNames);
}

private void extractTableNames(Connection conn) throws SQLException {
List<String> tableNames = new ArrayList<>();

ResultSet tables = conn
.getMetaData()
.getTables(conn.getCatalog(), null, ""%"", new String[]{""TABLE""});

while(tables.next()) {
tableNames.add(tables.getString(""table_name""));
}

this.tableNames = tableNames;
}

public void execute() {
entityManager.unwrap(Session.class)
.doWork(this::cleanUpDatabase);
}

private void cleanUpDatabase(Connection conn) throws SQLException {
Statement statement = conn.createStatement();
statement.executeUpdate(""SET REFERENTIAL_INTEGRITY FALSE"");

for (String tableName : tableNames) {
statement.executeUpdate(""TRUNCATE TABLE "" + tableName);
statement.executeUpdate(""ALTER TABLE "" + tableName + "" ALTER COLUMN id RESTART WITH 1"");
}

statement.executeUpdate(""SET REFERENTIAL_INTEGRITY TRUE"");
}
}
소스를 설명하자면 다음과 같습니다. @Component와 InitializingBean을 통해 Application Context가 이 컴포넌트를 최초 로딩할 때 afterPropertiesSet() 의 소스를 수행하게 합니다. EntityManager를 받아와서 db의 테이블 정보를 받아오는 쿼리를 통해 table 이름을 저장합니다. 사실 살펴보면 별거 없습니다. 여기서 재미있는점은 SET REFERENTIAL_INTEGRITY FALSE 를 통해 무결성 제약조건을 잠시 OFF 시켜서 테이블의 제거 순서를 신경쓰지 않아도 되게한다는점 입니다. 그러면 이 컴포넌트를 어떻게 이용할 수 있을까요? 저같은 경우는 위에서 만든 AcceptanceTest 추상 클래스에 DatabaseCleaner 클래스를 DI받아서 @AfterEach 에서 db clear작업을 진행해 주도록 하였습니다. 그러면 결과를 한번 볼까요? 3분이 넘게 걸리던 시간이 17~18초 정도로 줄어들었습니다! 엄청난 성과입니다. 깃들다 팀의 경우 인수테스트만 100개가 넘어가는데 이를 모두 새로운 ApplicationContext를 띄워서 테스트 하려고 하니 엄청난 시간이 걸렸던 것 입니다. 이번글에서는 DirtiesContext의 제거와 DB초기화에 대한 이야기를 나눠봤습니다. 다음 글에서는 테스트 환경을 통한 속도 향상 방법에 대해 작성해 보도록 하겠습니다. 읽어주셔서 감사합니다 🙂 Twitter Facebook Google+ # 백엔드 # test # 최적화",BE
523,"nGrinder 설치 방법 및 파일 업로드 테스트 예제 2021, Sep 30 1. nGrinder 안녕하세요, 케빈입니다. nGrinder는 Naver가 개발한 오픈 소스 프로젝트로서, 어플리케이션의 성능을 측정할 수 있는 스트레스 테스트 플랫폼을 제공합니다. Grinder 오픈 소스를 기반으로 하며 Jython으로 개발되었는데요. nGrinder를 통해 서비스의 가용성 및 임계점 등을 확인할 수 있습니다. nGrinder 이외에도 JMeter, K6, Gatling 등 다양한 성능 측정 도구가 있습니다. 이 중 nGrinder를 선택한 이유는 groovy 스크립트 기반의 테스트 작성 및 강력한 시각화 기능 제공 등의 장점 때문이었습니다. 아무래도 Gradle 및 Jenkins를 통해 groovy를 자주 다루었던 만큼 가장 친숙했습니다. 또한 공식 문서 내용이 풍부하고 관련 포럼이 활성화되어 있다는 점 또한 한 몫했습니다. 2. Architecture nGrinder를 구성하는 요소는 크게 Controller와 Agent 및 Target 등 세 가지가 존재합니다. Controller 전반적인 작업이 Controller를 통해 수행됩니다. 스트레스 테스트를 위한 웹 인터페이스를 제공합니다. 테스트 프로세스를 체계화하며, 테스트 결과를 수집해 통계를 보여줍니다. Controller를 통해 사용자는 테스트 수행을 위한 스크립트를 생성 및 수정할 수 있습니다. Agent Controller의 명령을 받아 작업을 수행합니다. Agent 모드시 프로세스 및 스레드를 실행시켜 Target 머신에 부하를 발생시킵니다. Monitor 모드시 대상 시스템의 CPU 및 Memory 등을 모니터링합니다. Target 테스트 대상이 되는 머신입니다. 2.1. 설치 방법 이번 글에서는 Docker를 활용해 EC2에 설치할 계획입니다. 자세한 내용은 DockerHub를 참조하길 바랍니다. 원한다면 Docker 없이 Controller 및 Agent를 WAR 파일로 다운받아 실행할 수 있습니다. Shell $ docker pull ngrinder/controller
$ docker run -d -v ~/ngrinder-controller:/opt/ngrinder-controller --name controller 
-p 80:80 -p 16001:16001 -p 12000-12009:12000-12009 ngrinder/controller
EC2에 Controller 컨테이너를 실행시키며, 필요한 포트 포워딩 및 볼륨 마운팅을 지정합니다. 80번 포트는 Controller의 기본 Web UI 포트입니다. 9010 - 9019 포트를 통해 Agent들이 Controller 클러스터로 연결됩니다. 12000 - 12029 포트는 테스트 실행 및 종료 등 컨트롤러 명령어와 에이전트별 테스트 실행 통계를 초별로 수집하는 포트입니다. 컨트롤러는 해당 포트를 통해 스트레스 테스트를 할당합니다. 16001 포트를 통해 테스트를 하지 않는 유휴 상태의 Agent가 Controller에게 테스트 가능 메시지를 전달합니다. Shell $ docker pull ngrinder/agent
공식 문서는 Controller 컨테이너가 동작 중인 머신과 Agent 컨테이너를 구동하지 말것을 강력하게 권고합니다. 여러 Agent들이 동작하다 보면, 부하를 발생시키는 머신의 자원을 모두 소모할 수 있기 때문입니다. Shell $ docker run -d --name agent --link controller:controller ngrinder/agent
그럼에도 불구하고 편의상 Controller 컨테이너가 동작 중인 EC2에 Agent를 구동해야 한다면 위 명령어를 입력하면 됩니다. 여러 Agent 컨테이너를 띄워도 됩니다. Shell $ docker run -v ~/ngrinder-agent:/opt/ngrinder-agent -d ngrinder/agent {controller-ec2-ip}:{controller-ec2-web-port}
별도의 EC2에서 Agent를 동작시킨다면 위처럼 Controller EC2 IP 및 Controller EC2 Web Port를 함께 인자로 넣어 컨테이너를 실행합니다. 이 때, 볼륨 마운팅 옵션은 생략해도 좋습니다. http://{controller-ec2-ip}로 nGrinder Controller 웹 페이지로 접속해봅니다. 저는 설명글 편의상 localhost에 띄웠습니다. ID : admin, Password : admin으로 로그인합니다. 계정 정보 - 에이전트 관리 탭에 지정한 Agent들이 Controller에 잘 연결되었는지 확인합니다. Agent가 없다면 구동한 Agent들이 Controller에 붙지 못한 것이니 설정을 재점검해야합니다. 3. Quick Start 이제 간단하게 nGrinder를 실행해보겠습니다. 메인 페이지의 검색란에 부하 테스트를 진행할 URL을 입력하고 테스트 시작 버튼을 누릅시다. 여러 메뉴들에 대해 간략하게 정리했습니다. 테스트명 : 추후 해당 테스트 기록을 식별하는데 사용합니다. Agent : 해당 테스트에 사용할 Agent 개수를 선택할 수 있습니다. VUser : Agent 별 VUser 개수를 선택할 수 있습니다. 사용할 수 있는 최대 VUser의 총합 개수는 Agent 개수 * Agent 별 VUser 개수입니다. 해당 부하 테스트를 수행할 기간 및 실행 회수 등을 세밀하게 조정할 수 있습니다. 저장 후 시작 버튼을 통해 테스트를 실행합니다. 테스트를 바로 시작할 수 있으며, 특정 시간에 예약을 걸어둘 수 있습니다. Quick Start로 테스트를 시작하면 nGrinder가 target-url/ 디렉토리 경로에 TestRunner.groovy 테스트 스크립트를 기본적으로 생성해줍니다. 원한다면 nGrinder에 작성해둔 다른 테스트 스크립트를 선택해서 사용할 수 있는데요. 우측의 R HEAD를 눌러서 스크립트를 편집할 수 있습니다. 테스트 스크립트는 groovy 문법을 사용하고 있어서 크게 어렵지 않습니다. 테스트할 API URL, Request Header, Request Body, Response Assertion, Logging 등을 스크립트에서 설정할 수 있습니다. 테스트를 시작하면 실시간 TPS 그래프가 화면에 시각화되며, 테스트가 종료되면 요약 결과가 집계됩니다. 아울러 좌측 하단에서 테스트 스크립트 로그 다운로드가 가능합니다. 상세 보고서를 클릭하면 TPS 이외의 지표들을 확인할 수 있으며, CSV 다운로드를 제공합니다. TestRunner.groovy @Test
public void test() {
HTTPResponse response = request.GET(""https://google.com"", params)

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}
샘플 테스트 스크립트의 Test 메서드에는 응답 결과에 대한 Assertion 검증이 있는 것을 확인할 수 있습니다. 테스트 수행시 Assertion 검증이 실패하면 에러로 잡히게 됩니다. 통상적으로 너무 많은 에러가 발생하는 경우 테스트가 중단됩니다. 3.1. 주의사항 Quick Start가 생성해준 TestRunner.groovy 파일에 테스트 스크립트를 작성(Overwrite)하는 경우, 날려먹기 정말 정말 쉽습니다. Quick Start로 https://test.com/api/comments URL에 대해 테스트를 시작했다고 가정해보겠습니다. nGrinder는 test.com/api/comments/ 디렉토리에 TestRunner.groovy 스크립트를 생성합니다. 해당 스크립트 파일에 테스트 수행 코드를 작성하고 저장하더라도, 이후 메인 페이지에서 실수로 https://test.com/api/comments 동일 URL에 대해 Quick Start 테스트 시작 버튼을 누르면 어떻게 될까요? nGrinder가 제공하는 기본 TestRunner.groovy 파일이 개발자가 커스터마이징한 TestRunner.groovy 파일을 덮어써버립니다! Quick Start는 무조건 target-url/ 디렉토리에 TestRunner.groovy 파일을 새로 생성하기 때문입니다. 또한 TestRunner라는 스크립트 파일 이름이 의미하는 바가 모호하기 때문에, 테스트 스크립트를 관리하기 복잡해집니다. 따라서 웹 상단의 스크립트 메뉴에서 직접 테스트 스크립트를 작성하는 것이 낫습니다. 테스트 스크립트의 이름을 적절하게 지어주면 나중에 테스트에서 원하는 스크립트를 찾기 수월하합니다. 아울러 부하 테스트 또한 웹 상단의 성능 테스트에서 직접 생성합시다. 4. 파일 업로드 테스트 예제 PostRequest.java public class PostRequest {

private List<MultipartFile> images;
private String githubRepoUrl;
private List<String> tags;
private String content;

private PostRequest() {
}

//생성자 및 Getter 생략
}
PostController.java @Slf4j
@RestController
public class PostController {

@PostMapping(value = ""/api/abc"")
public ResponseEntity<Void> write(PostRequest request) {
log.warn(""{} {} {}"", request.getImages(), request.getTags(), request.getContent());
if (request.getImages() == null) {
throw new IllegalStateException();
}
return ResponseEntity.ok().build();
}
}
MultipartFile 요청을 처리할 수 있는 컨트롤러를 작성하고 Spring 어플리케이션을 실행해봅시다. nGrinder가 업로드 테스트에 사용할 테스트용 정적 파일을 업로드하는 단계입니다. 테스트 스크립트와 동일한 위치에 resources 디렉토리를 생성하고, 해당 폴더 내부에 업로드에 사용할 파일을 업로드해둡시다. PostWrite.groovy import static net.grinder.script.Grinder.grinder
import static org.junit.Assert.*
import static org.hamcrest.Matchers.*
import net.grinder.script.GTest
import net.grinder.script.Grinder
import net.grinder.scriptengine.groovy.junit.GrinderRunner
import net.grinder.scriptengine.groovy.junit.annotation.BeforeProcess
import net.grinder.scriptengine.groovy.junit.annotation.BeforeThread
// import static net.grinder.util.GrinderUtils.* // You can use this if you're using nGrinder after 3.2.3
import org.junit.Before
import org.junit.BeforeClass
import org.junit.Test
import org.junit.runner.RunWith

import org.ngrinder.http.HTTPRequest
import org.ngrinder.http.HTTPRequestControl
import org.ngrinder.http.HTTPResponse
import HTTPClient.Codecs
import HTTPClient.NVPair
import org.ngrinder.http.cookie.Cookie
import org.ngrinder.http.cookie.CookieManager

/**
* A simple example using the HTTP plugin that shows the retrieval of a single page via HTTP.
*
* This script is automatically generated by ngrinder.
*
* @author admin
*/
@RunWith(GrinderRunner)
class PostWrite {

public static GTest test
public static HTTPRequest request
public static NVPair[] headers = []
public static List<Cookie> cookies = []

@BeforeProcess
public static void beforeProcess() {
HTTPRequestControl.setConnectionTimeout(300000)
test = new GTest(1, ""32.232.168.41"")
request = new HTTPRequest()
grinder.logger.info(""before process."")
}

@BeforeThread
public void beforeThread() {
test.record(this, ""test"")
grinder.statistics.delayReports = true
grinder.logger.info(""before thread."")
}

@Before
public void before() {
headers = [new NVPair(""Authorization"", ""Bearer abc.def.xyz""), new NVPair(""Content-Type"", ""multipart/form-data"")]
request.setHeaders(headers)
CookieManager.addCookies(cookies)
grinder.logger.info(""before. init headers and cookies"")
}

@Test
public void test() {
NVPair param1 = new NVPair(""githubRepoUrl"", ""https://github.com/test"");
NVPair param2 = new NVPair(""content"", ""content"");
NVPair param3 = new NVPair(""tags"", ""typescript"");
NVPair param4 = new NVPair(""tags"", ""html"");

NVPair[] params = [param1, param2, param3, param4];
NVPair[] files = [new NVPair(""images"", ""./resources/test.png""), new NVPair(""images"", ""./resources/test.png"")];

def data = Codecs.mpFormDataEncode(params, files, headers)
HTTPResponse response = request.POST(""http://32.232.168.41:8080/api/abc"", data);

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}
}
이제 테스트 스크립트를 작성할 차례입니다. 스크립트를 완성하면 스크립트 편집기 우측 상단의 검증 버튼을 눌러 API 서버와 잘 통신되는지 확인해봅시다. 아래는 코드에 대한 간략한 설명입니다. 헤더에 Authorization 및 Content-Type를 넣었습니다. /api/abc API 스펙대로 Request Body에 들어갈 multipart/form-data 형식의 데이터(Key-Value)를 정의합니다. 업로드할 MultipartFile의 경우, Value는 업로드할 파일의 상대 경로가 됩니다. Codecs.mpFormDataEncode 메서드는 Key-Value 쌍의 데이터 및 파일을 multipart/form-data 인코딩을 통해 바이트 배열로 인코딩합니다. 자세한 내용은 Docs를 참고하시길 바랍니다. 4.1. 삽질의 시작 그러나 테스트 스크립트를 검증해보면 예상했던 200 응답이 아닌 500 응답을 받았습니다. Multipart Servlet Request를 Resolve할 수 없다는 로그가 발생했는데요. multipart boundary가 표기되지 않아서 생기는 에러라고 합니다. Spring 어플리케이션의 Access Log를 보니 Authorization 및 Content-Type 헤더는 정상적으로 잘 적용된 것을 확인할 수 있었습니다. 또한 Request Body에 바이너리 데이터가 잘 들어와있다는 것을 추측할 수 있었는데요. Content-Length가 과하게 크며, Groovy 테스트 스크립트에서 println(data)를 호출했을 때 바이너리 배열이 출력됬기 때문입니다. PostWrite.groovy @Before
public void before() {
headers = [
new NVPair(""Authorization"", ""Bearer abc.def.xyz""),
new NVPair(""Content-Type"", ""multipart/form-data; boundary=-----1234"")
]
request.setHeaders(headers)
CookieManager.addCookies(cookies)
grinder.logger.info(""before. init headers and cookies"")
}
이번에는 관련 글을 참고하여, Content-Type을 정의할 때 boundary 값을 추가했습니다. 검증을 해본 결과, 이번에는 Multipart Servlet Request를 Resolve했으나 정작 PostRequest에 데이터가 전부 null로 바인딩되는 문제가 발생했습니다. Request Body에 바이너리 데이터는 확실히 들어있으니, 아무래도 요청 본문의 Form-Data들이 제가 지정한 Boundary로 잘 구분되지 않아 생긴 에러로 추측했습니다. Codecs.mpFormDataEncode 메서드 Docs를 확인해본 결과, 세 번째 파라미터에 대한 설명이 제대로 나와있습니다. 세 번째 파라미터에 배열을 대입하면, 배열의 0번째 원소를 Key: ""Content-Type"", Value: ""multipart/form-data; boundary={random_string}""으로 변환합니다. PostWrite.groovy grinder.logger.debug(""Before headers[0]: ${headers[0]}, headers[1]: ${headers[1]}"")
def data = Codecs.mpFormDataEncode(params, files, headers)
grinder.logger.debug(""After headers[0]: ${headers[0]}, headers[1]: ${headers[1]}"")
Log 2021-09-15 23:29:36,925 DEBUG Before headers[0]: HTTPClient.NVPair[name=Authorization,value=Bearer abc.def.xyz], headers[1]: HTTPClient.NVPair[name=Content-Type,value=multipart/form-data; boundary=-----1234]
2021-09-15 23:29:36,960 DEBUG After headers[0]: HTTPClient.NVPair[name=Content-Type,value=multipart/form-data; boundary=--------ieoau._._+2_8_GoodLuck8.3-ds0d0J0S0Kl234324jfLdsjfdAuaoei-----], headers[1]: HTTPClient.NVPair[name=Content-Type,value=multipart/form-data; boundary=-----1234]
해당 메서드 호출 전후로 로그를 찍어본 결과, headers 배열의 첫 번째 원소가 Authorization에서 Content-Type으로 변경된 것을 확인할 수 있었습니다. 그런데 검증을 다시 해봐도, Spring Controller에는 변경된 Headers가 아닌 예전 Headers가 담긴 요청이 날아가고 있었습니다. 😭 4.2. 삽질의 끝 PostWrite.groovy grinder.logger.debug(""Before headers[0]: ${headers[0]}, headers[1]: ${headers[1]}"")
def data = Codecs.mpFormDataEncode(params, files, headers)
request.setHeaders(headers);
grinder.logger.debug(""After headers[0]: ${headers[0]}, headers[1]: ${headers[1]}"")
HTTPResponse response = request.POST(""http://localhost:8081/api/abc"", data);
Codecs.mpFormDataEncode 메서드 수행 이후 request.setHeaders를 한 번더 호출해주니 문제가 해결되었습니다. @Before 메서드에서 request.setHeaders()를 호출하기 때문에 당연히 request 객체는 headers를 참조하기 있을 것이라 예단했었는데요. 참조 변수 headers 내부 값이 변경되면 request 또한 변경된 headers를 바라볼 것이라고 생각했기 때문입니다. HttpRequest.java public void setHeaders(NVPair[] nvPairHeaders) {
setHeaders(convert(nvPairHeaders, BasicHeader::new));
}
PairListConvertUtils.java public static <R> List<R> convert(NVPair[] pairs, BiFunction<String, String, R> converter) {
Function<NVPair, R> pairMapper = pair -> converter.apply(pair.getName(), pair.getValue());
return Arrays.stream(pairs)
.map(pairMapper)
.collect(toList());
}
실제 메서드 코드를 까보니 setHeaders() 메서드는 내부적으로 주어진 Map 혹은 NVPair 배열의 값을 방어적으로 복사해서 사용하고 있었습니다. 😣 4.3. 스크립트 최종본 PostWrite.groovy import static net.grinder.script.Grinder.grinder
import static org.junit.Assert.*
import static org.hamcrest.Matchers.*
import net.grinder.script.GTest
import net.grinder.script.Grinder
import net.grinder.scriptengine.groovy.junit.GrinderRunner
import net.grinder.scriptengine.groovy.junit.annotation.BeforeProcess
import net.grinder.scriptengine.groovy.junit.annotation.BeforeThread
// import static net.grinder.util.GrinderUtils.* // You can use this if you're using nGrinder after 3.2.3
import org.junit.Before
import org.junit.BeforeClass
import org.junit.Test
import org.junit.runner.RunWith

import org.ngrinder.http.HTTPRequest
import org.ngrinder.http.HTTPRequestControl
import org.ngrinder.http.HTTPResponse
import HTTPClient.Codecs
import HTTPClient.NVPair
import org.ngrinder.http.cookie.Cookie
import org.ngrinder.http.cookie.CookieManager

/**
* A simple example using the HTTP plugin that shows the retrieval of a single page via HTTP.
*
* This script is automatically generated by ngrinder.
*
* @author admin
*/
@RunWith(GrinderRunner)
class test {

public static GTest test
public static HTTPRequest request
public static NVPair[] headers = []
public static List<Cookie> cookies = []

@BeforeProcess
public static void beforeProcess() {
HTTPRequestControl.setConnectionTimeout(300000)
test = new GTest(1, ""32.232.168.41"")
request = new HTTPRequest()
grinder.logger.info(""before process."")
}

@BeforeThread
public void beforeThread() {
test.record(this, ""test"")
grinder.statistics.delayReports = true
grinder.logger.info(""before thread."")
}

@Before
public void before() {
headers = [new NVPair("""", """"), new NVPair(""Authorization"", ""Bearer abc.def.xyz"")]
request.setHeaders(headers)
CookieManager.addCookies(cookies)
grinder.logger.info(""before. init headers and cookies"")
}

@Test
public void test() {
NVPair param1 = new NVPair(""githubRepoUrl"", ""https://github.com/test"");
NVPair param2 = new NVPair(""content"", ""content"");
NVPair param3 = new NVPair(""tags"", ""typescript"");
NVPair param4 = new NVPair(""tags"", ""html"");

NVPair[] params = [param1, param2, param3, param4];
NVPair[] files = [new NVPair(""images"", ""./resources/test.png""), new NVPair(""images"", ""./resources/test.png"")];

def data = Codecs.mpFormDataEncode(params, files, headers)
request.setHeaders(headers);
HTTPResponse response = request.POST(""http://32.232.168.41:8080/api/abc"", data);

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}
}
아무튼 인코딩 메서드를 호출하면 주어진 Headers 배열의 0번 인덱스의 값이 변경된다는 것을 알았습니다. Authorization 헤더를 Headers 배열의 0번 인덱스에 지정해두면 인코딩 메서드 호출 이후 해당 헤더가 누락될 수 있으니, 1번 인덱스에 저장해두고 0번 인덱스는 빈 값을 넣어줍니다. 이후 검증해보면 MultipartFile이 Spring Controller의 DTO에 잘 바인딩되는 것을 확인할 수 있습니다. Reference nGrinder - WiKi 20190322 nGrinder file upload(multipart/form-data) 기능 확인 Twitter Facebook Google+ # 백엔드 # nGrinder # Test",BE
524,"nGrinder 스트레스 테스트를 통한 어플리케이션 성능 분석 및 개선 사례 2021, Sep 30 1. 들어가며 안녕하세요, 케빈입니다. 부하 및 스트레스 테스트는 각 시스템의 응답 성능 및 한계치를 확인하는 수단입니다. 특히, 부하가 많아질 때 나타나는 증상 등은 성능을 개선할 수 있는 단초가 됩니다. nGrinder를 바탕으로 어플리케이션 서비스의 성능 임계점 및 확장성 등을 간편하게 진단할 수 있습니다. 이번 글에서는 실제 Pick-Git 어플리케이션 서비스의 성능을 nGrinder로 분석하고 개선한 사례를 공유하고자 합니다. 만약 nGrinder 구성 방법이 궁금하시다면 nGrinder 설치 방법 및 파일 업로드 테스트 예제 글을 참고하시길 바랍니다. 2. 테스트 계획 2.1. 전제 조건 처음 테스트 계획을 수립하는 만큼 미숙한 점이 많습니다. 이 점 양해부탁드립니다. 초기 테스트 전제 조건은 다음과 같이 구성했습니다. 1일 예상 사용자(DAU) : 10만명 1명당 1일 평균 접속 수 : 15회 1일 총 접속 횟수 : 150만회 1일 평균 RPS(Request Per Second) : 17회 1일 최대 RPS(Request Per Second) : 50회 Latency : 50 ~ 100ms 이하 Throughput : 17회~50회 초기 목표 RPS는 약 70회로 잡았으며, 부하 테스트에서 사용할 VUser는 공식에 의거해 105 정도로 산출했습니다. 스트레스 테스트를 위한 VUser는 300 ~ 600 등 유동적으로 적용했습니다. 아울러 부하 및 스트레스 테스트 모두 30분 ~ 1시간을 진행합니다. 2.2. 시나리오 대상 접속 빈도가 높으며, 많은 DB 리소스를 조합해 결과를 보여주는 부분을 시나리오 대상으로 선정했습니다. 대표적으로 홈 피드(게시물) 조회 페이지 및 게시물 작성 기능이 있겠습니다. 2.3. 테스트 환경 운영 환경과 동일한 스펙의 Reverse Proxy, WAS, DB 서버 등을 구성했습니다. 테스트 성능은 DB에 많은 영향을 받기 때문에, User와 Post 및 Comment를 각각 20만건 저장해두었습니다. 미처 신경쓰지 못한 결과, 일부 테스트의 경우 누적된 쓰기 작업으로 인해 게시물이 35만건 저장된 상태에서 테스트가 진행되었습니다. 😅 또한 테스트 대상 시스템의 성능을 측정하기 위해서 외부 시스템은 항상 기대한 결과만을 반환하는 환경이 필요합니다. 이 때, 어플리케이션에서 객체를 Mocking하거나 Dummy Controller를 사용하지 않았습니다. Http Connection Pool 미사용 Connection Thread 미사용 I/O 미발생 등의 문제가 존재하기 때문입니다. 성능 테스트에서 중요한 관점인 Thread 및 리소스 사용을 무시하게 되며, 테스트 시스템의 자원과 리소스를 함께 사용하는 등 테스트의 신뢰성이 떨어집니다. 테스트를 위한 요소는 테스트 대상 시스템에 절대로 영향을 미쳐서는 안 되기 때문에, S3 이미지 업로드 및 GitHub API와 같은 외부 시스템은 테스트 대상 시스템과 완벽히 분리된 Mock 서버를 만들어 배포한 다음 테스트를 진행했습니다. nGrinder를 활용한 부하 및 스트레스 테스트에 대한 상세한 내용은 우아한형제들 기술블로그의 결제 시스템 성능, 부하, 스트레스 테스트 글을 참고하시길 바랍니다. 3. nGrinder 부하 테스트 홈 피드(게시물) 조회 기능을 테스트한다. VUser 105명으로, 데이터 셋이 적은 경우 TPS가 100 ~ 400으로 좋게 측정되었습니다. 반면 데이터 셋이 20만건인 경우, TPS가 평균 27로 좋지 못했습니다. 에러율 또한 7.1%에 달했습니다. 3. nGrinder 스트레스 테스트 3.1. 단일 DB 서버 Pick-Git의 초기 Infrastructure는 WAS 1대 및 DB 1대 등 단일 구조입니다. 이같은 환경에 대해 스트레스 테스트를 진행했습니다. 테스트에 사용되는 VUser는 총 300명입니다. VUser 150명이 게시물 작성 요청을 보내는 동안, VUser 150명은 홈 피드(게시물) 조회 요청을 보냅니다. 읽기 TPS : 평균 11.8, 최대 22.5 MTT : 12,854 쓰기 TPS 평균 11.6, 최대 22 MTT : 13,067 3.2. DB Replication 적용 단일 DB 환경에 대한 스트레스 테스트 진행 후, DB Replication 도입의 필요성을 느꼈습니다. 단일 장애점을 없애고 가용성을 높일 수 있을 뿐만 아니라 성능 측면에서도 도움이 될 것이라 판단했습니다. DB Replication은 DB 스토리지를 물리적으로 다른 서버에 복제하는 것입니다. 2대 이상의 DB 서버가 동일한 데이터를 담도록 실시간으로 동기화하는 기술인만큼 DB 데이터 백업이 가능합니다. 하나의 Slave Node에 장애가 발생하더라도 다른 Slave Node(다중화)를 통해 서비스가 중단되지 않고 계속 정상 운영됩니다. 특히 Master Node 서버에 장애가 발생해 서버가 중지되더라도, Slave Node 중 1개를 Master Node로 승격시켜 서비스 중단 없이 데이터를 빠르게 복구하는 자동 Failover 기능을 제공합니다. Write 작업은 Master Node가 담당하고 Read 작업은 Slave Node가 담당함으로써 DB 서버 부하를 분산시킬 수 있습니다. 어플리케이션에서 DB 부하로 인한 병목 현상이 서비스 장애의 주 원인인데, 이를 개선하는데 도움이 됩니다. 이번에는 DB Replication(Master 1대 및 Slave 2대) 적용 후 스트레스 테스트를 진행했습니다. 읽기 TPS : 평균 40.1, 최대 62 MTT : 3,789 제 불찰로 인해, 이전 쓰기 테스트로 인해 생성된 게시물을 삭제하지 않고 읽기 테스트를 진행했습니다. 지속적인 쓰기 작업으로 인해 게시물이 35만건으로 누적되는 상태라, TPS가 점진적으로 하락하는 모습을 보입니다. 이러한 점을 고려하더라도 읽기 성능이 기존보다 많이 개선된 것을 볼 수 있습니다. 만약 테스트 초기 게시물이 20만건이었다면 TPS 평균이 40을 훨씬 상회했을 것이라 추측합니다. 교훈 처음에는 테스트를 돌릴 때마다, DB를 초기화하고 반복문을 돌며 20만건씩 데이터를 넣는 작업을 진행했습니다. DB Dump를 떠두고 불필요한 반복문 순회 작업을 생략할 수 있다는 사실을 나중에 알았습니다. 😭😭 최초로 DB에 데이터를 넣을 때만 반복문을 순회하며, 이후 DB를 초기화하면 미리 떠둔 DB Dump로 데이터를 넣습니다. 쓰기 TPS : 평균 85.9, 최대 121 MMT : 1,774 확실히 DB Replication을 적용한 결과, 읽기 및 쓰기 성능 모두 드라마틱하게 상승한 것을 확인할 수 있었습니다! 3.3. 단일 WAS DB Replication을 적용했으나 현재 WAS는 Scale-Out 없이 여전히 1대만 운용 중입니다. 해당 환경에 대해서도 스트레스 테스트를 진행했습니다. VUser 600명이 홈 피드(게시물) 조회 요청을 보내며, 저장된 게시물은 35만건입니다. 읽기 (TPS 평균 31.4, 최대 35.5) 3.4. 로드 밸런싱 적용 이번에는 WAS를 1대 더 증설한 다음, Nginx Reverse Proxy로 Round Robin 방식의 로드 밸런싱을 적용했습니다. 읽기 (TPS 평균 31.3, 최대 40) 로드 밸런싱 적용 유무와는 상관없이 TPS 및 MMS는 큰 차이가 없었습니다. CloudWatch 분석 결과 단일 WAS 환경이나 로드 밸런싱 환경이나 Reverse Proxy EC2 및 WAS EC2의 CPU Utilization은 매우 낮은 수준이었습니다. 반면 DB 서버 CPU Utilization은 매우 높았습니다. 조회 요청에 대한 DB 부하가 매우 심한 것으로 보입니다. 성능 향상을 위해 DB 서버 Scale-Out과 Scale-Up 및 Indexing 등을 활용한 쿼리 튜닝 등이 필요한 시점이라는 생각이 들었습니다. 4. 서비스의 한계 데이터를 100만건 넣고 VUser를 750으로 높여 테스트를 진행하자, TPS가 극적으로 하락했습니다. 또한 테스트 실패율이 50%를 초과했습니다. 확실히 데이터 셋이 많아질수록 성능이 매우 하락한다는 것을 확인할 수 있었습니다. Log o.h.engine.jdbc.spi.SqlExceptionHelper : hikari-pool-1 – Connection is not available, request timed out after 30000ms.
로그를 분석해보니 작업 쓰레드가 트랜잭션 처리를 위해 커넥션을 요청했으나, 30초동안 받지 못해 예외를 발생시키고 있었습니다. hikariCP를 적절하게 설정함으로써 해결할 수 있다는 글을 보았는데, 근본적으로는 DB 서버 Scale-Out과 Scale-Up 및 조회 쿼리 성능 튜닝 등이 답이 아닐까 추측했습니다. 게시물 조회를 처리하는 트랜잭션의 경우, 많은 테이블의 데이터를 조합해 응답을 작성합니다. 조회 쿼리에 Join이 포함되는 등 쿼리가 매우 복잡해집니다. N + 1 해결을 위해 Batch Fetch Size를 설정하긴 했으나, 완벽한 한방 쿼리가 아니기 때문에 추가적인 쿼리가 나갑니다. CP 크기를 늘리더라도, 게시물 조회 트랜잭션 성능을 개선하지 못하는 이상 트랜잭션 스레드가 커넥션을 점유하는 시간은 여전히 길 것입니다. 즉, 늘린 CP 사이즈로도 감당하지 못할 트래픽이 몰린다면 동일한 HikariCP Dead Lock 문제가 또다시 발생할 것으로 추정됩니다. 추후 Pinpoint 등 다양한 도구를 도입해 더 섬세하게 지표를 분석하고 점진적으로 성능을 개선할 계획입니다. 😇 Reference HikariCP Dead lock에서 벗어나기 (이론편) 결제 시스템 성능, 부하, 스트레스 테스트 Twitter Facebook Google+ # 백엔드 # nGrinder # Test",BE
525,"변경에 유연한 설계, 우리는 2가지 종류의 DTO를 쓴다! 2021, Sep 30 변경에 유연한 설계, 우리는 2가지 종류의 DTO를 쓴다! 안녕하세요 깃-들다의 손너잘 입니다. 이번 글에서는 깃들다의 설계중에 DTO와 관련된 이야기를 해보고자 합니다. 여러분은 DTO를 어떤식으로 많이 사용하시나요? 아마 많은 프로젝트에서 View ↔ Controller ↔ Service, 혹은 DAO ↔ Service 와 데이터를 주고받을 때 사용하실겁니다. 저희도 역시 마찬가지로 View ↔ Controller ↔ Service 간의 데이터 이동시 DTO를 이용하는데요, 깃들다의 경우View ↔ Controller, Controller ↔ Service 에서 각각 서로 다른 DTO를 사용합니다. *DAO ↔ Service (persistance layer ↔ Application layer)는 JPA를 사용하기 때문에 DTO를 사용하지 안습니다.* 이번 글에서는 깃들다가 왜 DTO를 각 레이어간에서 사용하는지에 대한 이야기를 해보고자 합니다. Layerd Arch. 일반적인 웹 어플리케이션은 보통 4계의 계층으로 이루어져 있습니다. 바로 Presentation Application Domain Infrastructure 입니다. 이러한 용어가 어색할 수 있어 스프링 구조로 변경시켜 말하자면 Presentation : Controller Application : Service Domain : Domain Infrastructre : DAO, RepositoryImpl 라고 표현할 수 있습니다. 그리고 추상화 수준은 위에서부터 아래로 내려가면서 옅어집니다. 또한 각 레이어간의 의존성은 한 방향으로 흐르며 보통 위에서 아래로 흐르게 됩니다. 왜 그래야 하는지에 대하여 간략하게 설명해보겠습니다. Infrastructure 레이어에는 보통 외부 모듈등이 위치하게 됩니다. 엄밀히 말하면 우리가 사용하는 자바 라이브러리 또한 Infrastructure에 위치하게 됩니다. 그렇다면 java.lang.String을 예시로 들어보겠습니다. 우리는 Domain을 작성하면서 String이라는 클래스를 사용하게 됩니다. 이때 String이 A 어플리케이션의 어떤 클래스를 참조하고 있다고 가정해봅니다. 그렇다면 B 어플리케이션이 String을 가져다 사용할 때 무슨 문제가 생길까요? B 어플리케이션은 자신에게 필요하지 않은 A 어플리케이션의 클래스를 강제로 Import 받게 됩니다. 이뿐만이 아닙니다. 하위계층은 추상화되어 상위계층에서 이용됩니다. 즉, 여러개의 상위 계층은 하나의 하위계층에 의존할 수 있습니다. 하지만 만일 누군가가 위 원칙을 어기고 Domain 구현체를 의존하는 Infrastructure를 만들었다고 가정해봅시다. 그렇게 된다면 Domain 구현체의 변경이 Infrastructure에 속하는 객체에 영향을 끼칠 수 있습니다. 결국은 해당 Infrastructure계층에 속한 객체를 의존하는 다른 Domain객체들은, 다른 Domain 구현체의 변경으로 인해 전체적으로 변화가 일어날 가능성이 있습니다. 만일 누군가가 자신의 프로젝트를 수정했더니 java.lang.String에 변화가 일어나 String 객체를 사용하는 모든 프로젝트에 영향을 끼친다고 생각해 보세요.. 이러한 문제점으로 인해 레이어 아키텍쳐의 의존성 방향은 프로젝트의 유연한 변경에 큰 영향을 끼치게 됩니다. DTO는 왜 사용하는가? 여러분은 DTO를 왜 사용한다고 생각하시나요? DTO의 이름의 뜻은 Data Transfer Object입니다. 즉, 데이터를 전송하기 위한 객체를 의미합니다. 따라서 보통 DTO에는 Entity, Request로 들어온 값을 사상시켜 그 데이터를 전송하는 목적으로 사용됩니다. 따라서 DTO는 객체로 취급하지 않고 보통 Data Structure로 취급하는게 일반적입니다. Spring에서 DTO는 어떻게 사용되는가? Spring으로 웹 어플리케이션을 만들면서 아래와 같은 코드를 많이들 작성하셨을 것 입니다. @RestController
public class PostController {

private final PostService postService;

public PostController(PostService postService) {
this.postService = postService;
}

@PostMapping(""/posts"")
public ResponseEntity<PostResponse> create(@RequestBody PostRequest postRequest) {
PostResponse postResponse = postService.create(postRequest);

return ResponseEntity.ok(postResponse);
}
...
}
@Service
@Transactional
public class PostService {

private final PostRepository postRepository;

public PostService(PostRepository postRepository) {
this.postRepository = postRepository;
}

public PostResponse create(PostRequest postRequest) {
String title = postRequest.getTitle();
String content = postRequest.getContent();

Post post = postRepository.save(new Post(null, title, content));

return PostResponse.from(post);
}
...
}
아주 정석적인 Controller, Service 구조입니다. @RequestBody 를 통해 사용자로부터 요청한 데이터를 PostRequest라는 DTO에 Mapping시키고, 그 데이터를 Service로 보내는 행위를 하고 있습니다. 하지만 위 코드에서는 치명적인 문제점이 있습니다. 혹시 눈치 채셨나요? 바로 PostRequest를 service로 바로 넘기는 부분에 있습니다. 이 부분이 왜 문제인지 한번 확인해 보도록 하겠습니다. 위 프로젝트의 패키지 구조입니다. dto는 presentation 레이어에 위치하고 있습니다. 이때 각 클래스간의 의존성을 확인해 보도록 하겠습니다. 오른쪽 그림은 각 객체간의 의존성 방향을 나타내고 있습니다. 이렇게 보니까 무엇이 문제이지 잘 이해가 안될 수 있습니다. 그렇다면 객체들을 지우고, 패키지(레이어)의 의존성을 살펴보겠습니다. 이제 보이시나요? Presentation 레이어가 Application 레이어에 의존하고, Application 레이어가 Presentation 레이어에 의존하고 있습니다. 즉, 양방향 참조를 하고 있습니다. 이게 무슨 문제가 있는지는 위에서 설명했습니다. 조금 더 구현에 가까운 Application 계층이 Presentation 계층의 변화에 영향을 받고 OCP를 지키지 못하게 됩니다. 그러면 어떻게 해결할 수 있을까? 이를 해결하기 위해서 처음 말했듯, 깃들다 팀은 View ↔ Controller, Controller ↔ Service 에서 각각 서로 다른 DTO를 사용합니다. 패키지 구조를 보면 Application 레이어에 dto가 생긴것을 볼 수 있습니다. 코드로 한번 확인해 봅시다. @RestController
public class PostController {

private final PostService postService;

public PostController(PostService postService) {
this.postService = postService;
}

@PostMapping(""/posts"")
public ResponseEntity<PostResponse> create(@RequestBody PostRequest postRequest) {

PostRequestDto postRequestDto = new PostRequestDto(
postRequest.getTitle(),
postRequest.getContent()
);

PostResponseDto postResponseDto = postService.create(postRequestDto);

PostResponse postResponse = new PostResponse(
postResponseDto.getId(),
postRequest.getTitle(),
postRequest.getContent()
);

return ResponseEntity.ok(postResponse);
}
...
}
Controller 소스입니다. Controller에 들어온 PostRequest를 Application 레이어의 PostRequestDto로 변경하여 서비스로 넘기는 것을 볼 수 있습니다. 이렇게 변경함으로서 의존성에 어떤 변화가 일어났는지 그림으로 확인해 보겠습니다. 의존성이 드디어 한쪽 방향으로 흐르게 되었습니다! 이젠 Presentation에서 아무리 변화가 생기더라고 Application에는 아무 영향을 주지 않습니다. 즉, 변화에 닫혀있는(유연한) 설계를 할 수 있게 되었습니다!!!! 이런 실수를 조심하세요 이렇게 코드를 작성하고나니 Controller가 너무 더러워졌습니다. 마찬가지로 Service로 더러워지겠죠.. 이를 해결하기 위해서 메서드 분리를 진행했지만 그래도 필드가 많은 Entity를 Dto로 만들거나 하면 코드가 더러워 보이는 것은 사실입니다. 많은분들이 여기서 실수를 합니다. 바로 정적 팩토리 메서드 인데요. 나름 코드를 이쁘게 만들겠다고 아래와 같은 소스를 작성하시는 분들이 있습니다. @PostMapping(""/posts"")
public ResponseEntity<PostResponse> create(@RequestBody PostRequest postRequest) {
PostRequestDto postRequestDto = PostRequestDto.from(postRequest);
PostResponseDto postResponseDto = postService.create(postRequestDto);
PostResponse postResponse = PostResponse.from(postResponseDto);

return ResponseEntity.ok(postResponse);
}
확실히 소스는 깔끔해졌네요… 하지만 이 소스의 문제점은 무엇일까요? 다시 한번 의존성 방향 그림을 그려보겠습니다. DTO간의 의존성이 생기면서 또 다시 양방향 참조가 생겼습니다. 물론 PostRequest.from(postRequestDto) 와 같은 코드는 괜찬습니다. 어짜피 Presentation에 있는 DTO가 Application의 DTO를 참조하는것이니까요. 하지만 그 반대는 문제가 있습니다. 마찬가지로 아래와 같은 실수도 많이 합니다. public PostResponseDto create(PostRequestDto postRequestDto) {
String title = postRequestDto.getTitle();
String content = postRequestDto.getContent();

Post post = new Post(null, title, content);

Post savedPost = postRepository.save(post);

return new PostResponseDto(
savedPost.getId(),
savedPost.getTitle(),
savedPost.getContent()
);
}
위에서 설명한것 같이 2개의 Dto를 사용하게 되면 Service로직또한 위와같이 만들어지는데요, 코드를 깔끔하게 하기 위해서 아래와 같은식으로 리팩토링을 진행합니다. public PostResponseDto create(PostRequestDto postRequestDto) {
Post post = postRequestDto.toEntity();

Post savedPost = postRepository.save(post);

return PostResponseDto.from(post);
}
코드는 확실히 깔끔해 졌군요. 하지만 이것도 위와 같은 문제가 있습니다. 의존성 방향 그림을 그려보면, Domain과 Application 레이어가 양방향 참조를 하는것을 확인할 수 있습니다. 그렇다면 코드를 이렇게 더럽게 유지할 수 밖에 없을까요? 아니요 그렇지 않습니다. DTO라는 개념을 처음 제시한 마틴파울러의 PoEAA(Pattern of Enterprise Application Architecture)를 보면 DtoAssembler라는것을 제시합니다. 간단히 말하면 도메인을 Dto로 만들고, Dto를 도메인으로 만드는 로직을 몰아넣은 유틸성 객체를 만들라는 의미입니다. 위 구조는 실제 Pickgit의 패키지 구조인데요, Presentation ,Application 레이어에 Assembler가 있는것을 볼 수 있습니다. 내부를 보면 외부로 빼놓기에 너무 비대한 로직 변환 로직을 다 몰아넣을것을 볼 수 있습니다. 그러면 이런식으로 코드를 깔끔하게 유지할 수 있습니다. 결론 이로서 깃들다팀이 왜 DTO를 2개를 사용하는지, 변경에 유연한 설계 관점에서 서술해봤습니다. 많은 도움이 되셨나요? 개인적으로 프로젝트를 하면서 느끼는점은, 이러한 이론적인것도 좋지만 프로젝트의 크기와 일정을 고려하여 어느정도 타협하는것도 나쁘지 않다는 생각입니다 🙂 읽어주셔서 감사합니다. Reference 엔터프라이즈 애플리케이션 아키텍처 패턴 (마틴파울러 저) Twitter Facebook Google+ # 백엔드 # 설계 # DTO",BE
526,"DB Replication을 구성한 이유 2021, Oct 03 안녕하세요, 다니입니다. 🌻 프로젝트를 진행하며 DB Replication을 적용하는 경험을 했습니다. 이번 글에서는 이를 활용한 이유를 간단하게 정리하려 합니다. DB 기존 구조 현재는 사용자 수가 적어 WAS 1대, DB 1대로 인프라를 구축했습니다. 지금 당장은 서버에 문제가 생길 정도의 트래픽이 발생하지 않아 DB를 1대만 둬도 괜찮았습니다. 한편, DB를 1대만 두면 단일 장애점이 될 수 있다는 우려가 있었습니다. 그래서 DB를 확장해야 하나 고민하던 찰나, 팀의 기대치만큼 서버가 트래픽을 처리할 수 있는지 스트레스 테스트를 진행하고 의사결정을 하기로 했습니다. 성능 테스트 목표값 Throughput 17 ~ 50회 설정값 DB 데이터 User - 20만 건 Post - 20만 건 Comment - 20만 건 VUser 읽기 연산 - 152명 쓰기 연산 - 152명 방식 읽기 연산과 쓰기 연산을 30분간 동시 수행 읽기 연산 평균 TPS는 11.8, 최대 TPS는 22.5로 나왔습니다. 쓰기 연산 평균 TPS는 11.6, 최대 TPS는 22로 나왔습니다. 결론 예상보다 TPS가 낮게 나타나는 걸 확인했습니다. 따라서 DB를 Scale up 또는 Scale out하여 성능을 높이기로 했는데, 프로젝트 상황을 고려하면 Scale up은 불가능해 Scale out을 하기로 결정했습니다. DB 확장 방법 DB를 Scale out하는 방법은 Clustering과 Replication이 있습니다. Sharding도 Scale out을 하는 게 아닌가? 라고 생각할 수 있는데, Sharding은 데이터 조회 성능을 향상시키기 위해 데이터를 분산 저장하는 기술입니다. Clustering과 Replication은 각 서버가 동일한 데이터를 저장하고 있는 반면, Sharding은 각 서버가 서로 다른 데이터를 저장하고 있습니다. 따라서, 여기서는 Clustering과 Replication을 중심으로 비교하고 결론을 내리겠습니다. Clustering Clustering은 DB 서버를 2대 이상, DB 스토리지를 1대로 구성하는 형태입니다. 그림처럼 DB 서버 2대를 모두 Active 상태로 운영하면, DB 서버 1대가 죽더라도 DB 서버 1대는 살아있어 서비스는 정상적으로 할 수 있습니다. 또한 DB 서버를 여러 대로 두면, 트래픽을 분산하여 감당하게 되어 CPU와 Memory에 대한 부하가 적어지는 장점이 있습니다. 그러나, DB 서버들이 DB 스토리지를 공유하기 때문에 DB 스토리지에 병목이 생기는 단점이 있습니다. 그래서 DB 서버 2대 중 1대를 Stand-by 상태로 두어 단점을 보완할 수 있습니다. Stand-by 상태의 서버는 Active 상태의 서버에 문제가 생겼을 때, Fail Over를 통해 상호 전환되어 장애에 대응할 수 있습니다. 이와 같이 구성하면, DB 스토리지에 대한 병목 현상도 해결됩니다. 그렇지만, Fail Over가 이뤄지는 시간 동안은 영업 손실이 필연적으로 발생합니다. 또 DB 서버 비용은 이전과 동일하나, 가용률은 이전에 비해 대략 1/2로 줄어드는 단점도 있습니다. 추가적으로, Clustering은 DB 스토리지를 1대만 사용하기 때문에 DB 스토리지가 단일 장애점이 될 수 있습니다. DB 스토리지에 문제가 발생하면, 데이터를 복구할 수 없는 치명타가 생깁니다. Replication Replication은 각 DB 서버가 각자 DB 스토리지를 갖고 있는 형태입니다. 사용자가 Master DB에 SELECT/INSERT/UPDATE/DELETE을 하면, Master DB는 Slave DB에 데이터를 복제합니다. 이렇게 하면, DB 스토리지 단일 장애점을 해소하여 데이터 손실을 방지할 수 있습니다. 하지만, Master DB만 일을 하고 Slave DB는 놀게 되는 상황이 발생합니다. 그래서 Master DB에 INSERT/UPDATE/DELETE를 하고, Slave DB에 SELECT를 하는 방식으로 각각 DB에 트래픽을 분산할 수 있습니다. 결론 Clustering은 DB 스토리지를 1대로 구성한다는 부분에서 잘못하면 큰 문제로 이어질 것 같았습니다. 따라서 Replication을 선택하는 게 안전하고 올바르겠다는 생각이 들었고, Replication을 적용하기로 결정했습니다. 이때, Master DB 1대에 Slave DB는 몇 개로 둬야 할지 고민하게 됐습니다. SNS 특성상 읽기 연산이 빈번하게 발생하기 때문에 Slave DB를 1대로 두는 것보다 2대 이상 두는 것이 나을 거라 판단했습니다. 인프라 비용을 생각하면 Slave DB를 무작정 많이 두는 건 옳지 않았습니다. 그래서 Slave DB를 우선 2대로 두고, 나중에 더 많은 트래픽이 발생하면 Slave DB의 개수를 늘리기로 했습니다. DB Replication 이후 구조 앞에서 설명한 것처럼 Master DB 1대와 Slave DB 2대로 구성하게 됐습니다. 성능 테스트 목표값 Throughput 17 ~ 50회 설정값 DB 데이터 User - 20만 건 Post - 20만 건 Comment - 20만 건 VUser 읽기 연산 - 152명 쓰기 연산 - 152명 방식 읽기 연산과 쓰기 연산을 30분간 동시 수행 읽기 연산 평균 TPS는 40.1, 최대 TPS는 62로 나왔습니다. 쓰기 연산 평균 TPS는 85.9, 최대 TPS는 121로 나왔습니다. 결론 Replication을 설정하니 TPS가 기대치보다 높게 나타났습니다. DB를 Scale out하는 게 맞는 걸까? 라는 의구심이 있었는데, 적절했다는 확신을 얻게 됐습니다. 한편 읽기 연산에서 TPS가 점진적으로 하락하는 양상을 보이는데, 이는 쓰기 작업이 누적되어 데이터 개수가 늘어나서 영향을 받은 것으로 추측됩니다. 마무리 애플리케이션 개발에서 성능을 개선할 수 있는 지점은 다양하게 존재합니다. 이 경험에서는 DB 확장을 통해 성능을 조금 더 좋게 만들 수 있었습니다. References Clustering vs Replication vs Sharding How To Setup MySQL DB Replication On Kubernetes (썸네일) Twitter Facebook Google+ # 백엔드 # Database # Replication",BE
563,"우리는 왜 CI/CD에 실패하였는가? 2021, Sep 30 프로젝트를 진행하면서 CI/CD 를 통한 자동화 배포 프로세스를 구축하는 경우가 많다. 우테코에서 이번 팀 프로젝트를 진행하면서 깃들다팀 역시 CI/CD 프로세스를 구축했으나 결론적으로 CI/CD를 제대로 성공시키지 못했다는 생각을 떨쳐버릴수 없었다. 이번 글에서는 필자가 생각하는 CI/CD란 무엇인지, 그리고 어떤 부분이 문제였는지 한번 작성해 보도록 한다. CI/CD란 무엇인가? 많은 글과 주변 크루들의 이야기를 들어봤을때 CI/CD에 대한 의견이 많이 갈리는것을 볼 수 있었다. redhat에서 작성한 글과 그 외의 여러 글들을 통해 필자가 이해한 CI/CD에 대해 설명해보겠다. 지속적 통합 지속적 통합은 우리가 pr을 날리고 머지하는 행위라고 말하고싶다. 지속적 통합의 목표는 “머지데이”의 문제점을 제거하기 위함에있다. 과거, CI/CD 가 정착화되지 않았을때 “머지데이”는 많은 인력 리소스를 낭비시켰다. 필자의 경우만 하더라도, 학부시절 팀 프로젝트를 진행하면 서로 변경한 소스를 합치는데 있어서 많은 고통이 있었다(부끄럽게도 학부시절에 Git과같은 버전관리 시스템을 전혀 사용하지 않았다). CI 프로세스 (출처: redhat) 위 그림을 살펴보면 Build, Test, Merge까지의 과정을 CI라고 말하고있다. 결국 이 프로세스가 의미하는것은 정상적으로 동작하는, 그리고 충돌이 없는 코드의 병합 을 말한다. Github를 기준으로 말하자면, PR을 올리고 빌드, 테스트를 검증하고 머지하여 특정 브랜치에 코드가 병합하는 과정까지를 의미한다고 볼 수 있다. 지속적 전달 지속적 전달은 솔직히 말하자면 이름이 잘못된것같다. 이 과정의 목적은 즉시 배포 가능한 코드의 확보 라고 말하고싶다. Github를 예로 들자면, PR들이 특정 브랜치로 Merge됐다면, 그 브랜치의 코드는 언제든 배포 가능한 상태여야 한다는 것을 의미한다. 따라서 필자는 PR이 Merge될때마다 CI와 같이 Build, Test를 진행하여 코드가 정상적으로 동작하는지 확인해야 한다고 생각한다. 이와 관련하여 혹자는 “PR을 올렸을때 이미 Build와 Test를 진행하고 정상인 것을 확인하였는데 뭐하러 또 다시 이를 확인해야 하는가?” 라고 물을지도 모른다. 이에대한 답변은 우테코의 prolog 프로젝트에서 필자가 작성한 글을 인용하여 답변하도록 하겠다. 지속적 배포 이부분은 모두가 동의하는것 같다. 코드를 Merge 했다면, 적절한 서버로 어플리케이션을 자동 배포하는 그 행위를 의미한다. 레드헷에서는 지속적 배포란 개발자가 애플리케이션에 변경 사항을 작성한 후 몇 분 이내에 애플리케이션을 자동으로 실행할 수 있는 것을 의미합니다. 라고 명시하고 있다. 우리는 왜 CI/CD에 실패하였는가? 엄밀히 말하자면 필자는 CD보다는, CI에 실패했다고 생각하고 있다. 이전글에서도 말했듯, CI의 가장 큰 목적은 머지데이의 안좋은점을 제거하는데 있다고 생각한다. 하지만 프로젝트를 진행하면서 나름 치밀하게 CI/CD를 구축했다고 생각했음에도 불구하고 언제나 코드를 머지할때면 수 많은 Merge Confilict에 치이고 말았다. 이러한 상황은 필자를 “이게 정말 CI가 구축된것이 맞는가?” 란 의문으로 이끌었다. 이에대한 분석을 진행한 결과, 결국 모든 문제는 PR에서 부터 시작됐다는 결론을 내리게 됐다. 1. PR의 크기가 너무 크다 필자가 속한 깃들다팀은 모든 PR에 대하여 팀원들의 Approve가 있어야지만 PR의 Merge가 가능하도록 컨벤션을 만들었다. 그 말인 즉슨, 팀원들이 코드를 읽고, 피드백을 남겨야 한다는 것을 의미하게 된다. 위 숫자를 보자, 필자가 속한 팀의 PR Line수를 3개정도만 임의로 가져온 것이다. 기본적으로 항상 1000줄에 가까운 Line수를 보인다. 이러게 많은 Line의 추가와 File Changes는 코드리뷰의 병목을 가져올수밖에 없다. 각각의 팀원들도 자신이 맡은 일이 있기때문에 다른 팀원의 코드에 많은 리소스를 쏟을 수 없다. Line의 기술블로그 를 읽어보면 코드리뷰에 대하여 많은 팁을 제공하고 있으며 공통적으로 하는 말은 리뷰의 시간과 리소스를을 짧고, 적게 가져가자로 귀결된다. 그렇다. 결국 이러한 큰 PR단위가 CI를 망치는 주된 원인이 된 것이다. 아직까지는 감이 오지 않는 독자가 있을것이다. 그렇다면 필자는 큰 PR단위가 무엇이 문제이기에 CI를 망쳤다고 주장하는것일까? 2. PR이 너무 많이 쌓인다. 큰 PR단위는 결국 리뷰의 병목을 발생시키고 PR이 쌓이도록 유도한다. CI의 목적이 무엇인가? 이름 그대로 코드의 지속적 통합이다. 하지만 PR이 쌓인다는것은, 지속적인 통합이 진행되지 않고, 통합이 계속 미뤄진다는 것을 의미한다. 이제 눈치를 챘는가?? 큰 PR이 지속적으로 쌓이면 어떤 문제가 발생할지……. 3. 결국에는 또 다시 머지데이로… 그렇다. 큰 PR이 수십개 쌓이는 순간 머지데이의 늪에 또 다시 빠지게 된다. 수십개의 파일 변경과 수천라인의 PR이 적개는 몇개, 많게는 수십개 쌓였다면 이를 머지하는데 또 다시 다른 PR과의 병합을 해결해야 한다. 또한 지속적인 코드 병합에 실패하면서 지속적 통합이라는 그 근본에도 접근하지 못하게 된다. 이것이 과연 올바른 CI라고 할 수 있을까? 실제로 프로젝트를 진행하면서 이렇게 쌓인 PR은 데모데이 전날에 다함께 모여 후다닥 Merge하는 형태로 이어졌다. 이것도 머지데이의 차이점이 무엇인가? CI를 힘들게 구축해놓고 또 다시 과거의 머지데이를 반복하는꼴이 돼버렸다. 결론. 이번 글에서는 왜 필자의 프로젝트에서 구축한 CI가 실패한 CI라고 생각했는지에 대하여 작성해봤다. 정말 아이러니하게도. CI프로세스 자체는 제대로 구축되어 있었으나, 그 실패의 원인은 전혀 다른곳에 있었다. 작은 PR 법칙이 왜 존재하는지 또 다시 체감하였다. 그렇다면 이를 깨닫고 필자는 작은PR법칙을 잘 지키고 있냐고 물어본다면 대답은 NO다. 아직까지는 PR의 크기를 나누고, Issue의 발행 단위를 제대로 나누는 연습을 더 해야한다고 생각한다. 이론적으로는 이해했지만 아직까지 적용하기에는 내공이 부족한 것 같다. 이 글을 통해 CI/CD 프로세스를 구축하는것만으로 만족하지 말고 힘들게 구축한 프로세스를 제대로 활용할 수 있는 힌트가 되었으면 한다. reference https://blog.banksalad.com/tech/banksalad-code-review-culture/ https://engineering.linecorp.com/ko/blog/effective-codereview/#send-pull-request-early Twitter Facebook Google+ # 백엔드 # CI/CD",BE
564,"DB 리플리케이션 적용기 2021, Sep 30 INTRO DB Replication을 MySQL 공식 홈페이지에서 찾아보면 다음과 같이 말한다. Replication enables data from one MySQL databse server (known as a source) to be copied to one or more MySQL database servers (know as replicas) 출처 : 링크 즉, 하나의 데이터베이스(master/source)에서 다른 하나 또는 그 이상의 데이터베이스(slaves/replicas)로 데이터를 복제하여 저장하는 것이다. Replication은 비동기로 동작한다. 따라서 replicas가 master에 지속적으로 연결되어는 동기식으로 동작하지 않는다. 설정에 따라서 여러 데이터베이스, 선택된 데이터베이스, 선택된 테이블에만 replication을 적용할 수도 있다. MySQL replication 장점 공식 홈페이지에 나와있는 장점 4가지는 다음과 같다. Scale-out solutions 다수의 replicas를 두고 load를 분산해서 퍼포먼스를 높이는 장점이 있다. 대부분의 replication 적용이유이기도 하다. 쓰기 및 업데이트는 master 서버에서 이루어진다. 조회는 하나 또는 여러 slave 서버에 분산되서 처리된다. Data security master 서버와 slave 서버가 분리되어 있으므로 하나의 slave 서버에 문제가 생겨도 다른 slave 서버에 영향을 미치지 않고 데이터를 보존할 수 있다. 하지만 Master server에 장애가 생기면 문제가 생긴다. Analytics 실시간 데이터 생성 및 업데이터가 master 서버에서 이루어지는 동안 데이터 분석처리는 slave 서버에서 처리하여 master 서버에 성능저하를 전혀 일으키지 않도록 지원한다. Long-distance data distribution 리모트에 필요한 데이터를 위한 local 데이터 복제를 master에 접촉하지 않고 slave 서버에서 처리할 수 있다. 더 많은 정보를 위해서는 다음 링크를 참고한다. Replication 적용하기 적용이유 현재 진행중인 프로젝트에 DB replication을 적용하기로 했다. 그 이유는 프로젝트가 SNS의 일종이므로 유저에 의한 페이지 이동이 잦고 그것에 따른 조회 쿼리가 매우 많기 때문이다. 따라서 Master server 1개, slave server 2개를 두어 조회 쿼리를 slave 서버 2개로 분산했다. 참고사항 현재 본 프로젝트의 WAS가 AWS EC2 인스턴스에서 실행 중이며 이번에 DB Replication을 적용하면서 DB 서버를 분리했다. AWS EC2 인스턴스 3개를 추가로 생성해서 MySQL master 서버 1개 + slave 서버 2개를 구성했다. 쓰기 및 업데이트 작업은 master, 조회는 2개의 slave 서버를 RR(Round Robin) 방식으로 분산처리하도록 구성했다. DB는 MariaDB를 사용한다. 조회 작업은 Transaction의 read-only 속성을 통해 확인하고 slave db를 연결했다. 적용하기 DB replication 적용에는 크게 3가지 단계가 있다. 다음 레포지토리에 가면 적용을 위한 replication 학습테스트 코드를 확인할 수 있다. Remote 서버에 MariaDB 로컬 설치 및 기본 설정 Master 서버와 Slave 서버 replication 연결 설정 프로덕션 코드에 DB 수동 연결 및 (여러 slave 서버를 두고 있다면) slave DB 선택 로직 구현 1-1) MariaDB 설치 및 기본 설정 우분투에 MariaDB를 설치한다. $ sudo apt update
$ sudo apt install mariadb-server
현재 프로젝트를 위해서 제공받은 AWS 권한은 많이 닫혀있으므로 사용 가능한 포트(9000)으로 바꾸어주었다. 포트변경방법 프로젝트에 사용할 database를 생성한다. 현재 우리 프로젝트에서 사용하는 database는 pickgit이다. 각 DB 서버에 계정을 생성한다. create user 'replication'@'%' identified by 'password';
계정 이름 뒤에 %로 지정해야 전체에서 접속이 허용된다. 2-1) Master DB 설정 해당 계정에 권한을 부여한다. (master) $ grant all privileges on {database}.* to 'replication'@'%';

$ flush privileges;
위와 같이 하면 해당 계정에 대한 전체 권한이 열린다. 불안하다면 다음과 같이 replication에 대한 권한만 설정해도 된다. $ grant replication slave on *.* to 'replication'@'%';

$ flush privileges;
참고로 replication slave 권한을 줄 때는 *.*로 주지 않으면 db grant 및 global privileges 경고가 뜬다. 설정과정 다음 경로의 설정파일을 열어 수정한다. master db 서버의 서버 id를 설정하는 과정이다. 설정파일 경로 설정 수정 모든 설정이 끝난 뒤에 mysql를 재실행하여 설정을 적용한다. $ sudo service mysqld restart
Master DB 정보를 다음 명령어로 확인한다. File 값과 position 값으로 slave db에 master db에 대한 정보를 설정해야 한다. $ show master status;
위 두 정보가 의미하는 것이 무엇인지 확인하고 싶다면 다음 링크를 참고하자. The File column shows the name of the log file and the Position column shows the position within the file. In this example, the binary log file is mysql-bin.000003 and the position is 73. Record these values. You need them later when you are setting up the replica. They represent the replication coordinates at which the replica should begin processing new updates from the source. 간단히 말하면 replica가 master db의 데이터를 읽을 binary 파일과 읽기 시작할 위치인 position에 대한 정보이다. 2-2) Slave DB 설정 Master DB 과 동일하게 다음 설정경로로 가서 server-id를 수정한다. 현재 master의 server-id가 1이므로 slave1은 2, slave2는 3으로 설정해주었다. $ sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf
Slave Db에서 이전에 기록해둔 Master DB의 정보를 입력해 두 DB를 연결한다. mysql> change master to master_host={master_db_ip}, master_port={master_db_port}, master_user={master_username}, master_password={master_password}, master_log_file={master_bin_file}, master_log_pos={position};
Slave DB를 실행시킨다. mysql> start slave;
실행 시키고 다음 명령어를 치면 slave db의 상태와 master와의 연결상태 여부를 확인할 수 있다. 이제 master db에 데이터를 추가하면 slave db에도 적용이 되는 것을 확인할 수 있다. 3-1) Springboot DB configuration 설정 - datasource 정보 기입 DB 서버에서 하는 설정은 Master DB에 쓰기 및 업데이트 처리시 Slave DB에 적용이 되도록 하는 연결 설정이다. 이외의 datasource를 선택하고, 설정에 맞게 connection을 만들고, 실제 쿼리를 처리하도록 하는 것은 어플리케이션 코드에서 구현을 해야한다. 이전 yml datasource 설정 다음과 같이 datasource 정보를 yml 혹은 properties에 기록한다. datasource 정보 위 yml에 기입한 datasource 정보를 활용하기 위해서 다음과 같은 객체를 만들어 yml 정보를 바인딩 한다. 유의할 점은 내부에 선언된 정보를 위해서는 static inner class를 칼럼과 동일한 이름으로 생성해야 한다. 그러면 class 내부의 자료구조로 정보가 들어간다. getter 및 setter가 필수적으로 있어야한다. @ConfigurationProperties(prefix = ""datasource"") //이 annotation을 활용해서 yml 정보를 매핑한다.
public class MasterDataSourceProperties {

private final Map<String, Slave> slave = new HashMap<>();

private String url;
private String username;
private String password;

//getter 및 setter
//slave map에 대한 setter는 불필요하다.

public static class Slave { //중첩 데이터 명과 일치해야한다. 즉, datasource.slave의 두번째 요소와 동일한 이름으로 static class를 만들어야한다.

private String name;
private String url;
private String username;
private String password;

//getter 및 setter 생략
}
}
3-2) Springboot DB configuration 구현 위 입력한 datasource는 하나가 아니기 때문에 자동으로 연결이 안되고 상황에 따라 다른 datasource가 연결이 된다. 해당 작업을 수동으로 해야하기 때문에 몇가지 직접 설정해야하는 것들이 있다. 1) 첫번째는 적합한 상황에 다른 datasource를 제공하는 설정이다. 하나 이상의 datasource를 생성해 저장한다. 2) 두번째는 Jpa에 대한 entityManagerFactory @bean 설정이다. 본래 datasource가 자동연결되면서 JPA에 대한 설정도 되지만 여기서는 수동으로 해야한다. 이때 datasource가 매번 바뀌므로 entityManagerFactory 생성시 LazyConnectionDataSourceProxy 로 프록시 datasource를 연결해준다. 3) 세번재는 TransactionManager에 대한 설정이다. 이 또한 수동으로 datasource를 관리하려고 하니 추가해야하는 부분이다. 4) 기존에 자동으로 Datasource를 연결하던 설정을 해제하고, 수동으로 연결할 datasource의 properties를 지정해주어야 한다. (class 상단에 annotation으로 설정) @Configuration
@EnableAutoConfiguration(exclude = DataSourceAutoConfiguration.class) //4)
@EnableConfigurationProperties(MasterDataSourceProperties.class) //4)
public class DataSourceConfiguration {

private final MasterDataSourceProperties dataSourceProperties;
private final JpaProperties jpaProperties;

public DataSourceConfiguration(
MasterDataSourceProperties dataSourceProperties,
JpaProperties jpaProperties
) {
this.dataSourceProperties = dataSourceProperties;
this.jpaProperties = jpaProperties;
}

@Bean //1)번 부분
public DataSource routingDataSource() {
DataSource master = createDataSource(
dataSourceProperties.getUrl(),
dataSourceProperties.getUsername(),
dataSourceProperties.getPassword()
);

Map<Object, Object> dataSources = new HashMap<>();
dataSources.put(""master"", master);
dataSourceProperties.getSlave().forEach((key, value) ->
dataSources.put(value.getName(), createDataSource(
value.getUrl(), value.getUsername(), value.getPassword()
))
);

ReplicationRoutingDataSource replicationRoutingDataSource = new ReplicationRoutingDataSource();
replicationRoutingDataSource.setDefaultTargetDataSource(dataSources.get(""master""));
replicationRoutingDataSource.setTargetDataSources(dataSources);

return replicationRoutingDataSource;
}

private DataSource createDataSource(String url, String username, String password) {
return DataSourceBuilder.create()
.type(HikariDataSource.class)
.url(url)
.driverClassName(""org.mariadb.jdbc.Driver"")
.username(username)
.password(password)
.build();
}

@Bean //2)번 부분
public LocalContainerEntityManagerFactoryBean entityManagerFactory() {
EntityManagerFactoryBuilder entityManagerFactoryBuilder =
createEntityManagerFactoryBuilder(jpaProperties);

return entityManagerFactoryBuilder.dataSource(dataSource())
.packages(""com.pickgit.dbreplicationlearningtest"")
.build();
}

private EntityManagerFactoryBuilder createEntityManagerFactoryBuilder(
JpaProperties jpaProperties
) {
HibernateJpaVendorAdapter vendorAdapter = new HibernateJpaVendorAdapter();
return new EntityManagerFactoryBuilder(vendorAdapter, jpaProperties.getProperties(), null);
}

private DataSource dataSource() {
return new LazyConnectionDataSourceProxy(routingDataSource());
}

@Bean //3)번 부분
public PlatformTransactionManager transactionManager(
EntityManagerFactory entityManagerFactory
) {
JpaTransactionManager jpaTransactionManager = new JpaTransactionManager();
jpaTransactionManager.setEntityManagerFactory(entityManagerFactory);
return jpaTransactionManager;
}
}
3-3) 조회 쿼리시 datasource를 RR으로 선택하는 로직 현재 연결가능한 datasources들을 순회하면서 쓰기 및 업데이트면 master, 조회시에는 slave를 번갈아 선택하는 로직을 구현한다. public class ReplicationRoutingDataSource extends AbstractRoutingDataSource {

private static final Logger LOGGER = LoggerFactory.getLogger(ReplicationRoutingDataSource.class);

private SlaveNames slaveNames;

@Override
public void setTargetDataSources(Map<Object, Object> targetDataSources) {
super.setTargetDataSources(targetDataSources);

List<String> replicas = targetDataSources.keySet().stream()
.map(Object::toString)
.filter(string -> string.contains(""slave""))
.collect(toList());

this.slaveNames = new SlaveNames(replicas);
}

@Override
protected String determineCurrentLookupKey() {
boolean isReadOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly(); //조회 쿼리인 경우
if (isReadOnly) {
String slaveName = slaveNames.getNextName(); //다음 slave 선택

LOGGER.info(""Slave DB name: {}"", slaveName);

return slaveName;
}

return ""master"";
}
}

public class SlaveNames {

private final String[] value;
private int counter = 0;

public SlaveNames(List<String> slaveDataSourceProperties) {
this(slaveDataSourceProperties.toArray(String[]::new));
}

public SlaveNames(String[] value) {
this.value = value;
}

public String getNextName() {
int index = counter;
counter = (counter + 1) % value.length;
return value[index];
}
}
Replication Test 하기 Member를 입력하고 조회를 여러번 했을 때 의도된 대로 replication이 적용되는지 확인한다. @DataJpaTest로도 진행할 수 있으나, 빈으로 등록된 설정 요소들이 필요하기 때문에 @SpringBootTest로 테스트를 진행했다. (@DataJpaTest를 진행하면서 해당 configuration만 빈으로 등록하는 방식으로 테스트해도 무방하다.) datasource를 자동 연결하지 않는 설정 annotation을 class 상단에 추가해야한다. @SpringBootTest
@AutoConfigureTestDatabase(replace = Replace.NONE) //datasource 자동연결 x
@ActiveProfiles(""db"")
class MemberRepositoryTest {

@Autowired
private MemberRepository memberRepository;

@DisplayName(""Master DB에 데이터를 추가하면 slave DB에도 반영된다."")
@Test
void addMember_Success() {
// given
Member member = new Member(""pickgit"", 29);
memberRepository.save(member);
}

@DisplayName(""Slave DB에서 데이터를 조회한다 - 여러번 조회시 slave db를 번갈아 조회한다."")
@Test
void findMember_Success() {
// given
Member member = memberRepository.save( new Member(""pickgit"", 29));

// when
Member findMember1 = memberRepository.findById(member.getId())
.orElseThrow();
Member findMember2 = memberRepository.findById(member.getId())
.orElseThrow();
Member findMember3 = memberRepository.findById(member.getId())
.orElseThrow();
Member findMember4 = memberRepository.findById(member.getId())
.orElseThrow(); //조회 할 때마다 사용 DB 로거가 번갈아서 찍힌다.

// then
assertThat(findMember1.getId()).isNotNull();
assertThat(findMember1.getName()).isEqualTo(member.getName());
assertThat(findMember1.getAge()).isEqualTo(member.getAge());
}
}
결과 화면: 4번의 조회를 할때 1, 2 slave DB가 번갈아 선택된다. 마주한 이슈 yml에 properties 입력시 오타 주의 !! (자동완성 안해주기 때문에 접미사 -s 등을 주의해야함) JPA 정보 또한 자동연결할 때 해주는 설정들을 하나씩 다 명시해주어야한다. 기존에 ddl 전략을 외부에 기입했다면 왜인지 hbm2ddl.auto=create로 지정해야 적용이 되었다. 현재 JPA properties spring:
jpa:
properties:
hibernate:
show-sql: true
dialect: org.hibernate.dialect.MySQL8Dialect
format_sql: true
physical_naming_strategy: org.springframework.boot.orm.jpa.hibernate.SpringPhysicalNamingStrategy
hbm2ddl:
auto: create
generate-ddl: true
Slave DB 권한부여 및 bind address 오픈 Host의 접근이 허가되지 않는다는 오류가 날 때는 다음 두가지를 해주어야한다. slave db 계정 생성 및 권한 부여 (위 master db에 했던 작업과 동일) sudo vim /etc/mysql/mariadb.conf.d/50-server.cnf에 bind-address 부분 0.0.0.0 으로 지정 다음 링크를 참고하자. Springboot JPA에 대한 Hibernate Naming Strategy 지정 Springboot에서 자동으로 지정할 때는 알아서 네이밍전략이 설정되었으나, 수동을 할 때는 이 부분도 yml에 기입해주어야 한다. 그렇지 않으면 테이블 및 칼럼명이 그대로 camel case로 입력된다. yml에 다음 설정을 해서 DB에서 underscore로 지정되도록 전략을 지정한다. physical_naming_strategy: org.springframework.boot.orm.jpa.hibernate.SpringPhysicalNamingStrategy 번외) 기존 DB의 데이터 dump 하기 기존 DB에 있던 데이터들을 새로 생성한 master db에 옮기기 위해 mySqldump를 사용해 마이그레이션을 했다. $ mysqldump -u [사용자 계정] -p [원본 데이터베이스명] > [생성할 백업 파일명].sql #백업 sql 생성

#scp를 사용해 새로운 database가 있는 서버로 sql 파일 이동

$ mysql -u [사용자 계정] -p [복원할 DB] < [백업된 DB].sql #sql 파일을 사용해 데이터 복원
Master DB에만 적용하면 slave DB에 알아서 적용이 된다. [참고자료] https://mycup.tistory.com/237 https://velog.io/@max9106/DB-Spring-Replication https://mangkyu.tistory.com/97 https://velog.io/@kyujonglee/Mysql-%EB%B0%B1%EC%97%85-%EB%A7%88%EC%9D%B4%EA%B7%B8%EB%A0%88%EC%9D%B4%EC%85%98 백엔드 코다입니다 🙌 Twitter Facebook Google+ # 백엔드 # Database",BE
565,"Pick-Git의 Infrasturcture 2021, Sep 30 1. 들어가며 안녕하세요, 케빈입니다. Pick-Git 서비스 개발 초기에는 정말 몇 개 안되는 AWS EC2만을 사용했었는데요. 지속적으로 서비스를 개선 및 운영하면서 다양한 요구 사항을 직면하게 되었으며, 소기의 목적을 달성하기 위해 수 많은 AWS EC2를 생성해 사용하게 되었습니다. 그 결과 Pick-Git의 Infrasturcture가 다소 복잡해졌고, 프론트엔드뿐만 아니라 백엔드 팀원조차 자신이 맡은 영역이 아니면 제대로 이해하기 힘든 문제가 발생했습니다. 해당 문제를 해결하기 위해 Infrasturcture에 대한 팀내 WiKi 자료를 제작하게 되었는데요. 기술 블로그를 통해 독자님들에게도 Pick-Git 서비스를 지탱하는 Infrasturcture를 함께 공유하고자 합니다. 2. Overall Pick-Git에서 사용 중인 EC2는 크게 4가지 그룹으로 분류됩니다. Product 환경 관련 EC2 Develop 환경 관련 EC2 Test 환경 관련 EC2 기타 EC2 (Jenkins, SonarQube, nGrinder 등) 서비스에서 사용 중인 EC2 서버에 대한 SSH(22번 포트) 접근은 Bastion 서버 내부에서만 접속 가능하도록 구성했습니다. 개발 팀원은 자신의 Local PC에서 SSH를 통해 서비스 EC2 서버로 직접 접근할 수 없습니다. Bastion EC2 서버로 SSH 접속한 다음, Bastion 서버 내부에서 서비스 EC2 서버로 SSH 접속이 가능합니다. SSH(22번 포트)는 호스트들이 Public Network를 통해 통신할 때 보안적으로 안전하게 통신하기 위해 사용하는 프로토콜(공개키 & 비밀키 방식)입니다. 만약 SSH 관련 보안 결함이 발생하면 서비스에 심각한 문제를 야기할 수 있습니다. SSH(22번 포트)로 접근할 수 있는 모든 EC2 서버에 대해 동일한 수준의 보안을 설정하려면, Auto-Scaling 등 확장성을 고려한 구성과 배치가 필요합니다. Infrasturcture 규모가 커지면 관리 포인트 또한 늘어나기 때문에, 일반적으로 보안 설정을 일정 부분 포기해야 합니다. Bastion 서버로 보안을 집중 시키고, 새로 생성하는 EC2 서버를 Bastion 서버로 연결시키면 모든 서버가 동일한 수준의 보안을 손쉽게 가질 수 있습니다. 악성 루트킷 혹은 랜섬웨어 등으로 피해를 보더라도 Bastion Server만 재구성하면 되므로, 서비스에 끼치는 악영향을 최소화할 수 있습니다. Bastion 서버에 대한 구성 방법은 Bastion 서버 구성 글을 참고하시길 바랍니다. 3. Product 환경 서비스 초기에는 웹 서버(Product Reverse Proxy)에 프론트엔드 정적 파일을 배포한 다음, 정적 파일 요청은 웹 서버가 처리하고 동적인 컨텐츠 요청은 뒷단의 WAS로 요청을 위임해 처리하도록 구성했었습니다. 그러나 AWS에서 제공하는 CDN 서비스인 CloudFront로도 웹 서버가 제공하는 캐싱 기능 등의 이점을 충분히 누릴 수 있다고 판단했습니다. 따라서 AWS S3에 프론트엔드 정적 파일을 배포한 다음, S3 정적 파일 읽기 작업을 수행하는 CloudFront 주소에 Pick-Git 서비스 실 도메인 주소(https://pick-git.com)를 연결했습니다. Product Reverse Proxy 주소는 TLS 적용 및 Product API 서버 도메인 주소를 연결해둔 상태입니다. 이 외에도 Reverse Proxy는 Product WAS에 대한 로드 밸런싱 및 Blue/Green 배포 작업 등을 수행합니다. 서비스 초기에는 단일 DB 서버를 구성했으나, 단일 장애점을 없애고 가용성을 높이기 위해 DB Replication을 적용했습니다. DB Replication을 이번 글에서 전부 설명하긴 어려울 것 같은데요. 짧게 요약하자면 DB 스토리지를 물리적으로 다른 서버에 복제하는 것입니다. 2대 이상의 DB 서버가 동일한 데이터를 담도록 실시간으로 동기화하는 기술인만큼 DB 데이터 백업이 가능합니다. 하나의 Slave Node에 장애가 발생하더라도 다른 Slave Node(다중화)를 통해 서비스가 중단되지 않고 계속 정상 운영됩니다. 특히 Master Node 서버에 장애가 발생해 서버가 중지되더라도, Slave Node 중 1개를 Master Node로 승격시켜 서비스 중단 없이 데이터를 빠르게 복구하는 자동 Failover 기능을 제공합니다. Write 작업은 Master Node가 담당하고 Read 작업은 Slave Node가 담당함으로써 DB 서버 부하를 분산시킬 수 있습니다. 어플리케이션에서 DB 부하로 인한 병목 현상이 서비스 장애의 주 원인인데, 이를 개선하는데 도움이 됩니다. 이미지 업로드 요청 API의 경우 Product WAS가 S3로 이미지를 업로드합니다. 아울러 유저 검색 등의 기능을 위해 Elasticsearch를 사용 중입니다. 4. Test 환경 Test 환경은 백엔드 API와 시나리오에 대한 Load & Stress Test 수행 및 성능 진단을 목적으로 생성했습니다. 최대한 Product 환경과 유사하게 구성했으며, nGrinder 서버를 통해 테스트를 진행합니다. 테스트 대상 시스템의 성능을 측정하기 위해서 외부 시스템은 항상 기대한 결과만을 반환하는 환경이 필요합니다. 이 때, 어플리케이션에서 객체를 Mocking하거나 Dummy Controller를 사용하지 않았습니다. Http Connection Pool 미사용 Connection Thread 미사용 I/O 미발생 등의 문제가 존재하기 때문입니다. 성능 테스트에서 중요한 관점인 Thread 및 리소스 사용을 무시하게 되며, 테스트 시스템의 자원과 리소스를 함께 사용하는 등 테스트의 신뢰성이 떨어집니다. 테스트를 위한 요소는 테스트 대상 시스템에 절대로 영향을 미쳐서는 안 되기 때문에, S3 이미지 업로드 및 GitHub API와 같은 외부 시스템은 테스트 대상 시스템과 완벽히 분리된 Mock 서버를 만들어 배포한 다음 테스트를 진행했습니다. nGrinder를 활용한 부하 및 스트레스 테스트에 대한 상세한 내용은 우아한형제들 기술블로그의 결제 시스템 성능, 부하, 스트레스 테스트 글을 참고하시길 바랍니다. 5. Develop 환경 Develop 환경의 경우 주로 개발 과정에서 프론트엔드 및 백엔드 기능들이 유기적으로 잘 연동되는지 등을 확인하는 역할입니다. 웹 서버(Develop Reverse Proxy)에 프론트엔드 정적 파일을 배포합니다. 6. CI / CD Pipeline 전반적인 CI / CD Pipeline 플로우입니다. 하나씩 자세히 살펴보겠습니다. 6.1. SonarQube Pipeline Pick-Git 백엔드 프로젝트는 SonarQube를 통해 프로젝트 코드에 대한 정적 분석을 진행하고 있습니다. Backend 라벨이 달린 Pull Request가 생성 및 업데이트될 때마다 WebHook이 Jenkins 서버로 전송됩니다. Jenkins 서버의 SonarQube 파이프라인이 이를 감지하고 작업을 시작합니다. Build & Test 작업을 진행하고, JaCoCo 라이브러리를 통해 코드 커버리지 분석 리포트를 생성합니다. 최종적으로 SonarQube 서버에 정적 분석을 요청합니다. 이러한 분석 과정을 진행하면서 성공 혹은 실패했을 때 Jenkins는 GitHub Open API를 바탕으로 해당 PR에 결과를 댓글로 알림합니다. 테스트 커버리지 기준 미충족 등 파이프라인이 실패한 경우 Pull Request Merge를 방지하고 있습니다. 6.2. Deploy Pipeline Pull Request Merge가 발생하면 WebHook이 Jenkins 서버로 전송됩니다. 이 때, 어떤 브랜치로 코드가 병합됬으며 Pull Request의 라벨이 Backend와 Frontend 중 어떤 것인지 등의 정보를 Jenkins가 분석합니다. 이후 조건에 부합하는 Pipeline을 찾아 작업을 수행하고 적합한 서버로 코드를 배포합니다. 백엔드의 경우 Build & Test 작업 및 Rest Docs 등의 작업을 수행한 다음 코드를 배포합니다. 7. 마치며 이번 WiKi 자료를 제작하면서 커뮤니케이션의 중요성에 대해 다시금 생각해보는 계기가 되었습니다. 프론트엔드 팀원들과 기능 시연을 진행하던 도중 백엔드 이슈로 인해 시연이 중단되는 경우가 왕왕 있었는데요. 백엔드 팀원들은 나름대로(?) 문제 원인을 설명하고 디버깅에 착수하곤 했습니다. 그런데 문제 원인을 프론트엔드 팀원의 눈높이가 아닌 백엔드 입장에서 너무 간략하게 설명하고 넘어갔습니다. 😅 이후 회고 때, 프론트엔드 팀원이 문제 원인을 제대로 이해하지 못하고 백엔드가 문제가 해결할 때까지 그저 대기해야만 하니 약간 답답하다는 고충을 토로했습니다. 따라서 Infrasturcture 설명 자료를 제작하면서, 신규 기능 배포 과정에서 발생할 수 있는 백엔드 이슈들에 대한 FAQ를 정리해 프론트엔드 팀원들과 소통하는 시간을 가졌습니다. 항상 나만 아는 지식이 없도록 하자는 팀 규칙을 준수하려고 노력했는데요. 소통의 시작은 상대방과 눈높이를 맞추는 것에서 시작한다는 기본적인 격언부터 충실히 실천해야겠다고 느낀 하루였습니다. References 결제 시스템 성능, 부하, 스트레스 테스트 Twitter Facebook Google+ # 백엔드 # Infrasturcture",BE
566,"Gradle 프로젝트에 JaCoCo & SonarQube 적용 2021, Sep 30 1. 들어가며 안녕하세요, 케빈입니다. 개발 관련 서적 및 강의에서 단연 강조하는 내용 중 하나가 테스트 코드 작성의 중요성이 아닐까 싶습니다. 테스트 코드를 꼼꼼하게 작성하면 어떤 장점이 있을까요? 개발자가 작성한 기능들이 의도한대로 정상 동작하는지 확인할 수 있습니다. 기능 검증 과정을 통해 미처 생각하지 못한 엣지 케이스를 사전에 포착 및 대응함으로써 서비스의 안정성을 높여줍니다. 기존 기능이 수정되거나 신규 기능이 추가되었을 때 발생할 수 있는 부작용(Side-Effect)를 쉽게 발견하고 제거합니다. TDD를 통해 가독성 및 재사용성이 높은 객체지향적인 코드를 작성하는데 의식적인 노력을 할 수 있습니다. 소프트웨어는 변화가 잦은 만큼 개발 과정에서 예기치못한 부작용을 직면하기 쉽습니다. 이 때문에 Pick-Git 팀 또한 꼼꼼하게 테스트 코드를 작성하고자 의식적으로 노력 중입니다. 위의 장점을 제대로 누리기 위해서는 로직 분기상 발생할 수 있는 모든 경우의 수에 대해 빠짐없이 테스트 코드를 작성해야 합니다. 그러나 개발자도 사람인지라 테스트 코드를 작성하는 과정에서, 일부 로직에 대한 테스트 코드 작성이 인적 실수로 누락되고는 합니다. 우리가 작성한 테스트 코드가 얼마나 꼼꼼하게 잘 작성되었는지 정량적으로 확인할 수 있는 지표가 있다면, 이런 문제를 초기에 해결함으로써 코드 품질을 높게 유지할 수 있을 것입니다. 오늘 글에서 알아볼 코드 커버리지 및 정적 코드 분석 도구가 바로 테스트 코드 작성에 대한 대표적인 정량 지표입니다. 2. Code Coverage 코드 커버리지는 소프트웨어의 테스트 케이스가 얼마나 충족되었는지를 나타내는 지표 중 하나입니다. 테스트를 진행했을 때 코드 자체가 얼마나 실행되었는지 수치를 의미합니다. 코드의 안정성을 보장해주는 지표라고 생각하면 쉽습니다. 코드 커버리지는 대게 소스 코드를 기반으로 수행하는 테스트를 통해 측정합니다. 테스트 코드는 모든 시나리오에 대해 작성되는 것이 베스트지만, 인적 실수로 인해 누락되기 싶습니다. 코드 커버리지 분석을 통해 테스트가 누락된 부분을 빠르게 파악함으로써 인적 실수를 예방할 수 있습니다. 코드 커버리지는 후술할 정적 코드 분석 도구 등과 함께 활용하는데요. 코드 커버리지 기준에 미달하면 코드를 Merge하지 못하게 방지함으로써, 코드 커버리지 및 프로덕트 코드 품질을 항상 일정 수준 이상으로 유지하는 것이 일반적입니다. 2.1. 기준 코드의 구조는 크게 구문, 조건, 결정으로 분류할 수 있습니다. 코드 커버리지는 이러한 코드 구조를 얼마나 커버했느냐에 따라 측정 기준이 달라집니다. 예제 및 상세한 설명이 궁금하다면 코드 분석 도구 적용기 - 1편, 코드 커버리지(Code Coverage)가 뭔가요? 글을 참고 바랍니다. 구문(Statement) 라인 커버리지로서, 코드 한 줄이 한 번이상 실행되면 충족됩니다. 가장 많이 사용되는 기준이다. 조건(Condition) 모든 조건식의 내부 조건이 true/false을 가지게 되면 충족된다. 조건 커버리지를 기준으로 테스트를 진행할 경우, 구문 커버리지와 결정 커버리지를 만족하지 못하는 경우가 존재합니다. 결정(Decision) 브랜치 커버리지로서, 모든 조건식이 true/false을 가지게 되면 충족됩니다. 2.2. 각 기준의 장단점 조건 및 결정 커버리지는 코드 실행에 대한 테스트보다는 로직의 시나리오에 대한 테스트입니다. 조건(true/false)에 대해 모두 만족하면 코드 커버리지를 만족한다고 봅니다. 즉, 조건문이 존재하지 않는 코드는 두 커버리지의 대상에서 제외되며 해당 코드를 아예 테스트하지 않습니다. 반면 라인 커버리지는 조건문의 true/false 시나리오에 대해 완벽히 테스트했다고 보장할 수 없지만, 조건문 내부의 코드가 실행되었을 때 문제가 없다는 것은 보장할 수 있습니다. 다시 말해 라인 커버리지를 만족하면 모든 시나리오를 테스트한다는 보장은 할 수 없지만, 어떤 코드가 실행되더라도 해당 코드는 문제가 없다는 보장은 할 수 있다는 의미입니다. 따라서 라인 커버리지를 많이 사용하는 편입니다. 3. JaCoCo Java 어플리케이션의 코드 커버리지를 분석해주는 라이브러리입니다. 대안으로 Cobertura 및 Clover 등이 있으나 JaCoCo가 가장 레퍼런스가 많으며 사용 방법이 간단합니다. 코드 커버리지에 대한 결과를 HTML, CSV, XML 등의 리포트로 산출합니다. 설정한 커버리지 만족 여부를 확인할 수 있습니다. 4. Gradle 설정 build.gradle plugins {
id 'jacoco'
}

jacoco {
toolVersion = '0.8.7'
}
기본적으로 HTML 리포트는 $buildDir/reports/jacoco/test 디렉토리에 생성됩니다. build.gradle test {
useJUnitPlatform()
finalizedBy jacocoTestReport
}

jacocoTestReport {
dependsOn test
reports {
html.enabled true
xml.enabled true
csv.enabled true

html.destination file(""src/jacoco/jacoco.html"")
xml.destination file(""src/jacoco/jacoco.xml"")
}
finalizedBy 'jacocoTestCoverageVerification'
}

jacocoTestCoverageVerification {
violationRules {
rule {
element = 'CLASS'
enabled = true

limit {
counter = 'LINE'
value = 'COVEREDRATIO'
minimum = 0.60
}

limit {
counter = 'METHOD'
value = 'COVEREDRATIO'
minimum = 0.60
}
}
}
}
JaCoCo로 분석 리포트를 생성하는 Task 수행 전에 항상 테스트 Task 수행되도록 finalizedBy 및 dependsOn 명령어를 추가합니다. jacocoTestReport는 코드 커버리지 결과를 원하는 형태의 리포트로 생성해주는 Task입니다. 분석 리포트 수행시 생성되는 파일 종류를 설정할 수 있으며, build 디렉토리에 생성되는 파일을 원하는 위치로 가져올 수 있습니다. 이후 수행되는 jacocoTestCoverageVerification는 분석된 코드 커버리지가 미리 설정한 기준치에 충족하는지 여부를 확인합니다. 참고로 limit 구문에 들어갈 수 있는 Key 값과 Value 값은 다양하니 공식 문서를 참고하길 바랍니다. 기능적으로 테스트들은 모두 통과하더라도, violationRules에 정한 기준치를 통과하지 못하면 결국 jacocoTestCoverageVerification Task가 실패해서 빌드에 실패합니다. 코드 커버리지 충족이 어렵다면 기준을 현실적으로 적당히 타협한 뒤 점진적으로 향상하는 방법을 고려합시다. 아울러 테스트 할 필요가 없거나 하기 어려운 파일들은 인위적으로 검사에서 제외할 수 있습니다. build.gradle jacocoTestReport {
dependsOn test
reports {
html.enabled true
xml.enabled true
csv.enabled true

html.destination file(""src/jacoco/jacoco.html"")
xml.destination file(""src/jacoco/jacoco.xml"")
}

def Qdomains = []
for (qPattern in '**/QA'..'**/QZ') {
Qdomains.add(qPattern + '*')
}

afterEvaluate {
classDirectories.setFrom(
files(classDirectories.files.collect {
 fileTree(dir: it, excludes: [
 '**/BlogApplication*',
 '**/*Request*',
 '**/*Response*',
 '**/*Dto*',
 '**/*OAuthClient*',
 '**/*Interceptor*',
 '**/*Exception*',
 '**/*Storage*',
 '**/*BaseDate*',
 '**/*PageController*'
 ] + Qdomains)
})
)
}
finalizedBy 'jacocoTestCoverageVerification'
}

jacocoTestCoverageVerification {
def Qdomains = []
for (qPattern in '*.QA'..'*.QZ') {
Qdomains.add(qPattern + '*')
}

violationRules {
rule {
element = 'CLASS'
enabled = true

limit {
counter = 'LINE'
value = 'COVEREDRATIO'
minimum = 0.60
}

limit {
counter = 'METHOD'
value = 'COVEREDRATIO'
minimum = 0.60
}

excludes = [
 '**.*BlogApplication*',
 '**.*Request*',
 '**.*Response*',
 '**.*Dto*',
 '**.*OAuthClient*',
 '**.*Interceptor*',
 '**.*Exception*',
 '**.*Storage*',
 '**.*BaseDate*',
 '**.*PageController*',
] + Qdomains
}
}
}
afterEvaluate 옵션은 분석 리포트 생성시 특정 파일들을 제외합니다. excludes 옵션은 코드 커버리지 조건 충족 확인시 특정 파일들을 제외합니다. QueryDsl을 사용하는 경우 추가적으로 생성되는 QClass 파일들을 분석 리포트 생성 및 커버리지 조건 검사에서 제외시켜야 합니다. 테스트를 수행하면 build 디렉토리에 분석 리포트가 잘 생성된 것을 확인할 수 있습니다. 또한 build 디렉토리에 생성된 파일들이 원하는 경로로 복사된 것을 확인할 수 있습니다. 세부적으로 코드 커버리지를 확인할 수 있습니다. 4.1. Lombok 제외 lombok.config lombok.addLombokGeneratedAnnotation = true
해당 파일을 gradlew가 위치한 프로젝트 루트 위치에 저장장합니다. 이는 @Builder, @Getter 등의 테스트를 제외시킵니다. 5. SonarQube 정적 프로그램 분석이란 실제 실행 없이 컴퓨터 소프트웨어를 분석하는 것을 의미합니다. 정적 코드 분석 도구는 개발된 코드에서 버그, 코드 스멜, 보안 취약점 등 코드 품질을 지속적으로 검사해주는 플랫폼입니다. 앞서 언급한 JaCoCo는 코드 커버리지만 체크할 뿐 그 외적인 코드 품질 요소를 검사하는 기능은 없습니다. 이를 보완하기 위해 정적 코드 분석 도구를 함께 사용합니다. SonarQube는 대표적인 정적 코드 분석 도구입니다. PMD, FindBugs, CheckStyle 등 대안 도구가 존재하지만 레퍼런스가 상대적으로 부족합니다. 또한 SonarQube는 Github 및 Jenkins 등과 연동이 간편하며, 자동 정적 코드 분석을 쉽게 구성할 수 있습니다. 아울러 오픈 소스이며 플러그인 및 언어를 다양하게 지원합니다. JaCoCo(코드 커버리지 분석 리포트) 및 SonarQube(정적 분석 도구)를 함께 사용하면 개발된 코드를 자동 및 지속적으로 검사하게 됩니다. 이는 코드 커버리지 및 코드 품질 목표를 달성하고 일정 수준 이상을 유지하는데 기여합니다. 6. EC2 SonarQube 컨테이너 실행 Shell $ sudo apt update && sudo apt install -y docker.io && sudo apt install -y docker-compose
$ sudo usermod -aG docker ubuntu
Docker를 설치하고 이후 docker-compose.yml 파일을 작성합니다. docker-compose.yml version: '3'
services:
SonarQube:
image: sonarqube:7.7-community
container_name: SonarQube-conatiner
ports:
- 8080:9000
# environment:
# - sonar.jdbc.url=jdbc:mysql://localhost:3306/sonarqube?useUnicode=true&characterEncoding=utf8&rewriteBatchedStatements=true&useConfigs=maxPerformance
# - sonar.jdbc.username=sonar
# - sonar.jdbc.password=sonar
# volumes:
# - /home/SonarQube:/opt/sonarqube
SonarQube 7.8 이상부터는 JDK11 및 PostgreSQL만 지원하기 때문에 7.7을 사용했습니다. SonarQube 분석 결과를 별도로 DB에 저장할 수 있는데요. 원한다면 해당 옵션 주석을 해제하고 DB를 구성하면 됩니다. SonarQube는 기본적으로 9000 포트를 사용하는데, 위 예제의 경우 8080으로 접속하도록 설정했습니다. Shell $ docker-compose up -d
컨테이너를 실행합니다. 7. SonarQube 설정 http://{ec2-SonarQube-public-ip}:{port}로 접속한 뒤 로그인합니다. 기본 계정은 아이디 및 비밀번호가 모두 admin입니다. 프로젝트를 생성합니다. 토큰을 생성하고 메모해둡니다. 이후 친절하게 Gradle로 SonarQube Scanner를 실행하는 방법을 알려주는데요. 해당 방법은 Gradle 프로젝트에 SonarQube를 설정하는 방식이라 이번 글에서는 사용하지 않습니다. 생성한 프로젝트로 들어가서 Webhook을 위와 같이 설정해줍니다. Jenkins에서 SonarQube 정적 분석 요청을 보내면 처리 결과를 Jenkins로 보내기 위함입니다. 8. Jenkins 설정 아직 Jenkins를 구축하지 않았다면 Jenkins Pipeline를 통한 CI/CD 구현 글을 참고하여 Jenkins 서버를 구축합니다. 이후 다음 플러그인 3개를 설치합니다. SonarQube Scanner for Jenkins Sonar Quality Gates Plugin GitHub Pull Request Builder Jenkins 관리- Credentials 관리 탭에서 새로운 Credential을 추가합시다. 이전에 발급받은 SonarQube Token을 Secret text로 설정하면 됩니다. 그 다음 환경 설정에서 SonarQube Server 이름과 URL 및 토큰을 지정해주면 됩니다. 동일한 환경 설정 탭에서 GitHub Pull Request Builder를 설정해줍시다. Credentials는 GitHub Personal Access Token입니다. Admin list에 본인의 GitHub id를 기입해둡시다. Jenkins 관리- Global Tool Configuration에서 위 사진과 같이 SonarQube Scanner를 설정합니다. 8.1. GitHub Pull Request Builder 코드 배포를 위해 사용하는 일반적인 Jenkins 파이프라인은 특정 브랜치에 코드가 푸시될 때, 빌드 및 테스트 등을 진행하는 구조입니다. 그러나 SonarQube 파이프라인의 경우 PR이 생성되고 Merge되기 전에 파이프라인이 동작합니다. SonarQube 파이프라인은 다음 항목들을 검사할 계획입니다. Build & Test 성공인가? JaCoCo가 설정한 코드 커버리지 기준을 충족하는가? SonarQube가 설정한 코드 스멜 기준을 충족하는가? 만약 하나라도 실패한다면 PR Merge를 금지시킵니다. 이를 통해 코드의 문제점을 신속하게 특정하고 점진적으로 코드 퀄리티를 향상 시킬 수 있습니다. GitHub Pull Request Builder는 PR이 발생하면 스크립트를 실행하고, 빌드가 끝난 뒤에 결과를 리포팅 해주는 플러그인입니다. Jenkins가 PR에 결과 리포팅을 하기 위해 어드민의 Personal Access Token이 필요한 것입니다. 9. Pipeline 프로젝트 설정 먼저 SonarQube를 위한 Pipeline 프로젝트를 생성하고 위와 같이 설정해줍시다. 고급 버튼을 선택하여 Pipeline 스크립트 실행을 유발시키는 라벨과 그렇지 않은 라벨 및 화이트 리스트 등을 자유롭게 선택할 수 있습니다. 저는 backend 라벨을 붙인 PR에 한해서 정적 분석을 진행하고자 합니다. Use github hooks for build triggering을 체크한 뒤 저장했면, 정적 분석을 진행할 GitHub 프로젝트 레포지토리에 위와 같은 Webhook이 추가됩니다. 9.1. 참고 SonarQube를 사용하는 방법에는 크게 3가지가 있습니다. 복잡한 파이프라인이 불필요하다면 Freestyle로도 충분합니다. SonarQube 분석에 필요한 정보(서버 주소, 토큰, 파일 위치 등)를 Gradle 프로젝트에 기술하고 Jenkins Pipeline에서는 간단히 ./gradlew --info sonarqube를 실행하는 방식. 위 사진 처럼 Jenkins Freestyle 프로젝트를 통해 간단히 SonarQube를 사용하는 방식. Gradle 프로젝트에 별다른 설정 없이 이번 글처럼 Jenkins Pipeline을 통해 SonarQube를 사용하는 방식. 10. Pipeline 스크립트 작성 Pipeline Script pipeline {
agent any

stages {
stage('Checkout') {
steps {
checkout([$class: 'GitSCM', branches: [[name: '${sha1}']], extensions: [[$class: 'SubmoduleOption', disableSubmodules: false, parentCredentials: true, recursiveSubmodules: true, reference: '서브모듈 레포 주소', trackingSubmodules: true]], userRemoteConfigs: [[credentialsId: '액세스토큰 지정값', refspec: '+refs/pull/*:refs/remotes/origin/pr/*', url: '프로젝트 레포 주소']]])
}
}

stage('Build & Test') {
steps {
sh'''
cd backend/ && ./gradlew clean build || IS_FAIL=true
if [[ $IS_FAIL = ""true"" ]]; then
 echo ""Build Failure""
 /* 원한다면 GitHub API PR에 댓글다는 기능을 여기에 작성하여 실패 사유 알람을 보낼 수 있다. */
 exit 1
else
 echo ""Build Success""
fi
'''
}
}

stage('JaCoCo Report') {
steps {
sh'''
 cd backend/ && ./gradlew jacocoTestReport || IS_FAIL=true
 if [[ $IS_FAIL = ""true"" ]]; then
 echo ""JaCoCo Report Failure""
 /* 원한다면 GitHub API PR에 댓글다는 기능을 여기에 작성하여 실패 사유 알람을 보낼 수 있다. */
 exit 1
 else
 echo ""JaCoCo Report Success""
 fi
'''
}
}

stage('SonarQube Analysis') {
steps {
withSonarQubeEnv('환경설정에서 지정한 소나큐브 서버 이름') {
 sh'''
 cd backend/
 /var/jenkins_home/tools/hudson.plugins.sonar.SonarRunnerInstallation/지정한 소나큐브 스캐너 이름/bin/sonar-scanner 
 -D sonar.host.url=소나큐브 서버 주소 
 -D sonar.login=소나큐브 토큰 
 -D sonar.projectKey=소나큐브 프로젝트 키 
 -D sonar.projectName=소나큐브 프로젝트 이름 
 -D sonar.projectVersion=0.0.1-SNAPSHOT 
 -D sonar.sources=src/main/java 
 -D sonar.tests=src/test 
 -D sonar.java.binaries=build/classes 
 -D sonar.java.libraries= 
 -D sonar.java.libraries.empty=true 
 -D sonar.sourceEncoding=UTF-8 
 -D sonar.java.coveragePlugin=jacoco 
 -D sonar.coverage.jacoco.xmlReportPaths=src/jacoco/jacoco.xml 
 -D sonar.test.inclusions=**/*Test.java 
 -D sonar.exclusions=**/dto/**,**/*OAuthClient.java,**/S3*.java 
 '''
}
}
}

stage('SonarQube') {
steps {
script {
 def qg = waitForQualityGate();
 if (qg.status != 'OK') {
 sh'''
 echo ""SonarQube Quality Failure""
 /* 원한다면 GitHub API PR에 댓글다는 기능을 여기에 작성하여 실패 사유 알람을 보낼 수 있다. */
 '''
 error ""Pipeline aborted due to quality gate failure""
 }
 echo ""All Passed!""
 /* 원한다면 GitHub API PR에 댓글다는 기능을 여기에 작성하여 성공 사유 알람을 보낼 수 있다. */
}
}
}
}
}
Pipeline을 작성하는 방법은 Jenkins Pipeline를 통한 CI/CD 구현 글을 참고하길 바랍니다. Checkout의 경우 서브 모듈을 사용하고 있는 스크립트이기 때문에, 서브 모듈을 사용하지 않는다면 스크립트가 다를 수 있습니다. stage(‘Checkout’) branches: [[name: '${sha1}']]
refspec: '+refs/pull/*:refs/remotes/origin/pr/*'
Checkout의 경우 위 두 부분을 주의해서 작성해야 합니다. ${sha1} 는 PR을 발생시킨 브랜치 명을 의미합니다. stage(‘JaCoCo Report’) stage('JaCoCo Report') {
steps {
sh'''
cd backend/ && ./gradlew jacocoTestReport || IS_FAIL=true
if [[ $IS_FAIL = ""true"" ]]; then
echo ""JaCoCo Report Failure""
exit 1
else
echo ""JaCoCo Report Success""
fi
'''
}
}
JaCoCo 리포트를 생성하는 Task를 실행합니다. 사실 이번 글의 Gradle 예제에서는 JaCoCo 관련 Task가 Test Task와 의존 관계를 맺기에, 그 이전의 Stage인 Build & Test에서 ./gradlew clean build를 할 때 리포트가 생성됩니다. 따라서 해당 stage는 지워도 되는데요. Pick-Git 서비스의 경우 예제 Gradle 설정과 다르게 Test 및 JaCoCo 관련 Task들이 서로 의존 관계가 없는 상태라 추가해두었습니다. stage(‘SonarQube Analysis’) stage('SonarQube Analysis') {
steps {
withSonarQubeEnv('환경설정에서 지정한 소나큐브 서버 이름') {
sh'''
cd backend/
/var/jenkins_home/tools/hudson.plugins.sonar.SonarRunnerInstallation/지정한 소나큐브 스캐너 이름/bin/sonar-scanner 
-D sonar.host.url=소나큐브 서버 주소 
-D sonar.login=소나큐브 토큰 
-D sonar.projectKey=소나큐브 프로젝트 키 
-D sonar.projectName=소나큐브 프로젝트 이름 
-D sonar.projectVersion=0.0.1-SNAPSHOT 
-D sonar.sources=src/main/java 
-D sonar.tests=src/test 
-D sonar.java.binaries=build/classes 
-D sonar.java.libraries= 
-D sonar.java.libraries.empty=true 
-D sonar.sourceEncoding=UTF-8 
-D sonar.java.coveragePlugin=jacoco 
-D sonar.coverage.jacoco.xmlReportPaths=src/jacoco/jacoco.xml 
-D sonar.test.inclusions=**/*Test.java 
-D sonar.exclusions=**/dto/**,**/*OAuthClient.java,**/S3*.java 
'''
}
}
}
withSonarQubeEnv 블록을 사용해 연동할 SonarQube 서버를 선택할 수 있습니다. Jenkins global configuration에 저장된 연결 설정 정보를 자동으로 Scanner에 전달합니다. Freestyle 프로젝트는 설정 값을 넘겨주면 알아서 Sonar Scanner를 실행하지만, Pipeline의 경우 직접 실행 스크립트를 작성해야 합니다. -D 를 통해 제공하는 파라미터는 적당히 본인의 상황에 맞게 잘 조절하면 됩니다. stage(‘SonarQube Analysis’) stage('SonarQube analysis') {
// requires SonarQube Scanner 2.8+
def scannerHome = tool 'SonarQube Scanner 2.8';
withSonarQubeEnv('My SonarQube Server') {
sh ""${scannerHome}/bin/sonar-scanner""
}
}
코드가 너무 길다고 느껴지면 공식 문서를 참고해 축약하는 것을 시도해보시길 바랍니다. stage(‘SonarQube’) stage('SonarQube') {
steps {
script {
def qg = waitForQualityGate();
if (qg.status != 'OK') {
sh'''
echo ""SonarQube Quality Failure""
'''
error ""Pipeline aborted due to quality gate failure""
}
echo ""All Passed!""
}
}
}
waitForQualityGate를 사용하면, SaonrQube 서버가 분석을 완료하고 보낸 응답 결과를 받아볼 수 있습니다. SonarQube 쪽의 프로젝트에서 반드시 이 글 초기 설정처럼 Jenkins 서버에 대한 <your Jenkins instance>/sonarqube-webhook Webhook을 지정해두어야 합니다. SonarQube 서버가 정상적으로 분석을 처리하는 것을 확인할 수 있습니다. 10.1. 주의 Log /var/jenkins_home/tools/hudson.plugins.sonar.SonarRunnerInstallation/sonarQube-server/bin/sonar-scanner: not found
파이프라인 빌드할 때 위 예외가 발생하면, Jenkins EC2 내부의 볼륨 마운팅 디렉토리에 해당 경로가 존재하는지 확인합니다. 만약 없다면 신규 Pipeline 프로젝트 생성 후 다음과 같은 스크립트들을 빌드해보고, 해당 경로가 생겼는지 확인해봅시다. Pipeline Script stage(""SonarQube analysis"") {
steps {
script {
def scannerHome = tool 'SonarQube Scanner';
withSonarQubeEnv('지정한 소나 큐브 서버 이름') {
 sh 'mvn clean package sonar:sonar'
}
}
}
}
혹은 Pipeline Script stage('SonarQube analysis') {
withSonarQubeEnv('지정한 소나 큐브 서버 이름') {
// requires SonarQube Scanner for Maven 3.2+
sh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.2:sonar'
}
}
이래도 해결되지 않는다면 Sonar Scanner를 mvn으로 다운받는 방법들을 찾아 적용해보시길 바랍니다. /var/jenkins_home/tools/hudson.plugins.sonar.SonarRunnerInstallation/sonarQube-server/bin/sonar-scanner 디렉토리가 정상적으로 생성되면 기존의 SonarQube 스크립트가 잘 작동할 것입니다. 10.2. 기타 파이프라인 구축에 큰 문제가 없다면, PR 생성시 Jenkins가 스크립트 빌드를 시작합니다. 이후 코드 커버리지 및 정적 분석 기준 충족 여하에 따라 해당 PR에 결과를 리포팅합니다. 정적 분석 Pipeline이 실패하는 경우 특정 브랜치로의 Merge를 금지하도록 브랜치 보호 규칙을 설정할 수 있습니다. Reference Special Thanks to 손너잘 & 다니 코드 분석 도구 적용기 - 1편, 코드 커버리지(Code Coverage)가 뭔가요? 코드 분석 도구 적용기 - 2편, JaCoCo 적용하기 Gradle 프로젝트에 JaCoCo 설정하기 The JaCoCo Plugin - Gradle User Manual 좌충우돌 jacoco 적용기 SonarQube 정적분석 및 Jenkins CI/CD 통합 SonarQube: MySQL, PostgreSQL 전환 및 버전 업 Analyzing with SonarQube Scanner for Jenkins No tool named SonarQube Scanner 2.8 found error Set a SonarQube webhook in Jenkinsfile 빌드/테스트는 내가 해줄게. 너는 코딩에 집중해 (by GitHub Pull Request Builder) Twitter Facebook Google+ # 백엔드 # SonarQube # JaCoCo",BE
571,"Spring REST Docs 적용 (Gradle 7) 2021, Sep 28 1. Spring REST Docs 안녕하세요, 케빈입니다. Pick-Git 서비스 개발 초기에는 프론트엔드 팀원들과 API 스펙 협의 후 Notion에 작성해서 공유했었는데요. 해당 방법에는 여러 문제점이 존재했습니다. 수기로 작성하다 보니 인적 실수로 인해 오기재 하는 경우가 많았습니다. 또한 개발 과정에서 API 스펙 변경이 매우 빈번하게 발생하는데, 이를 미기재하는 실수로 인해 일정에 차질이 종종 생기곤 했습니다. 팀원 모두 조금 더 규격화된 API 문서화의 도입의 필요성을 느꼈습니다. 우리 팀이 선택한 도구는 Spring Rest Docs입니다. Spring REST Docs는 RESTful 서비스의 문서화를 도와주는 도구인데요. 문서 작성 도구로 기본적으로 Asciidoctor를 사용하고, 이를 통해 최종적으로 HTML을 생성합니다. 심지어 Markdown을 사용하도록 설정할 수 있습니다. 전반적인 흐름은 다음과 같습니다. Spring MVC Test, Spring WebFlux WebTestClient, RestAssured 등으로 테스트 코드를 작성한다. 해당 테스트 코드를 통해 Snippet이 자동 생성된다. 사용자가 미리 작성해둔 템플릿 문서와 Snippet을 결합해 최종적인 API 문서를 만들어낸다. 1.1. Swagger 대비 장점 비슷한 대안으로 Swagger가 있습니다. 이 또한 쉽게 API를 문서화할 수 있는데요. 특히 Swagger는 API 동작을 테스트하는데 특화되었습니다. 그러나 Swagger는 프로덕션 코드에 애너테이션을 부착하는 등 코드 침투라는 단점이 존재합니다. 반면 Spring REST Docs는 프로덕션 코드에 별다른 영향을 주지 않습니다. 아울러 테스트를 기반으로 생성되며, Snippet이 올바르지 않을 경우 테스트가 실패합니다. 테스트를 강제하며, 테스트가 검증되면 작성되는 문서 또한 신뢰할 수 있다는 장점이 존재합니다. 2. 설정 log Some problems were found with the configuration of task ':asciidoctor' (type 'AsciidoctorTask').
- In plugin 'org.asciidoctor.convert' type 'org.asciidoctor.gradle.AsciidoctorTask' method 'asGemPath()' should not be annotated with: @Optional, @InputDirectory.
설정을 위해 공식 문서에 기술된 방법을 참고했으나 위 에러에 계속 직면했습니다. build.gradle plugins {
id 'org.asciidoctor.jvm.convert' version '3.3.2'
}
Gradle 7 부터는 org.asciidoctor.convert가 아닌 asciidoctor.jvm.convert를 사용한다고 합니다. build.gradle dependencies {
testImplementation 'org.springframework.restdocs:spring-restdocs-mockmvc'
}
REST Docs를 RestAssured 혹은 WebTestClient로 작성하고 싶다면 spring-restdocs-webtestclient 혹은 spring-restdocs-restassured로 교체하면 됩니다. build.gradle ext {
snippetsDir = file('build/generated-snippets')
}

test {
outputs.dir snippetsDir
useJUnitPlatform()
}

asciidoctor {
inputs.dir snippetsDir
dependsOn test
}
Gradle 문법에 익숙하지 않지만 알아두면 좋은 개념들을 정리했습니다. dependsOn : 특정 Task가 의존하는 Task를 명시합니다. 즉, asciidoctor Task는 의존하는 test Task 이후에 수행됩니다. finalizedBy : 특정 Task의 후행 Task를 명시합니다. asciidoctor Task에 finalizedBy copy라고 명시되어 있으면, asciidoctor Task 이후 copy Task가 실행됩니다. build.gradle ext {
snippetsDir = file('build/generated-snippets')
}
생성된 스니펫의 저장 위치를 명시합니다. build.gradle test {
outputs.dir snippetsDir
useJUnitPlatform()
}
테스트 Task의 아웃풋 디렉토리를 스니펫 저장 위치로 설정합니다. build.gradle asciidoctor {
inputs.dir snippetsDir
dependsOn test
}
스니펫 저장 위치를 인풋으로 지정합니다. asciidoctor Task는 테스트 Task 수행 이후에 수행됩니다. 따라서 문서가 생성되기 전에 테스트가 수행되도록 보장합니다. 물론 꼭 의존성을 위와 같이 설정하지 않고 Jenkins Pipeline에서 Task를 적당히 순서에 맞춰 수행해도 됩니다. 3. 테스트 코드 작성 PostControllerTest.java // when, then
ResultActions resultActions = mockMvc.perform(get(""/api/posts/{id}"", ""1"")
.accept(MediaType.APPLICATION_JSON_VALUE))
.andExpect(status().isOk())
.andExpect(content().string(expected));

verify(postService, times(1)).readById(1L);

// restDocs
resultActions.andDo(document(""post-read-one"",
preprocessRequest(prettyPrint()),
preprocessRequest(prettyPrint()),
responseFields(fieldWithPath(""id"").description(""게시물 id""),
fieldWithPath(""title"").description(""제목""),
fieldWithPath(""content"").description(""내용""),
fieldWithPath(""author"").description(""글쓴이""),
fieldWithPath(""viewCounts"").description(""조회수""),
fieldWithPath(""createdDate"").description(""작성일""),
fieldWithPath(""modifiedDate"").description(""수정일""),
fieldWithPath(""imageUrls"").description(""사진 링크"")))
);
MockMvcRestDocumentation public static RestDocumentationResultHandler document(String identifier,
 OperationRequestPreprocessor requestPreprocessor, OperationResponsePreprocessor responsePreprocessor,
 Snippet... snippets) {
//...
}
andDo()메서드에 document()를 추가하여 스니펫을 생성합니다. 가장 첫 번째 인자는 스니펫의 Identifier입니다. preprocessRequest(prettyPrint())를 넣어주면 요청과 응답의 내용을 형식화함으로써 가독성이 높아집니다. document() 메서드는 오버로딩되어 있어서 prettyPrint()를 요청, 응답, 요청 및 응답 모두 등 총 3가지에 적용할 수 있습니다. Snippet의 경우 요청 헤더, 응답 헤더, 응답 본문 데이터 필드 등 다양하게 작성할 수 있는데요. 사용 예제 및 공식 문서를 보면서 학습하면 이해가 빠를 것입니다. PostControllerTest.java // restDocs
resultActions.andDo(document(""post-read-multiple"",
getDocumentRequest(),
getDocumentResponse(),
responseFields(fieldWithPath(""simplePostResponses[].id"").description(""게시물 id""),
fieldWithPath(""simplePostResponses[].title"").description(""제목""),
fieldWithPath(""simplePostResponses[].content"").description(""내용""),
fieldWithPath(""simplePostResponses[].author"").description(""글쓴이""),
fieldWithPath(""simplePostResponses[].viewCounts"").description(""조회수""),
fieldWithPath(""simplePostResponses[].createdDate"").description(""작성일""),
fieldWithPath(""simplePostResponses[].modifiedDate"").description(""수정일""),
fieldWithPath(""startPage"").description(""시작 페이지""),
fieldWithPath(""endPage"").description(""끝 페이지""),
fieldWithPath(""prev"").description(""이전 페이지 여부""),
fieldWithPath(""next"").description(""다음 페이지 여부"")))
);
List가 포함된 응답은 []를 통해 처리합니다. 만약 응답이 단일 리스트만을 포함하고 있다면 예제와 다르게 [].id, [].title과 같이 작성할 수 있습니다. PostControllerTest.java resultActions.andDo(document(""post-write-login"",
getDocumentRequest(),
getDocumentResponse(),
requestHeaders(headerWithName(HttpHeaders.AUTHORIZATION).description(""Bearer token"")),
requestPartBody(""images""),
responseHeaders(headerWithName(HttpHeaders.LOCATION).description(""게시물 주소"")))
);
헤더 또한 검증이 가능합니다. Gradle 옵션에서 Test를 실행하면 다음과 같이 스니펫이 생성됩니다. 테스트 코드를 통해 생성된 스니펫과 사용자가 미리 작성해둔 템플릿 문서를 결합해 최종 API HTML 문서를 생성합니다. 사용자가 미리 작성해둘 파일은 Gradle 기준 src/docs/asciidoc에 위치시킵니다. 이 때 파일 확장자는 .adoc입니다. api.adoc ifndef::snippets[]
:snippets: ./build/generated-snippets
endif::[]

== User
=== 회원탈퇴 (비로그인)
==== Request
include::{snippets}/withdraw-not-login/http-request.adoc[]
==== Response
include::{snippets}/withdraw-not-login/http-response.adoc[]

=== 회원탈퇴 (로그인)
=== 게시물 단건 조회
==== Request
include::{snippets}/post-read-one/http-request.adoc[]
==== Response
include::{snippets}/post-read-one/http-response.adoc[]
==== Fields
include::{snippets}/post-read-one/response-fields.adoc[]
정적 블로그를 꾸미듯이 API 문서 또한 조금만 더 노력을 들이면 매우 아름답게 구성할 수 있습니다. 또한 각각의 스니펫 identifier 폴더 하위에는 첨부할 수 있는 다양한 adoc들이 존재하는데, 상황에 맞게 잘 선택하시면 됩니다. build.gradle bootJar {
dependsOn asciidoctor
copy {
from ""${asciidoctor.outputDir}""
into 'BOOT-INF/classes/static/docs'
}
}
해당 설정을 추가해주고, 이번에는 Gradle 옵션에서 bootJar를 실행해봅시다. bootJar를 실행하면 src/docs/asciidoc에 위치시킨 사용자가 정의한 adoc 파일과 테스트 코드에서 생성된 스니펫들이 조합된 최종 API 문서가 build/docds/asciidoc 디렉토리에 생성됩니다. build.gradle bootJar {
dependsOn asciidoctor
copy {
from ""${asciidoctor.outputDir}""
into 'BOOT-INF/classes/static/docs'
}
finalizedBy 'copyDocument'
}

task copyDocument(type: Copy) {
dependsOn bootJar
from file(""build/docs/asciidoc"")
into file(""src/main/resources/static/docs"")
}
bootJar Task를 통해 api.html이 생성되면, 후행 Task로 copyDocument가 실행되도록 합시다. copyDocument가 수행되면 classpath(resources)로 API 문서가 복사됩니다. bootJar로 실행하면 컴파일을 거치고 테스트 Task가 시작됩니다. 테스트 Task가 종료되면 의존 관계에 따라 asciidoctor, bootJar, copyDocument Task가 각각 실행됩니다. 어플리케이션을 실행하면 잘 접속되는 것을 확인할 수 있습니다. 3. 기타 build.gradle asciidoctor.doFirst {
delete file('src/main/resources/static/docs')
}
만약 문서에 수정이 발생했음에도 반영되지 않는다면 다음 태스크를 추가하여 복사 이전에 기존에 있던 클래스패스의 API 문서를 제거하도록 합시다. Reference Spring REST Docs [Spring] Spring rest docs 적용기(gradle 7.0.2) Configuring asciidoctor when using Spring Restdoc Twitter Facebook Google+ # 백엔드 # Spring",BE
572,"Log4j2 및 Logback의 Async Logging 성능 테스트 비교 2021, Sep 29 1. 들어가며 안녕하세요, 케빈입니다. Java 및 Spring 진영에서 사용할 수 있는 Logging Framework는 Log4j와 Logback 및 Log4j2 등 다양합니다. Pick-Git을 개발하면서 팀원들과 어떤 Logging Framework를 선택해야할지 고믾이 많았습니다. Logging Framework 종류별 특징을 학습하는 것에서 그치지 않고, 직접 Logging Framework들의 비동기 로깅(Async Logging) 성능을 테스트해보고 결과를 비교했습니다. 최종적으로 Log4j2를 선택하게 되었는데요. 의사 결정 과정에 대해 공유하고자 합니다. 2. Logging Framework 종류별 특징 및 관련 용어 정리 Logging Framework 종류별 특징 및 관련 용어 등을 먼저 정리해보고자 합니다. 이미 잘 알고 계시는 독자님들이라면 다음 절로 넘어가셔도 됩니다. 이번 절의 내용은 제가 예전에 정리한 Logging 및 Spring Boot Logback 설정 글에서 발췌했습니다. 초기의 Spring은 JCL(Jakarta Commons Logging)을 사용하여 로깅을 구현했습니다. JCL을 사용하면 기본적인 인터페이스인 Log와 Log 객체 생성을 담당하는 LogFactory만 구현하면 로깅 구현체 교체가 자유롭습니다. JCL은 GC가 제대로 작동하지 않는 등의 단점이 존재했는데요. 이 때 도입된 것이 SLF4J입니다. SLF4J는 Java 진영의 다양한 로깅 프레임워크들에 대한 공용 인터페이스(Facade)입니다. 공용 인터페이스를 통해 특정 로깅 프레임워크에서 다른 로깅 프레임워크로 쉽게 전환할 수 있습니다. JCL이 지닌 문제 해결을 위해, 클래스 로더 대신에 컴파일 시점에 구현체를 선택하도록 변경되었습니다. 다음은 각 Logging Framework 종류별 특징입니다. Log4j : Apache의 Java 기반의 로깅 프레임워크입니다. 가장 오래된 로깅 프레임워크며, 현재는 개발이 중단되었습니다. Logback : Log4j 이후에 출시된 로깅 프레임워크로, Spring Boot가 기본적으로 사용합니다. Log4j보다 향상된 성능 및 필터링 옵션을 제공합니다. 로그 레벨 변경 등에 대해 서버를 재시작할 필요 없이 자동 리로딩을 지원합니다. Log4j2 : 파일뿐만 아니라 HTTP, DB, Kafka에 로그를 남길 수 있으며 비동기적인 로거를 지원합니다. 로깅 성능이 중요시될 때 Log4j2 사용을 고려합니다. Log4j2 사용을 위해서는 spring-boot-starter-logging 모듈을 exclude하고 spring-boot-starter-logging-log4j2 의존성을 주입해야 합니다. Logback과 달리 멀티 쓰레드 환경에서 비동기 로거(Async Logger)의 경우 Log4j 1.x 및 Logback보다 성능이 우수합니다. 3. Async Logging Test.java public void log() {
for (int i = 0; i < 10; i++) {
foo.bar();
log.info(""foo invoked"");
}
}
별도의 설정없이 로그 메시지를 남기면 동기적인 방식으로 파일 쓰기 작업이 진행됩니다. 즉, 로깅 메서드를 호출하는 시점(로그 발생)에 매번 파일 쓰기 작업이 수행된다. 파일 쓰기와 같은 I/O는 인메모리 작업이 아니라서 상대적으로 딜레이 및 부하가 큽니다. 반면 비동기 로깅을 사용하면 로그 발생과 로그 쓰기를 분리시킵니다. 로깅 메서드가 호출되면(로그 발생), Thread A는 인메모리 Queue 혹은 Buffer에 로그 정보를 집어넣습니다. Thread B는 Queue 혹은 Buffer에 쌓인 로그 데이터를 꺼내서 파일 쓰기만 수행합니다. 로깅 프레임워크마다 비동기 로깅 세부 구현이 다소 다르기 때문에 성능 차이가 발생할 수 있습니다. 3.1. 장점 로깅 메서드를 호출하는 시점(로그 발생)에 파일 I/O 작업이 바로 수행되지 않기 때문에, 어플리케이션의 성능이 빨라집니다. 3.2. 단점 Queue 혹은 Buffer의 용량이 80%를 도달하면, WARN 레벨 미만의 로그는 큐에서 제거되는 등 유실의 위험이 존재합니다. 또한 Queue 혹은 Buffer에 로그가 쌓인 상태에서 프로세스가 종료되면, 해당 로그는 기록되지 않습니다. 물론 옵션 조정을 통해 로깅 성능을 일부 타협하는 대신 로그 유실 가능성을 낮출 수 있습니다. Queue 혹은 Buffer 크기를 조절할 수 있습니다. Queue 혹은 Buffer가 꽉찬 경우 로그를 유실시킬지, 블락킹 상태에서 기다릴지 등을 선택할 수 있습니다. 또한 비동기 로깅은 기본적으로 Method Name 및 Line Number를 출력하지 않습니다. 이 또한 옵션 설정을 통해 출력을 가능하게 만들 수 있으나, 5 ~ 20배 정도 속도가 저하됩니다. 3.3. Sync & Async Test.js function a() {
let result = b();
console.log(result);
console.log(""a is done"");
}

function b() {
return 10;
}

/*
실행 결과
10
a is done
*/
Synchronous(동기)란 작업을 요청한 후 작업의 결과가 나올 때까지 기다린 후 처리하는 것을 의미합니다. A 함수가 B 함수를 호출했을 때, A 함수가 B 함수의 수행 결과 및 종료를 신경쓰는 경우를 예로 들 수 있습니다. 일반적인 경우 Blocking과 동일한 의미로 사용될 수 있습니다. Test.js function a() {
fetch(url, options)
.then(response => console.log(""response arrives""))
.catch(error => console.log(""error thrown""));
console.log(""a is done"");
}

/*
실행 결과
a is done
response arrives
*/
반면 Asynchronous(비동기)란 두 주체가 서로의 시작/종료 시간과는 관계 없이 별도의 수행 시작/종료시간을 가지고 있는 것을 의미합니다. A 함수가 B 함수를 호출했을 때, 호출된 함수의 수행 결과 및 종료를 호출된 함수 혼자 직접 신경 쓰고 처리하는 경우를 예로 들 수 있습니다. 대게 결과를 돌려주었을 때 순서와 결과(처리)에 관심이 있는지 아닌지로 판단할 수 있습니다. 4. Log4j2 Async Logger Performance Log4j2 Performance 공식 문서에 기재된 여러 벤치마크 결과 지표에 따르면, Log4j2 AsyncLogger를 사용한 비동기 로깅의 성능이 가장 우수합니다. Logback 및 Log4j2 모두 AsyncAppender라고 하는 전통적인 비동기 로깅 기능을 지원하는데, 이는 ArrayBlockingQueue 를 사용합니다. 반면 비교적 나중에 공개된 Log4j2의 AsyncLogger는 LMAX Disruptor라는 Lock-Free Inter-Thread Communication 라이브러리를 사용함으로써, 더 높은 처리량 및 낮은 지연 시간을 달성했습니다. 실제로도 Log4j2의 AsyncLogger를 활용한 비동기 로깅이 Log4j2의 AsyncAppender 및 Logback의 AsyncAppender를 활용한 비동기 로깅보다 우수할까요? 개인적인 호기심이 생겨 테스트를 진행해보았습니다. 5. Log4j2 Async Logging 성능 테스트 5.1. 테스트 환경 build.gradle dependencies {
implementation 'org.springframework.boot:spring-boot-starter-log4j2'
implementation 'com.lmax:disruptor:3.4.2'

modules {
module(""org.springframework.boot:spring-boot-starter-logging"") {
 replacedBy(""org.springframework.boot:spring-boot-starter-log4j2"", ""Use Log4j2 instead of Logback"")
}
}

//기타 Spring 관련 의존성 생략
}
Log4j2 및 Disruptor 의존성을 추가해주고, Spring Boot가 기본적으로 제공하는 spring-boot-starter-logging 모듈은 제외해줍니다. LogController.java @RequiredArgsConstructor
@RestController
public class LogController {

private final LogService logService;

@GetMapping(""/log"")
public String log() {
logService.log();
return ""log"";
}
}
LogService.java @Slf4j
@Transactional(readOnly = true)
@Service
public class LogService {

public void log() {
for (int i = 0; i < 10_000; i++) {
 log.info(""info log"");
}
}
}
최대한 Async Logging의 성능을 저하시키는 방향으로 Logger를 설정한 다음, nGrinder를 통해 테스트를 진행했습니다. 10분동안 VUser 1000명이 GET /log로 요청을 보냅니다. 5.2. Logback Async Logging (with AsyncAppender) logback-spring.xml <?xml version=""1.0"" encoding=""UTF-8""?>
<configuration debug=""false"">
<include resource=""org/springframework/boot/logging/logback/base.xml""/>
<property name=""home"" value=""logs/""/>

<appender name=""info-logger"" class=""ch.qos.logback.core.rolling.RollingFileAppender"">
<file>${home}info.log</file>
<append>false</append>
<immediateFlush>false</immediateFlush>
<rollingPolicy class=""ch.qos.logback.core.rolling.TimeBasedRollingPolicy"">
<fileNamePattern>${home}info-%d{yyyyMMdd}-%i.log</fileNamePattern>
<timeBasedFileNamingAndTriggeringPolicy class=""ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"">
<maxFileSize>15MB</maxFileSize>
</timeBasedFileNamingAndTriggeringPolicy>
<maxHistory>30</maxHistory>
</rollingPolicy>
<encoder>
<charset>utf8</charset>
<Pattern>
%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %t %class{36}.%M L:%L %n > %m%n
</Pattern>
</encoder>
</appender>

<appender name=""file-async"" class=""ch.qos.logback.classic.AsyncAppender"">
<appender-ref ref=""info-logger""/>
<includeCallerData>true</includeCallerData>
<queueSize>1024</queueSize>
<neverBlock>false</neverBlock>
<discardingThreshold>0</discardingThreshold>
</appender>

<logger name=""com.example.logging"" level=""INFO"" additivity=""false"">
<appender-ref ref=""file-async""/>
</logger>
</configuration>
먼저 Logback AsyncAppender를 사용하는 비동기 로깅을 테스트를 진행했습니다. 추후 테스트할 Log4j2의 AsyncAppender와 최대한 비슷하게 동작하도록 옵션들을 설정했습니다. RollingFileAppender append : true(기본값)면 로그 출력이 기존 파일에 추가되며, false면 기존 파일의 내용이 모두 삭제되고 출력이 쌓입니다. immediateFlush : true(기본값)면 로그 메세지들이 전혀 버퍼되지 않습니다. AsyncAppender includeCallerData : 비동기 로깅에서도 Method Name 및 Line Number 등 위치 정보를 출력하게 해주는 옵션입니다. queueSize : 기본값은 256지만 Log4j2의 기본값과 동일하게 1024로 설정합니다. neverBlock : false(기본값)면 로그 발생시 Queue에 넣을 공간이 없으면 빈 공간이 생길 때 까지 블락킹 상태로 기다리며, 로그를 유실하지 않습니다. discardingThreshold : Queue의 남은 용량이 {해당 설정값 n}% 이하가 되면, WARN 미만 로그가 유실되기 시작합니다. 기본 값은 20이며, Queue에 남은 용량이 20% 이하가 되면 로그 유실이 발생합니다. 0으로 세팅하면 Queue에 쌓인 로그를 드랍하지 않습니다. TPS는 평균 4.0, 최대 6.0입니다. MTT는 125,214입니다. 총 수행된 테스트는 3,916개인데, 이 중 에러는 1,608개로 에러율이 41%였습니다. 후술할 Log4j2의 동기적인 로깅 방식보다 성능이 나쁜 점을 볼 때, 아무래도 AsyncAppender를 너무 나쁘게 설정한 것 같습니다. 😅 5.3. Log4j2 Sync Logging log4j2.xml <?xml version=""1.0"" encoding=""UTF-8""?>
<Configuration>
<Properties>
<Property name=""log-path"">logs/log</Property>
<Property name=""log-pattern"">%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %t %class{36}.%M L:%L %n >
%m%n
</Property>
<Property name=""file-pattern"">%d{yyyy-MM-dd}-%i.log.gz</Property>
</Properties>

<Appenders>
<RollingFile name=""FileAppender"" fileName=""${log-path}.log""
filePattern=""${log-path}-${file-pattern}"" append=""false"">
<PatternLayout pattern=""${log-pattern}""/>
<Policies>
<SizeBasedTriggeringPolicy size=""15MB""/>
<TimeBasedTriggeringPolicy modulate=""true"" interval=""1""/>
</Policies>
<DefaultRolloverStrategy>
<Delete basePath=""logs/log"" maxDepth=""1"">
<IfLastModified age=""30d""/>
</Delete>
</DefaultRolloverStrategy>
</RollingFile>
</Appenders>

<Loggers>
<Logger name=""com.example.logging"" level=""info"" additivity=""false"">
<AppenderRef ref=""FileAppender""/>
</Logger>
</Loggers>
</Configuration>
이번에는 Log4j2를 사용하되 비동기 로깅이 아닌 동기적인 로깅 방식으로 테스트를 진행했습니다. TPS는 평균 5.6, 최대 8.0입니다. MTT는 126,803입니다. 총 수행된 테스트는 3,891개인데, 이 중 에러는 684개로 에러율이 17.6%였습니다. Log org.springframework.transaction.CannotCreateTransactionException: Could not open JPA EntityManager for transaction; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection, Could not open JPA EntityManager for transaction; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection
하나의 트랜잭션이 커넥션을 점유하면서 1만번 로그 파일 쓰기 I/O 작업을 진행하는데요. 동기적인 처리로 인해 트랜잭션 수행 시간이 길어지다 보니, DB 커넥션을 30초 이상 얻지 못해 예외를 던지는 스레드가 많이 발생했습니다. 5.4. Log4j2 Async Logging (with AsyncAppender) log4j2.xml <?xml version=""1.0"" encoding=""UTF-8""?>
<Configuration>
<Properties>
<Property name=""log-path"">logs/log</Property>
<Property name=""log-pattern"">%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %t %class{36}.%M L:%L %n >
%m%n
</Property>
<Property name=""file-pattern"">%d{yyyy-MM-dd}-%i.log.gz</Property>
</Properties>

<Appenders>
<RollingFile name=""FileAppender"" fileName=""${log-path}.log""
filePattern=""${log-path}-${file-pattern}"" immediateFlush=""false"" append=""false"">
<PatternLayout pattern=""${log-pattern}""/>
<Policies>
<SizeBasedTriggeringPolicy size=""15MB""/>
<TimeBasedTriggeringPolicy modulate=""true"" interval=""1""/>
</Policies>
<DefaultRolloverStrategy>
<Delete basePath=""logs/log"" maxDepth=""1"">
<IfLastModified age=""30d""/>
</Delete>
</DefaultRolloverStrategy>
</RollingFile>

<Async name=""asyncAppender"" includeLocation=""true"">
<AppenderRef ref=""FileAppender""/>
</Async>
</Appenders>

<Loggers>
<Logger name=""com.example.logging"" level=""info"" additivity=""false"">
<AppenderRef ref=""asyncAppender""/>
</Logger>
</Loggers>
</Configuration>
Log4j2의 AsyncAppender를 활용한 비동기 로깅을 테스트했습니다. RollingFile append 및 immediateFlush 옵션은 Logback과 동일합니다. AsyncAppender includeLocation : Logback의 includeCallerData 옵션과 동일합니다. 기본값은 false이고, 사용하게 될 경우 속도가 5 ~ 20배 정도 느려집니다. bufferSize : 기본값은 1024입니다. blocking : true(기본값)면 로그 발생시 Queue에 넣을 공간이 없으면 빈 공간이 생길 때 까지 블락킹 상태로 기다리며, 로그를 유실하지 않습니다. false면 로그 발생시 Queue가 꽉 찼다면 로그를 유실하고, Error Appender에 이벤트를 기록합니다. TPS는 평균 7.0, 최대 9.0입니다. MTT는 112,305입니다. 총 수행된 테스트는 4,506개인데, 이 중 에러는 488개로 에러율이 10.8%였습니다. 확실히 Logback의 AsyncAppender 및 Log4j2의 Sync Logging보다 성능이 좋습니다. 5.5. Log4j2 Async Logging (with AsyncLogger) log4j2.xml <?xml version=""1.0"" encoding=""UTF-8""?>
<Configuration>
<Properties>
<Property name=""log-path"">logs/log</Property>
<Property name=""log-pattern"">%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %t %class{36}.%M L:%L %n >
%m%n
</Property>
<Property name=""file-pattern"">%d{yyyy-MM-dd}-%i.log.gz</Property>
</Properties>

<Appenders>
<RollingFile name=""FileAppender"" fileName=""${log-path}.log""
filePattern=""${log-path}-${file-pattern}"" immediateFlush=""false"" append=""false"">
<PatternLayout pattern=""${log-pattern}""/>
<Policies>
<SizeBasedTriggeringPolicy size=""15MB""/>
<TimeBasedTriggeringPolicy modulate=""true"" interval=""1""/>
</Policies>
<DefaultRolloverStrategy>
<Delete basePath=""logs/log"" maxDepth=""1"">
<IfLastModified age=""30d""/>
</Delete>
</DefaultRolloverStrategy>
</RollingFile>
</Appenders>

<Loggers>
<AsyncLogger name=""com.example.logging"" level=""info"" additivity=""false"" includeLocation=""true"">
<AppenderRef ref=""FileAppender""/>
</AsyncLogger>
</Loggers>
</Configuration>
마지막으로 Log4j2의 AsyncLogger를 활용한 비동기 로깅을 테스트했습니다. TPS는 평균 16.3, 최대 19.5입니다. MTT는 58,116입니다. 총 수행된 테스트는 9,351개인데, 이 중 에러는 32개로 에러율이 0.3%였습니다. 성능이 저하될 수 있는 옵션을 끄고 테스트를 진행한 결과, TPS는 평균 68.2 및 최대 77로 매우 높게 향상되는 것을 확인할 수 있었습니다. 6. 마치며 Logback의 AsyncAppender와 Log4j2의 AsyncAppender 및 Log4j2의 AsyncLogger 등의 비동기 로깅 성능 테스트를 비교 분석해보았습니다. 공식 문서의 벤치 마크 지표를 보고 호기심이 생겨 시작한 실험이었는데, 의의보다는 한계가 더 큰 것 같습니다. 테스트를 위해 각 Logging Framework들이 최대한 비슷하게 동작하도록 여러 옵션들을 설정했는데요. 결과적으로는 AsyncLogger의 성능이 가장 우수하다는 결론을 도출할 수 있었지만, 옵션 설정과 같은 여러 테스트 통제 환경이 비동기 로깅 성능들을 객관적으로 비교하는데 충분했는지 의문입니다. Logging Framework에서 제공하는 비동기 로깅 기능에 대한 이해도가 부족한만큼, 통제하지 못한 변인들도 많다고 생각됩니다. 제 스스로도 확실하지 않아 테스트를 신뢰할 수 없다는 점이 아쉽습니다. 😭 그럼에도 불구하고 한 가지 경향성을 확인할 수 있었습니다. 로그가 유실되지 않도록 설정하더라도, 트랜잭션에서 로그 발생이 잦다면 비동기 로깅을 채택하는 것이 TPS 및 에러율 등 성능 측면에서 동기적인 로깅 방식보다 유리하다는 점이었습니다. 비동기 로깅의 경우, 로그 파일 쓰기 I/O 작업이 즉시 진행되지 않아 트랜잭션 수행 시간이 짧아지면서 스레드간 DB 커넥션 회전이 원할해졌습니다. 따라서 기존의 동기적인 로깅 방식에서 발생하던 Hikari CP Dead Lock 에러가 많이 줄어든 것을 확인할 수 있었습니다. 비동기 로깅 방식을 채택함으로써 로그 유실로 인한 신뢰성 문제 등 관리 포인트가 늘어나지만, 성능이 중요한 어플리케이션이라면 충분히 고려해볼만한 기능이라고 생각됩니다. Pick-Git 팀은 Log4j2가 Logback보다 비동기 성능이 우수하며 다양한 Appender(Kafka, DB, HTTP)를 제공하는 만큼 확장성이 좋다고 판단해, Log4j2를 사용하기로 최종 결정했습니다. Reference Log4j2 공식 문서 Logback 공식 문서 Logback AsyncAppender Twitter Facebook Google+ # 백엔드 # Logging",BE
573,"Git Branch 전략 2021, Sep 30 안녕하세요, 다니입니다. 🌻 기본 Git Flow main, hotfix, release, develop, feature - 총 5개의 브랜치를 사용합니다. 처음에 main을 기준으로 develop을 생성합니다. 각 기능마다 develop을 기준으로 feature를 생성해서 개발을 진행합니다. 개발을 완료하면 develop에 merge합니다. 배포를 준비할 때는 release를 활용하여 develop -> release로 merge합니다. 만약 release에 bug가 있을 경우 hotfix를 따로 이용하지 않고, release에서 바로 해결합니다. 배포 준비를 완료하면 release -> main으로 merge하고, release -> develop으로 develop에도 동일하게 반영합니다. 만약 main에 bug가 있을 경우 hotfix를 사용하여 해결합니다. 그리고 hotfix -> main, hotfix -> develop으로 해당 commit을 반영합니다. 배포를 버전별로 관리하기 위해 main에 tag를 붙입니다. 깃-들다 Git Flow main, hotfix, develop, feature - 총 4개의 브랜치를 사용합니다. 브랜치 전략을 최대한 간단하게 갖고가자는 생각으로 release는 활용하지 않기로 결정했습니다. 처음에 main을 기준으로 develop을 생성합니다. 각 기능마다 develop을 기준으로 feature를 생성해서 개발을 진행합니다. 개발하던 중 develop에 새로운 commit이 추가되면, feature의 base 해시값이 변경된 것이므로 develop의 가장 최신 commit으로 rebase합니다. 개발을 완료하면 feature -> develop으로 PR을 생성합니다. 이때, 젠킨스를 통해 해당 PR에 대한 Build + Test 및 SonarQube 검증을 진행합니다. SonarQube 검증을 통과하고, 팀원들 중 최소 1명 이상(FE 팀원 수를 고려하여 결정했습니다)이 PR Approve를 하면 merge를 위한 최소 요건이 충족됩니다. 최소 요건을 충족하면 develop에 squash-merge합니다. 이는 여러 개의 commit을 하나의 commit으로 압축하여 커밋 그래프를 단순하게 가져가기 위한 전략입니다. 배포를 할 때는 develop -> main으로 PR을 생성합니다. 이때는 SonarQube 검증은 따로 하지 않지만, 팀원 모두가 PR Approve를 해야 merge할 수 있습니다. 요건을 충족하면 main에 squash-merge합니다. 그리고 develop에 해당 commit을 반영하기 위해 pull(fetch + merge)합니다. 만약 main에 버그가 있을 경우 hotfix를 사용하여 해결합니다. 그리고 hotfix -> main, hotfix -> develop으로 해당 commit을 반영합니다. 배포를 버전별로 관리하기 위해 main에 tag를 붙입니다. References 우린 Git-flow를 사용하고 있어요 Creating a New Branch in GitHub Made Effortless (썸네일) Twitter Facebook Google+ # 백엔드 # Git # Branch # Strategy",BE
574,"테스트코드 최적화 여행기 (1) 2021, Sep 30 우테코 3단계를 진행하면서 프로젝트로 깃허브 기반의 SNS를 제작하고 있다. 팀의 컨벤션으로 테스트코드를 정말 빡시게 짜고있는데, 덕분에 리팩토링, 트러블 슈팅 등 많은 부분에서 큰 도움을 얻고 있다. 하지만 프로젝트의 기능이 하나식 추가되고 커지면서 문제점이 발생하는데, 바로 큰 도움을 준 테스트 코드가 그 주인공이다. 테스트 코드를 작성하다보니 어느 순간 500개가 넘는 TC가 쌓였다. 하지만 그에 따른 테스트 시간이 비약적으로 증가하였다. 체감상으로는 테스트가 쌓일수록 지수그래프를 따라 상승하는 것 같다. 또한 그 뿐만 아니라, 짧은 시간안에 많은 기능들을 구현하고자 하니 팀원들간의 테스트 컨벤션이 맞질 않아 테스트 픽스쳐가 여기저기 흩뿌려져 있고, 인수테스트의 경우 api를 호출하는 코드가 여기저기 중복되어 가독성 또한 심각하게 떨어지는 문제점이 있다. 이번 글에서는 이 수많은 테스트 코드를 리팩토링 하는 이야기를 담고자 한다. 현재 내가 생각하는 리팩토링 순위는 다음과 같다. 테스트 시간 단축 현재 글 작성 기준으로 전체 테스를 수행하는데 노트북(맥북 m1, 16gb)으로 약 1분이 소요되며테스크탑(인텔 10400, 16gb)의 경우는 약 3분 30초가 소요된다. 또한 ec2(medium)의 경우 는 약 4~5분이 소요된다. 따라서 리팩토링 및 프로덕션 작성 시 테스트를 돌리는데 약간은 겁이 난다. 이러한 겁은 리팩토링에 방해가 된다.. 또한 pr작성 시 젠킨스를 통해 빌드와 테스트 검사를 하고, 이를 통과 해야 merge가 가능한데, 이것만 약 5분이 소요되며 merge 후 배포시에도 다시 한번 테스트가 진행되므로 새로운 pr을 dev서버에 배포하는데만 약 10분이 소요되는꼴이다. 이걸 과연 CI라고 할 수 있을지… 공통 로직 제거 현재 너무나도 많은 중복 코드가 존재한다. 예를 들면 특정 인수테스트가 있고, 관련된 테스트가 10개라면 10개 모두에 api를 call하는 RestAssured코드가 존재하는 경우도 있다. 이러한 코드 중복을 제거하여 장황한 테스트 코드를 줄이고 가독성을 높히는게 중요하다고 생각된다. 픽스쳐 정리 픽스쳐가 여기저기 흩뿌려져 있다. 예를들어, 어느 코드에서는 유저를 생성하는데 테스트 유저를 생성하는 UserFactory를 사용하는 반면, 어느 코드에서는 new를 이용하여 테스트용 유저를 직접 생성하기도 한다. 이를 정리하여 통일성을 높히는게 중요할 것 이다. 3가지 리팩토링 순위를 나열했으나 이번 글은 테스트 시간 단축에 초점을 두어 작성할 것이다. 이 글에서 진행하는 테스트 최적화 방법은 필자의 뇌피셜에 기반한것이다. 하지만 그러면 어떠한가. 결과만 유의미하다면… 테스트 최적화 테스트 중에서 가장 시간이 많이 걸리는 테스트를 고르자면 당연히 인수테스트다. 그리고 그 다음은 통합 테스트다. 사실 Mock을 사용하는 슬라이스 테스트나 유닛 테스트는 최적화 할만한 요소가 그리 없다고 생각된다. 따라서 최적화 기법 또한 인수테스트와 통합테스트를 기준으로 작성하도록 하겠다. [테스트 최적화 실험에 사용한 코드] https://github.com/bperhaps/TestOptimizationStudyTest 모든 방법에 대한 테스트는 위 레포지토리에서 진행하였으며 1편에서는 간략한 방법론에 대하여 논하고 2편에서는 자세한 설정법을 알아보도록 한다. DirtiesContext 제거 현재 깃들다의 인수테스트는 DirtiesContext를 기본적으로 사용하고 있다. 심지어는 몇몇 통합테스트에서도 DirtiesContext를 사용한다. 그렇다. 사실 이게 제일 큰 문제다. DirtiesContext는 매 테스트케이스 마다 스프링 컨텍스트 전체를 내렸다가 다시 올린다.이는 결고 가벼운작업이 아니며 어쩌면 스프링 전체 프로세스중에서 가장 무거운 작업일 수도 있다. 따라서 DirtiesContext만 제거하더라도 테스트 속도는 비약적으로 증가할 것이다. 아마 DirtiesContext를 사용한 가장 큰 이유는, 데이터베이스의 초기화 때문일 것이다. 보통 DirtiesContext를 사용하는 이유는 빈의 상태를 변경시키는 테스트를 진행했을 때 이를 초기화 하기 위함이다. 하지만 깃들다의 경우 모든 빈들을 불변객체로 유지하기 때문에 빈의 상태를 변경시킬 일은 없다. 따라서 DirtiesContext를 제거하여 테스트 케이스들이 스프링 컨텍스트를 공유하여 사용할 수 있도록 하겠다. 위에 서술한 테스트 레포지토리를 통해 간략한 테스트를 진행해 보았다. DirtiesContext만 제거했을 뿐인데 의미있는 시간 시간 단축이 이루어졌다. 컨텍스트를 새로 로드하는 오버헤드는 상당히 크며 적절한 이유 없이 DirtiesContext를 사용하는 행위는 스프링 테스트의 Context Cache 이점을 전혀 이용하지 못한다. 따라서 DirtiesContext는 꼭 필요할때가 아니면 사용을 지양하는것이 좋다. 테스트 환경을 통일시킨다. 스프링 테스트가 제공하는 Application Context Cache기능을 제대로 활용하기 위해서는 테스트의 Configuration이 다르면 스프링은 새로운 Context를 올린다는 사실을 알아야한다. 또한 이를 통해 테스트의 속도를 더욱 끌어올릴 수 있다. 조회관련 테스트는, 미리 정의된 데이터를 통해 테스트한다. 필자가 생각하기에 또 시간적 비용이 많이 드는 부분을 고르자면, 테스트 함에 있어서 조회 테스트를 진행할 때 조회에 필요한 데이터를 set하는 부분이다. 조회 테스트를 진행하는 테스트 코드또한 만만치 않게 많기 때문에 이 데이터들을 데이터베이스에 올려놓고 조회 테스트를 시행하면 테스트를 더 빠르게 실행시킬 수 있을것이라고 생각한다. 즉, query를 위한 db와 update, create, delete를 위한 db를 분리한 뒤 테스트를 진행하겠다는 의미이다. 이것이 가능한 이유는 조회는 안전한 메소드(GET)을 사용함에 기반한다. 위 방법론을 간략하게 도식화 하면 아래와 같다. 매 테스트마다 조회를 위한 데이터를 추가했던 기존의 조회테스트와는 다르게, 조회 테스트에 필요한 Fixture를 초시화시에 미리 셋팅해두고 조회 관련 테스트를 진행한다. CUD 테스트는 기존과 동일하게 진행한다. 먼저 이를 적용하기 전의 테스트 시간은 위와 같이 10초가량이 소요된다. 적용 후 DB를 두개로 분리하여 테스트한 결과이다. 시간 거의 40%나 절감되었으며 조회에서는 역시 엄청난 성능 향상을 보였다. 물론 극단적인 케이스를 보였기때문에 이런 엄청난 성능향상폭을 보인걸수도 있다. 하지만 실제 프로덕션에 존재하는 수많은 read관련 테스트의 개수를 생각한다면, 적용시 테스트 시간을 극적으로 줄일 수 있을 것이라고 생각한다. 이번 글에서는 테스트 최적화의 방법론과 테스트 결과에 대하여 간단하게 논해 보았다. 다음글에서는 실제 프로젝트에 방법론을 적용하고, 그 결과를 살펴본다. 또한 각 방법의 자세한 방법에 대하여 논한다. Twitter Facebook Google+ # 백엔드 # test # 최적화",BE
576,"DB 리플리케이션 적용시 Binary 로그 에러 해결방법 2021, Sep 28 INTRO 현재 진행중인 프로젝트에서 DB Replication을 적용했었다. Replication 알아보기 DB replication 적용 이후 Master DB를 업그레이드 해야하는 상황에서 replicas와의 연동에 문제가 생긴적이 있었다. 이때 Master와 replicas 간의 데이터 연동 방법을 이해하고 해결한 (매우 간단한) 방법을 기록한다. Master DB와 replicas 동기화 Master DB에 데이터를 쓰기 위해서는 replicas에서 master db 의 데이터와 연결되어 있어야 한다. 그러기 위해서 replication을 설정할 때 show master status 라는 명령어를 통해서 나온 File값과 Position 값을 replica db 설정시 적용해 주었다. MariaDB [pickgit]> show master status;
+--------------------+----------+--------------+------------------+
| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+--------------------+----------+--------------+------------------+
| mariadb-bin.000008 | 68143505 | | |
+--------------------+----------+--------------+------------------+
1 row in set (0.000 sec)
여기서 File은 master db의 binary 로그 파일이고 Position 값은 해당 파일의 현재 위치이다. 위 log 파일에는 어떤 내용이 담겨 있을까? The MariaDB binary log is a series of files that contain events. An event is a description of a modification to the contents of our database. 출처: Big Data and Business Intelligence 로그 파일에는 데이터베이스에서 일어난 event들에 대해서 적혀 있는데, event라고 하는 것은 데이터베이스의 컨텐츠에 대해 일어난 변경사항을 말한다. MariaDB [pickgit]> show master status;
+--------------------+----------+--------------+------------------+
| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+--------------------+----------+--------------+------------------+
| mariadb-bin.000008 | 68143656 | | |
+--------------------+----------+--------------+------------------+
1 row in set (0.000 sec)
실제로 테이블을 추가하는 쿼리를 날린 후 다시 확인해보니 Position값이 증가한 것을 확인할 수 있다. Replicas 설정시 위 값을 지정한다는 것은 replicas에 데이터를 업데이트하는 file과 해당 file에서의 위치를 지정하는 것이다. 번외로 만일 binary loggin이 비활성되어 있는 상태에서 master 데이터베이스가 실행중이었다면 show master status; 명령어에 나오는 값이 비어있을 것이다. 그 경우 replicas에 master의 로그파일과 position을 지정할 때 빈 스트링 (‘‘)과 4를 지정하면 된다. 문제상황 및 해결 본래 사용한 MariaDB 버전은 10.1이었다. 하지만 Flyway를 적용한 이후 MariaDB를 10.4로 업그레이드 하지 않으면 적용할 수 없다는 오류가 생겼다. MariaDB 버전을 업그레이드 할 수 있는 방법을 찾아보았지만 현재 사용 중인 DB 데이터를 백업하고 삭제 후 10.4 버전을 새로 설치하여 데이터를 복원하라는 내용밖에 나오지 않았다. 현재 Master 1개 slave 2개를 사용중이었기 때문에 DB 3개를 모두 삭제하고 재설치하는 것은 지나치게 많은 작업이라고 생각했다. (replication 설정, 유저 생성 및 권한 부여 등등 자잘한 설정이 많음) 따라서 Flyway가 직접 적용되는 Master DB만 수정하고 Slave DB는 기존의 것을 유지하기로 했다. Master DB를 새로 구성하는 와중에 다음과 같은 문제 상황이 발생했다. 문제 상황 Master DB의 설정을 마치고 Slave에 Master를 지정하여 연결을 완료함 Master DB의 replication 유저에게 외부에서 쓰기 권한을 부여하지 않은 것을 깨달음 Master DB의 replication 유저에게 권한을 부여함 Slave DB에 Master DB의 데이터가 반영이 되지 않음 Master에서 연결된 slave hosts를 확인해 보면 잘 연결되어 있는 것을 확인할 수 있다. MariaDB [pickgit]> show slave hosts;
+-----------+------+------+-----------+
| Server_id | Host | Port | Master_id |
+-----------+------+------+-----------+
| 3 | | 9000 | 1 |
| 2 | | 9000 | 1 |
+-----------+------+------+-----------+
2 rows in set (0.000 sec)
Slave 의 상태를 확인해보면 Slave_IO_State: Waiting for master to sent event 라고 나와있는 것을 확인할 수 있다. 위 상태의 더 아래에 Last_Error 와 Last_SQL_Error를 확인해보면 특정 쿼리에 에러가 발생했다는 로그가 출력되어 있다. 즉, replication 이라는 유저가 Master에서는 잘 적용이 되었지만 Slave DB에는 존재하지 않기 때문에 에러가 발생한 것이다. 해당 로그 이후에 추가 및 변경된 데이터에 대해서는 slave db에 더 이상 반영이 되지 않았다. 위 문제를 해결하기 위해서는 Slave DB가 Master의 로그 파일을 읽는 Position을 위 쿼리가 실행된 이후로 옮겨서 해당 쿼리를 건너뛰어야 한다. 따라서 show master status를 다시 실행하여 나온 최신 position을 slave DB 설정에 넣어주어 문제를 해결했다. 여기서 주의할 점은 만일 이전에 변경된 데이터가 있다면 해당 변경 로그도 모두 건너뛰게 되니 다시 적용해주어야 한다. [참고자료] https://dev.mysql.com/doc/refman/8.0/en/replication-setup-replicas.html#replication-howto-newservers https://dev.mysql.com/doc/refman/8.0/en/replication-howto-masterstatus.html 백엔드 코다입니다 🙌 Twitter Facebook Google+ # 백엔드 # Database",BE
577,"DIP - 변경에 유연하고 테스트하기 좋은 코드 설계 2021, Sep 28 1. 들어가며 안녕하세요, 케빈입니다. 클린 코드 관련 저서들이 공통적으로 강조하는 내용은 단연 변경에 유연하고 테스트하기 좋은 코드의 중요성입니다. 소프트웨어는 항상 변경이 발생하기 쉽습니다. 요구 사항이 변경될 때마다 코드를 대거 수정해야 한다면 유지보수 측면에서 좋지 않습니다. 요구 사항 변경에 유연하게 대처할 수 있는 구조의 코드는 소프트웨어의 지속적인 성장을 위한 초석입니다. a code smell is any characteristic in the source code of a program that possibly indicates a deeper problem. 어떻게 하면 작성한 코드 구조가 좋고 나쁜지를 판별할 수 있을까요? 저희 프로젝트 팀은 테스트하기 쉬운 코드 구조를 지표로 삼았습니다. 만약 작성한 코드가 테스트하기 어렵다면 코드에서 냄새가 난다고 간주했습니다. 코드 냄새는 컴퓨터 프로그래밍 코드에서 더 심오한 문제를 일으킬 가능성이 있는 프로그램 소스 코드의 특징을 가리킵니다. 테스트하기 어려운 코드는 설계가 잘못되었음을 시사하며, 설계를 개선할 경우 문제가 해결될 공산이 큽니다. 이번 글에서는 프로젝트 Pick-Git 개발 중 DIP를 통해 변경에 유연하고 테스트하기 좋은 코드를 설계한 사례를 공유하고자 합니다. 2. Backgrounds 프로젝트 Pick-Git은 Github Repo 기반 개발 장려 SNS이며, 어플리케이션 특성상 GitHub Open API를 사용해 클라이언트의 GitHub 통계 정보를 시각화합니다. 프로젝트 아키텍쳐는 Presentation - Application - Domain - Infrastructure 순의 Layered Architecture를 기반으로 구성되었습니다. 상위 계층은 인접한 하위 계층만을 의존하는 것이 특징입니다. 3. Problems UserService.java @RequiredArgsConstructor
@Transactional(readOnly = true)
@Service
public class UserService {

private final UserRepository userRepository;
private final GitHubContributionCalculator gitHubContributionCalculator;

public ContributionResponseDto calculateContributions(ContributionRequestDto requestDto) {
User user = userRepository.findByName(requestDto.getUsername())
.orElseThrow(InvalidUserException::new);

Contribution contribution = gitHubContributionCalculator
.calculate(requestDto.getAccessToken(), user.getName());

return ContributionResponseDto.builder()
.starsCount(contribution.getStarsCount())
.commitsCount(contribution.getCommitsCount())
.prsCount(contribution.getPrsCount())
.issuesCount(contribution.getIssuesCount())
.reposCount(contribution.getReposCount())
.build();
}
}
GitHubContributionCalculator.java @RequiredArgsConstructor
@Component
public class GithubContributionCalculator {

@Value(""${github.contribution.url}"")
private final String url;

public Contribution calculate(String accessToken, String username) {
HttpHeaders httpHeaders = new HttpHeaders();
httpHeaders.setBearerAuth(accessToken);
RequestEntity<Void> requestEntity = RequestEntity
.get(url)
.headers(httpHeaders)
.build();
RestTemplate restTemplate = new RestTemplate();
return restTemplate.exchange(requestEntity, Contribution.class)
.getBody();
}
}
GitHub Open API를 바탕으로 클라이언트의 연동된 GitHub 계정의 통계 정보를 조회하는 예제 코드입니다. 위 서비스 로직은 Application 계층이 Domain 계층만 의존하는 것이 아니라, Infrastructure 구현 기술까지 직접 의존하는 문제가 있습니다. 3.1. 변경의 가능성 서두에서 언급했듯이, 소프트웨어는 항상 변경이 발생하기 쉽습니다. 만약 요구사항 변경으로 인해 현재 프로젝트가 더이상 GitHub 플랫폼 연동을 지원하지 않고 GitLab 등 다른 플랫폼을 지원하게 된다면 어떻게 될까요? UserService.java Contribution contribution = gitHubContributionCalculator
.calculate(requestDto.getAccessToken(), user.getName());
GithubContributionCalculator 뿐만 아니라 GitHub API 관련 Infrastructure 구현체를 사용하는 코드들이 전부 수정되어야 합니다. 만약 프로젝트 전역에 걸쳐 GitHub API 관련 Infrastructure 구현체를 사용하는 곳이 1000곳이라면, 1000번의 코드 수정이 발생하는 것입니다. 이는 코드가 변경에 유연하지 못하며 유지보수하기 힘들다는 점을 시사합니다. 3.2. 테스트하기 어려운 구조 UserServiceIntegrationTest.java @Autowired
private UserRepository userRepository;

@Autowired
private UserService userService;

@MockBean
private GitHubContributionCalculator gitHubContributionCalculator;

@DisplayName(""사용자는 활동 통계를 조회할 수 있다."")
@Test
void calculateContributions_LoginUser_Success() {
// given
userRepository.save(UserFactory.user());
ContributionRequestDto requestDto = ContributionRequestDto.builder()
.accessToken(""access-token"")
.username(""test-user-61"")
.build();
ContributionResponseDto contributions = getExpectedContributionResponseDto();
given(gitHubContributionCalculator.calculate(""access-token"", ""test-user-61"")).willReturn(contributions);

// when
ContributionResponseDto responseDto = userService.calculateContributions(requestDto);

// then
assertThat(responseDto)
.usingRecursiveComparison()
.isEqualTo(contributions);
}
GitHubContributionCalculator는 RestTemplate을 통해 GitHub로 API 요청을 보냅니다. 이 때, GitHub OAuth 로그인을 통해 얻은 실제 Access Token을 Header에 담습니다. 이에 대한 테스트 코드는 어떻게 작성할까요? 외부 API와의 통신은 직접 테스트하기 현실적으로 번거롭고 어려움이 많습니다. 외부 브라우저를 통해 로그인하고 실제 GitHub Access Token을 가져와 테스트 코드에 입력해야하기 때문입니다. 따라서 주로 Mocking하는 방식을 주로 사용합니다. 그러나 슬라이스 테스트가 아닌 통합 테스트에 매번 Mockito 혹은 RestClientTest 작업을 해주는 것 또한 번거롭습니다. 4. DIP(Dependency Inversion Principle) 우리가 다루는 모듈은 고수준 모듈과 저수준 모듈로 나눌 수 있습니다. 고수준 모듈이란 의미있는 단일 기능을 제공하는 모듈이며, 저수준 모듈은 고수준 모듈의 기능을 구현하기 위해 필요한 하위 기능의 실제 구현인 모듈입니다. Layered Architecture 상에서 Application 및 Domain 등의 고수준 모듈은 Infrastructure라는 저수준 모듈을 의존합니다. 그 결과, 앞서 언급한 것처럼 구현 부분의 변경에 유연하지 못하고 테스트하기 어렵다는 문제점이 발생합니다. DIP(Dependency Inversion Principle)이란 의존 관계를 역전시켜서 저수준 모듈이 고수준 모듈에 의존하도록 구현하는 것을 의미합니다. 고수준 모듈이 저수준 모듈을 직접 의존하는 것이 아니라, 저수준 모듈이 인터페이스 등 추상을 매개체로 고수준 모듈을 참조하도록 합니다. 이를 통해 구현 기술과 관련된 종속성을 쉽게 제거할 수 있습니다. PlatformContributionCalculator.java public interface PlatformContributionCalculator {

Contribution calculate(String accessToken, String username);
}
GitHubContributionCalculator.java @RequiredArgsConstructor
@Component
public class GithubContributionCalculator implements PlatformContributionCalculator {

// ...
}
UserService.java @RequiredArgsConstructor
@Transactional
@Service
public class UserService {

private final UserRepository userRepository;
private final PlatformContributionCalculator platformContributionCalculator;

public ContributionResponseDto calculateContributions(ContributionRequestDto requestDto) {
User user = userRepository.findByName(requestDto.getUsername())
.orElseThrow(InvalidUserException::new);

Contribution contribution = platformContributionCalculator
.calculate(requestDto.getAccessToken(), user.getName());

return ContributionResponseDto.builder()
.starsCount(contribution.getStarsCount())
.commitsCount(contribution.getCommitsCount())
.prsCount(contribution.getPrsCount())
.issuesCount(contribution.getIssuesCount())
.reposCount(contribution.getReposCount())
.build();
}
}
Application 계층이 Infrastructure 계층의 구현체가 아닌 Domain 계층의 PlatformContributionCalculator 인터페이스를 의존하도록 코드를 변경했습니다. 만약 요구사항 변경으로 인해 저수준 모듈이 GitHub 플랫폼 연동에서 GitLab 플랫폼 연동으로 변경되더라도, 고수준 모듈에서의 변경을 최소화할 수 있게 됩니다. UserServiceIntegrationTest.java @Autowired
private UserRepository userRepository;

private UserService userService;

@BeforeEach
void setUp() {
PlatformContributionCalculator platformContributionCalculator = (accessToken, username) -> {
// do something
};
userService = new UserService(userRepository, platformContributionCalculator);
}
또한 테스트를 진행할 때 PlatformContributionCalculator에 적합한 Mock Object를 주입할 수 있게 됩니다. 따라서 GitHub 서버가 아닌 Mock 서버로 API 요청을 보내게끔 테스트 대역을 조절함으로써, Access Token으로 인한 문제에서 벗어나 자동화된 테스트를 쉽게 작성할 수 있습니다. InfrastructureTestConfiguration.java @TestConfiguration
public class InfrastructureTestConfiguration {

@Bean
public PlatformContributionCalculator platformContributionCalculator() {
return new MockContributionCalculator();
}
}
테스트 클래스에 정의하는 것이 번거롭다면 대역을 테스트용 Bean으로 주입해 여러 통합 테스트에서 사용할 수 있습니다. 5. 마치며 어떤 경우에 인터페이스를 추출하고 DIP를 적용해야할지 등에 대해 아직 감이 잡히지 않는다면, 우아한형제들 기술 블로그의 안정된 의존관계 원칙과 안정된 추상화 원칙에 대하여 글을 참고하시면 큰 도움이 되실겁니다. 의존은 안정적인 쪽(Presentation -> Application -> Domain -> Infrastructure)으로 향해야 한다는 글입니다. 안정성이란 추상성을 내포한다고 말하기 때문에 따라서 의존 관계는 추상성의 방향으로 흘러야 합니다. 안정된 추상화 원칙은 안정적인 패키지는 그 안정성 때문에 확장이 불가능하지 않도록 추상적이기도 해야하며, 거꾸로 이 원칙에 따르면 불안정한 패키지는 구체적이어야 하는데, 그 불안정성이 그 패키지 안의 구체적인 코드가 쉽게 변경될 수 있도록 허용하기 때문입니다. Email 발송, Push, SMS 발송, Logging 등등 인프라성 코드는 거의 불안정성이 0에 가깝습니다(안정적). 호출자는 매우 많은데 그 자신이 의존하는 것은 별로 많지 않은 경우가 많습니다. 그러면서도 그 구현체는 시스템의 성장에 따라 바뀌기 쉽습니다(Email의 경우 SMTP → DB → MQ). 안정성이 높으면 변경 대응을 위해 추상적이어야 합니다. 따라서 인프라성 Service는 거의 무조건 interface를 구현해야 합니다. DIP가 항상 만능은 아닙니다. Runtime에 의존 대상이 결정되기 때문에 코드를 이해하거나 디버깅하기 어려워집니다. 처음부터 모든 의존 관계에 인터페이스를 추출하여 DIP를 적용하면 코드의 복잡도가 높아집니다. 특정 모듈의 변경 가능성 및 의존도 등을 다각도로 검토하여 인터페이스 추출과 DIP를 고려해야 합니다. References 외부 툴 변경에 휘둘리지 않는 서버 코드 작성기 도메인 주도 설계로 소프트웨어 만들기 안정된 의존관계 원칙과 안정된 추상화 원칙에 대하여 DDD 아키텍처 Twitter Facebook Google+ # 백엔드 # SOLID",BE
578,"JPA 프록시 관련 버그 경험기 2021, Sep 28 INTRO JPA 에서는 데이터베이스에서 연관객체 탐색을 효율적으로 하기 위해서 지연로딩 전략을 사용한다. 지연로딩의 핵심은 연관관계에 있는 Entity가 실제로 사용되기 이전까지 DB에 실제로 참조하지 않고 프록시 객체로 대체하는 것이다. JPA의 프록시 객체는 유용하지만 내부 동작방식에 대해서 제대로 알고있지 않으면 찾기 어려운 버그를 만날 수도 있다. 다음은 JPA proxy 관련해서 프로젝트 진행시 만난 버그에 대한 내용이다. 문제 상황 Entity 구조 참고: 설명과 관련된 부분만 남기고 다른 로직 및 어노테이션은 대부분 생략했다. Post - 게시물 엔티티 @Entity
public class Post {

@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""user_id"")
private User user;

@Embedded
private Images images;

@Embedded
private PostContent content;

@Embedded
private Likes likes; //설명과 관련된 프로퍼티!!

@Embedded
private Comments comments;

@Embedded
private PostTags postTags;

private String githubRepoUrl;

protected Post() {
}

//...부생성자 생략

//...불필요한 비지니스 로직 생략
//...getter 생략
}
Likes와 Like - Post 엔티티 하위의 Embedded 게시물 Like collection 포장객체 참고: 설명하고자 하는 부분과 깊게 연관된 핵심 Entity는 아니지만 상황 설명을 위해 간단히 프로퍼티만 소개한다. @Embeddable // Post 엔티티 안에 Embedded 되어 있음
public class Likes {

@OneToMany(
mappedBy = ""post"",
fetch = FetchType.LAZY,
cascade = CascadeType.PERSIST,
orphanRemoval = true
)
private List<Like> likes;

public Likes() {
this(new ArrayList<>());
}

public Likes(List<Like> likes) {
this.likes = likes;
}

//...getter 및 불필요한 비지니스 로직 생략
}

@Entity
public class Like {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""post_id"")
private Post post;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""user_id"")
private User user;

protected Like() {
}

//...getter 및 불필요한 비지니스 로직 생략
}
User - 어플리케이션 사용자 (게시물 좋아요, 유저간 팔로우 팔로잉 등의 행위를 함) @Entity
public class User {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@Embedded
private Followers followers;

@Embedded
private Followings followings;

//...일부 프로퍼티 생략

protected User() {
}

//...부 생성자 생략

public Boolean isFollowing(User targetUser) { //문제를 발생시킨 핵심 메소드!!!
if (this.equals(targetUser)) {
return null;
}

return this.followings.isFollowing(targetUser);
}

//...getter 및 불필요한 비지니스 로직 생략

@Override
public boolean equals(Object o) { //User는 Entity 이므로 Id로 동일성 및 동등성 확인
if (this == o) {
return true;
}

if (o == null || getClass() != o.getClass()) { //중요한 포인트!!!!
return false;
}

User user = (User) o;

return id != null ? id.equals(user.getId()) : user.getId() == null;
}

@Override
public int hashCode() {
return Objects.hash(getId());
}
}
Followings와 Follow Followings - 해당 User의 팔로워리스트를 저장하는 포장객체 (Followers도 동일한 형태로 되어 있다.) Follow - Followers, Followings 리스트에 담겨 있는 VO 엔티티로 source, target 유저간의 팔로우 관계를 나타내는 엔티티 @Embeddable
public class Followings {

@OneToMany(
mappedBy = ""source"",
fetch = FetchType.LAZY,
cascade = CascadeType.PERSIST,
orphanRemoval = true
)
private List<Follow> followings;

protected Followings() {
}

//...생성자 생략

public Boolean isFollowing(User targetUser) { //문제를 발생시킨 핵심 메소드!!!
return followings.stream()
.anyMatch(follow -> follow.isFollowing(targetUser));
}

//...일부 비지니스 로직 생략
}

@Entity
@Table(
uniqueConstraints = {
@UniqueConstraint(columnNames = {""source_id"", ""target_id""})
}
)
public class Follow {

@Id @GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""source_id"")
private User source;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""target_id"")
private User target;

protected Follow() {
}

//...생성자 및 유효성 검사 로직 생략

public boolean isFollowing(User targetUser) { //문제를 발생시킨 핵심 메소드!!!
return this.target.equals(targetUser);
}

//...getter 생략
//equals & hashcode는 VO로 취급되어 필드가 같은지 확인 (즉, 유저가 같은 유저인지 확인)
}
PostRepository 게시물 좋아요 리스트 조회 쿼리 @Query(""select distinct p from Post p left join fetch p.likes where p.id = :postId"")
Optional<Post> findPostWithLikeUsers(@Param(""postId"") Long postId); // 포스트를 조회할 때 좋아요 리스트를 fetch join 해서 즉시로딩 한다.
버그 발생 현재 흐름은 다음과 같다. Post를 좋아요 한 유저 리스트를 반환하려함. (Post내부의 Likes를 반환) 좋아요 한 유저 리스트를 조회할 때, 조회하는 source 유저가 팔로잉 하고 있는 target 유저는 팔로잉 중이라고 나타냄. source 유저가 target user를 following 하고 있는 여부를 User의 isFollowing 메소드를 통해서 확인함. 이때 source와 target이 같은 경우(자기 자신인 경우) - null 반환 source가 target을 팔로잉 중인 경우 - true 반환 source가 target을 필로우 하지 않는 경우 - false 반환 문제는, 로그인 한 source 유저와 즉시로딩 해 가져온 좋아요 리스트의 User 간의 팔로잉 여부가 모두 false로 출력이 된 것이다. 발생 원인 PostRepository에서 findPostWithLikeUsers()를 사용해 포스트와 좋아요 리스트를 즉시로딩(@OneToMany 관계) 할 때 Like 엔티티 내부의 User는 즉시로딩 하지 않으므로 proxy 객체이다. 좋아요한 target 유저 리스트를 가져와서 로그인 유저인 source 유저의 isFollowing() 메소드로 두 유저간의 팔로잉 여부를 확인한다. // User.java
public Boolean isFollowing(User targetUser) {
if (this.equals(targetUser)) { //자기 자신일 경우 null 반환
return null;
}

return this.followings.isFollowing(targetUser);
}

//Followings.java
public Boolean isFollowing(User targetUser) {
return followings.stream()
.anyMatch(follow -> follow.isFollowing(targetUser));
}

//Follow.java
public boolean isFollowing(User targetUser) {
return this.target.equals(targetUser); //User.java 의 equals & hashCode를 사용
}
아무리 디버깅을 해봐도 비교하는 source 유저와 target 유저의 식별자(Id)가 같음에도 불구하고 Follow.java의 isFollowing()에서 false가 반환 되었다. 그 원인은 User.java에서 오버라이드한 equals hashcode에 있었다. @Override
public boolean equals(Object o) {
if (this == o) {
return true;
}

if (o == null || getClass() != o.getClass()) { //(1) 중요한 포인트!!!!
return false;
}

User user = (User) o;

return id != null ? id.equals(user.getId()) : user.getId() == null;
}

@Override
public int hashCode() {
return Objects.hash(getId());
}
위 (1) 에서 User객체의 Id로 비교하기 이전에 두 객체가 같은 클래스인지 o.getClass()로 비교하고 있었다. 하지만 proxy 객체는 getClass() 로 비교하면 실제 entity와 같지 않기 때문에 false를 반환한다. 따라서 프록시 객체와 실제 entity를 비교할때는 instance of를 사용해야한다. JPA Proxy 참고링크 해결 방법 생각한 해결방법은 2가지 이다. Post와 Like의 fetch join 시 Like의 User 까지 모두 fetch join으로 즉시로딩 Post와 Like -> @OneToMany 관계 Like와 User -> @ManyToOne 관계 위와 같은 연관관계는 두 번 fetch join 하여 Like의 User까지 즉시로딩 할 수 있다. @Query(""select distinct p from Post p left join fetch p.likes.likes l left join fetch l.user where p.id = :postId"")
Optional<Post> findPostWithLikeUsers(@Param(""postId"") Long postId);
위와 같이 User까지 즉시로딩 한다면, User가 더 이상 proxy 객체가 아니기 때문에 getClass()를 해도 문제가 발생하지 않는다. 하지만 지나치게 복잡한 연관관계를 즉시로딩 하는 것이며 JPQL에서 fetch join 시 별칭을 쓰는 것은 JPA 표준 스펙에 맞지 않기 때문에 추천하는 방법이 아니다. (Hibernate 구현상 가능하므로 할 수 있긴 하다.) JPA fetch join 시 별칭 참고링크 User의 equals() 메소드의 getClass() 비교 부분을 instance of 로 수정 User.java의 equals 메소드를 다음과 같이 수정하면 올바른 값을 반환한다. @Override
public boolean equals(Object o) {
if (this == o) {
return true;
}
if (!(o instanceof User)) { // 이 부분!!
return false;
}

User user = (User) o;

return id != null ? id.equals(user.getId()) : user.getId() == null;
}
마무리 JPA Proxy 객체에 대해서 학습했으나, 이론으로 알고 있던 부분을 직접 버그로 경험하며 학습할 수 있었다. 디버깅 시 User 객체의 주소값이 ID가 같을 경우 같은 해시값으로 찍혔기 때문에 원인을 알기 더 어려웠다. 또한 디버깅 포인트를 override 하여 IDE에서 자동으로 추가한 equals()에 걸 생각을 하지 못한 것도 디버깅을 어렵게 했던 포인트였다. 개인적으로 equals()를 수정하는 두번째 해결방법을 추천하지만, override 한 메소드를 수정하는 것이 다른 팀원에게 충분히 공유되지 않으면 또다른 버그 포인트가 될 수 있다고 생각한다. (당연하게 생각하여 자세히 들여다보지 않는 부분이므로) 백엔드 코다입니다 🙌 Twitter Facebook Google+ # 백엔드 # JPA",BE
579,"JPA Pagination, 그리고 N + 1 문제 2021, Sep 28 1. 들어가며 안녕하세요, 케빈입니다. 게시판 기능을 제공하는 웹 어플리케이션에 접속하여 게시물 목록을 요청하는 경우를 상상해봅시다. DB에 저장되어 있는 게시물은 수백 만개에 육박할 수도 있습니다. 모든 게시물 목록을 조회해 화면에 렌더링하는 경우, 클라이언트가 브라우저 혹은 모바일 기기로 이를 한 눈에 보기 어려움을 겪을 공산이 큽니다. 또한 클라이언트가 보지도 않을 데이터까지 DB에서 조회하여 네트워크를 통해 전달하기 때문에, 서버의 리소스가 불필요하게 낭비됩니다. 반면 위 사진처럼 한 페이지에서는 N개의 데이터만 보여주고, 다음 페이지로 이동하라는 클라이언트의 추가 요청이 있을 때 마다 다음 순번의 N개의 데이터를 보여준다면 UX 및 리소스 측면의 단점을 보완할 수 있습니다. 이 처럼 한 화면에 보여주는 데이터의 범위를 결정하는 일련의 방식을 페이지네이션 혹은 페이징이라고 합니다. Pick-Git은 위 사진과 같이 게시판 형태가 아닌 인피니티 스크롤을 기반으로 페이지네이션을 적용하고 있습니다. 그리고 이번 글에서는 Pick-Git을 개발하면서 당면했던 JPA Pagination 및 N + 1 문제 및 해결 방법을 공유하고자 합니다. 2. Pagination 일반적으로 DB 벤더에 따라 페이징을 처리하는 쿼리가 천차만별입니다. MySQL의 경우 LIMIT 및 OFFSET 구문 등을 사용함으로써 페이징을 처리할 수 있지만, Oracle은 그보다 더 복잡한 쿼리가 수반됩니다. 그러나 JPA를 사용한다면 별도의 쿼리 작성 없이 Pagination API를 사용하기만 하면 됩니다. JPA가 설정된 DB 벤더 방언에 맞게 페이징 쿼리를 자동으로 생성하기 때문입니다. PostRepository.java @Query(""select p from Post p"")
List<Post> findWithPagination(Pageable pageable);
PostRepositoryTest.java @DisplayName(""Pageable을 사용하여 페이징 처리한다."")
@Test
void pagination() {
postRepository.findWithPagination(Pageable.ofSize(10));
// postRepository.findWithPagination(PageRequest.of(0, 2)); 가능
}
SQL Hibernate:
select
post0_.id as id1_4_,
post0_.content as content2_4_
from
post post0_ limit ?
Spring Data JPA의 경우 Pageable 구현체를 Repository 쿼리 메서드의 파라미터로 전달함으로써 쿼리에 페이징을 동적으로 추가할 수 있습니다. 개발자가 Pageable 인터페이스를 직접 구현하거나, 미리 준비되어 있는 정적 팩토리 메서드를 통해 간편하게 페이지네이션 범위를 지정할 수 있습니다. 현재 설정된 방언에 따라 JPQL 쿼리 및 페이지네이션 기능이 SQL 쿼리로 변환되는 것을 확인할 수 있습니다. 3. N + 1 및 Fetch Join Post.java @Entity
public class Post {

@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;

@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = ""user_id"")
private User user;

@OneToMany(mappedBy = ""post"", fetch = FetchType.LAZY)
private List<Image> images = new ArrayList<>();

@OneToMany(mappedBy = ""post"", fetch = FetchType.LAZY)
private List<Like> likes;

@OneToMany(mappedBy = ""post"", fetch = FetchType.LAZY)
private List<Comment> comments;

@OneToMany(mappedBy = ""post"", fetch = FetchType.LAZY)
private List<PostTag> postTags;

// ...
}
Pick-Git 프로젝트에서 사용하는 Post 엔티티를 간략하게 표현한 예제 코드입니다. PostService.java @Transactional(readOnly = true)
public List<PostResponseDto> homeFeed(HomeFeedRequestDto homeFeedRequestDto) {
Pageable pageable = getPagination(homeFeedRequestDto);
User requestUser = findUserByName(homeFeedRequestDto.getRequestUserName());
List<Post> result = postRepository.findAllAssociatedPostsByUser(requestUser, pageable);
return PostDtoAssembler.assembleFrom(requestUser, result);
}
복수건의 게시물 조회 요청의 경우, Pagination API를 바탕으로 게시물을 조회합니다. PostResponseDto.java public class PostResponseDto {

private Long id;
private List<String> imageUrls;
private String authorName;
private Integer likesCount;
private List<String> tags;
private List<CommentResponseDto> comments;
// ...
}
이후 Post 엔티티 내부 필드(연관 엔티티)들을 getter로 호출해 가공하여 응답 DTO를 작성합니다. 기능을 구현하고 보니 JPA가 Post 페이징 조회 쿼리 1건 이외에도 추가적으로 5건(Comment, Like, Image, User, Tag)의 조회 쿼리를 날리고 있었습니다. 즉, 한 개의 트랜잭션을 처리하는데 총 6번의 SQL 조회 쿼리가 발생하는 심각한 문제였습니다. 이러한 현상을 N + 1 이라고 합니다. 쿼리 1번으로 N건의 엔티티를 가져왔는데, 글로벌 지연 로딩 전략으로 인해 관련 컬럼을 얻기 위해 쿼리를 N번 추가로 수행하는 현상을 의미합니다. 테스트 코드를 통해 더 자세히 살펴보겠습니다. PostRepositoryTest.java @DisplayName(""Pageable을 사용하여 페이징 처리한다."")
@Test
void pagination() {
List<Post> posts = postRepository.findWithPagination(PageRequest.of(0, 3));

for (Post post : posts) {
System.out.println(post.getComments());
}
}
SQL Hibernate:
select
post0_.id as id1_4_,
post0_.content as content2_4_
from
post post0_ limit ?
Hibernate:
select
comments0_.post_id as post_id4_2_0_,
comments0_.id as id1_2_0_,
comments0_.id as id1_2_1_,
comments0_.content as content2_2_1_,
comments0_.like_id as like_id3_2_1_,
comments0_.post_id as post_id4_2_1_
from
comment comments0_
where
comments0_.post_id=?
[com.learning.jpa.domain.Comment@3a1238cc]
Hibernate:
select
comments0_.post_id as post_id4_2_0_,
comments0_.id as id1_2_0_,
comments0_.id as id1_2_1_,
comments0_.content as content2_2_1_,
comments0_.like_id as like_id3_2_1_,
comments0_.post_id as post_id4_2_1_
from
comment comments0_
where
comments0_.post_id=?
[com.learning.jpa.domain.Comment@ce5df3f]
Hibernate:
select
comments0_.post_id as post_id4_2_0_,
comments0_.id as id1_2_0_,
comments0_.id as id1_2_1_,
comments0_.content as content2_2_1_,
comments0_.like_id as like_id3_2_1_,
comments0_.post_id as post_id4_2_1_
from
comment comments0_
where
comments0_.post_id=?
[com.learning.jpa.domain.Comment@5ab690ec]
페이징으로 조회한 N개의 Post 엔티티를 순회하며 Comment를 조회해봅시다. Post는 1:N 관계를 맺고 있는 Comment에 대해 글로벌 지연 로딩 전략을 채택하고 있습니다. 그 결과, 페이징(LIMIT)을 통해 조회한 Post 리스트를 순회하면서 getComments()를 호출할 때마다 추가적으로 Comment 조회 쿼리가 발생합니다. 즉, 조회한 Post가 1000개라면 1000개의 추가 쿼리가 발생하는 전형적인 N + 1 문제가 나타납니다. 실제 Pick-Git에서 사용하는 Post 엔티티의 경우, 응답을 작성할 때 Comment 뿐만 아니라 Image, Tag, Like, User 등 여러 지연 로딩된 엔티티 테이블들을 추가적으로 참조합니다. 따라서 조회한 Post가 1000건이면 5000건의 추가 쿼리가 발생하게 됩니다. 이를 해결하기 위해 대게 Fetch Join을 적용하게 됩니다. 3.1. Fetch Join Fetch Join이란 JPQL로 특정 엔티티를 조회할 때 연관된 엔티티 혹은 컬렉션을 즉시 로딩과 같이 한 번에 함께 조회하는 기능입니다. PostRepository.java @Query(""select distinct p from Post p join fetch p.comments"")
List<Post> findWithPagination(Pageable pageable);
SQL 2021-07-26 22:34:32.764 WARN 14832 --- [ Test worker] o.h.h.internal.ast.QueryTranslatorImpl : HHH000104: firstResult/maxResults specified with collection fetch; applying in memory!
Hibernate:
select
distinct post0_.id as id1_4_0_,
comments1_.id as id1_2_1_,
post0_.content as content2_4_0_,
comments1_.content as content2_2_1_,
comments1_.like_id as like_id3_2_1_,
comments1_.post_id as post_id4_2_1_,
comments1_.post_id as post_id4_2_0__,
comments1_.id as id1_2_0__
from
post post0_
inner join
comment comments1_
on post0_.id=comments1_.post_id
[com.learning.jpa.domain.Comment@72dc246c]
[com.learning.jpa.domain.Comment@6e76be45]
[com.learning.jpa.domain.Comment@37ef8e6b]
Post 전체 조회 쿼리 1개 (1) + 각 Post별 Comment 조회 쿼리 3개 (N) 총 쿼리가 4개가 날아가던 것이 Fetch Join을 적용함으로써 1개로 줄어들었습니다. 성능이 최적화된 것으로 보이지만 다소 이상한 부분이 존재합니다. 먼저, 페이징할 때 사용하던 기존의 SQL LIMIT 구문이 등장하지 않았습니다. 또한 쿼리 결과를 전부 메모리에 적재한 뒤 어플리케이션 단에서 Pagination 작업을 수행한다는 경고 로그가 발생합니다. 왜 이러한 현상이 발생하는 것일까요? Post 엔티티가 3개 있고, 각각의 Post 엔티티는 연관된 Comment가 7개 존재한다고 가정해봅시다. 1:N 관계를 Join하면 총 21(3 * 7)개의 DB Row가 조회됩니다. 데이터의 수가 변경되기 때문에 단순하게 LIMIT 구문을 사용하는 쿼리로 페이지네이션을 적용하기 어렵습니다. 따라서 조회한 결과를 모두 메모리로 가져와서 JPA가 페이지네이션 계산을 진행합니다. 1:N 관계의 컬렉션을 Fetch Join하면서 동시에 Pagination API를 사용하면 OutOfMemoryError가 발생할 수 있기 때문에, 이 둘을 동시에 사용해서는 안 됩니다. CommentTest.java List<Comment> comments = entityManager
.createQuery(""select c from Comment c join fetch c.post"", Comment.class)
.setFirstResult(0)
.setMaxResults(10)
.getResultList();
SQL Hibernate:
select
comment0_.id as id1_2_0_,
post1_.id as id1_4_1_,
comment0_.content as content2_2_0_,
comment0_.post_id as post_id4_2_0_,
post1_.content as content2_4_1_
from
comment comment0_
inner join
post post1_
on comment0_.post_id=post1_.id limit ?
반대로 N:1 관계의 엔티티를 Fetch Join할 때는 문제없이 Pagination API를 적용할 수 있습니다. Comment와 Post는 N:1 관계이기 때문에 Join해도 조회되는 DB ROW의 수가 변경되지 않기 때문입니다. SQL 로그에도 LIMIT 구문이 잘 찍혀있고, 별도의 경고 로그가 발생하지 않습니다. 4. 해결 방안 Pagination API를 사용할 때 ~ToOne 관계의 엔티티는 Fetch Join해도 괜찮지만, ~ToMany 관계의 엔티티에 대해서는 다른 접근이 필요합니다. application.properties spring.jpa.properties.hibernate.default_batch_fetch_size=1000
그 중 가장 쉬운 해결 방안으로는 Batch Size를 지정하는 것입니다. @BatchSize 애너테이션을 부착할 수도 있지만, 어플리케이션 전역 설정을 위해 properties에 값을 정의했습니다. 해당 옵션에 대한 설명은 후술하겠습니다. PostRepositoryTest.java List<Post> posts = entityManager.createQuery(""select p from Post p"", Post.class)
.setFirstResult(0)
.setMaxResults(10)
.getResultList();
SQL Hibernate:
select
post0_.id as id1_4_,
post0_.content as content2_4_
from
post post0_ limit ?
Hibernate:
select
comments0_.post_id as post_id4_2_1_,
comments0_.id as id1_2_1_,
comments0_.id as id1_2_0_,
comments0_.content as content2_2_0_,
comments0_.like_id as like_id3_2_0_,
comments0_.post_id as post_id4_2_0_
from
comment comments0_
where
comments0_.post_id in (
?, ?, ?
)
[com.learning.jpa.domain.Comment@6090a8fc]
[com.learning.jpa.domain.Comment@77d45b37]
[com.learning.jpa.domain.Comment@23447af]
Fetch Join 없이 Pagination API를 사용해보았습니다. 페이징(LIMIT)을 통해 POST 리스트를 조회하지만, Comment 관련 조회 쿼리가 기존과 다르게 나갑니다. 기존에는 반복문을 순회하면서 N개의 Post 엔티티에 대해 where comments0_.post_id=?를 포함하는 Comment 조회 쿼리가 N번 발생했습니다. 그러나 Batch Size를 적용한 결과 where comments0_.post_id in (?, ?, ?)를 포함하는 Comment 조회 쿼리 1개로 줄어들었습니다. @BatchSize 혹은 spring.jpa.properties.hibernate.default_batch_fetch_size 옵션을 적용하면 X 타입 엔티티가 지연 로딩된 ~ToMany 관계의 Y 타입 컬렉션을 최초 조회할 때 이미 조회한 X 타입 엔티티(즉, 영속성 컨텍스트에서 관리되고 있는 엔티티)들의 ID들을 모아서 WHERE Y.X_ID IN (?, ?, ?...) 와 같은 SQL IN 구문에 담아 Y 타입 데이터 조회 쿼리를 날립니다. X 타입 엔티티들이 필요로 하는 모든 Y 타입 데이터를 한 번에 조회합니다. 여기서 Batch Size 옵션에 할당되는 숫자는 IN 구문에 넣을 부모 엔티티 Key(ID)의 최대 개수를 의미합니다. 예를 들어 봅시다. 별다른 조치를 취하지 않은 상황에서, Post 1000개가 담긴 리스트를 순회하면서 Comment를 호출하는 코드는 지연 로딩으로 인해 1000개의 추가 쿼리가 발생합니다. 반면 Batch Size 옵션을 1000으로 지정해두면, 반복문을 순회하며 Comments를 최초로 조회하는 시점에 영속성 컨텍스트에서 관리되고 있는 1000개의 Post 엔티티 ID가 Comments 조회 쿼리의 IN 구문 where comment.post_id in (?, ?, ?, ...)에 포함되어 날아갑니다. 단 하나의 Comment 조회 쿼리로 1000개의 Post 엔티티가 필요로 하는 모든 Comment 관련 데이터를 조회해옵니다. 보통 옵션값을 1,000 이상 주지 않습니다. 4.1. 결론 PostRepository.java @Query(""select p from Post p left join fetch p.user order by p.createdAt desc"")
List<Post> findAllPosts(Pageable pageable);
따라서 Pick-Git은 Post를 Pagination API를 통해 조회할 때 다음과 같은 방법을 사용합니다. @ManyToOne 등 N:1 관계의 자식 엔티티에 대해서는 모두 Fetch Join을 적용함으로써 한방 쿼리를 수행한다. @OneToMany 등 1:N 관계의 자식 엔티티는 Fetch Join을 하지 않는다. 대신 상기 선언한 hibernate.default_batch_fetch_size 적용을 통한 in 쿼리로 성능을 보장한다. 5. 마치며 Batch Size 옵션은 상술한 Pagination + Fetch Join 문제뿐만 아니라, 다른 JPA의 한계를 극복하는데 도움이 됩니다. 가령, JPA는 엔티티를 조회할 때 2개 이상의 1:N 관계의 컬렉션을 Fetch Join하지 못하도록 되어있습니다. 조회되는 데이터가 너무 많아지는 카테시안 곱(Cartesian Product) 때문에, 2개 이상의 컬렉션을 Fetch Join하면 MultipleBagFetchException이 발생합니다. 오직 1개의 컬렉션만 Fetch Join이 가능합니다. 특정 엔티티가 보유하는 1:N 관계의 컬렉션 모두를 즉시 로딩처럼 조회할 수 없다는 것은, 이번 글의 Post 엔티티 예제처럼 N + 1 문제가 발생한다는 것을 시사합니다. 하지만 Batch Size 옵션을 통해 이러한 JPA Fetch Join 한계로 인해 발생하는 N + 1 문제를 어느 정도 해소할 수 있습니다. Reference jpa fetch join MultipleBagFetchException 발생시 해결 방법 fetch join 시 paging 문제 fetch join 과 pagination 을 같이 쓸 때 [HHH000104: firstResult/maxResults specified with collection fetch; applying in memory] N+1 쿼리 문제 이미지 출처 티저 출처 Twitter Facebook Google+ # 백엔드 # JPA",BE
583,"Interceptor path 등록 자동화 구현기 feat. Component Scan 2021, Aug 16 예제 소스: https://github.com/bperhaps/ComponentScan 안녕하세요. 깃-들다 팀의 손너잘입니다. 이번 글에서는 저희 팀의 인증, 인가 로직 리팩토링과 관련된 이야기를 해보고자 합니다. 현재 저희 팀은 API에 접근하는 유저의 인증 및 인가 로직을 구현하기 위해 Interceptor를 약간 커스텀 하여 사용하고 있습니다. 커스텀을 진행한 이유는 간단한데요, 동일한 URL에 대해서 각각의 HttpMethod마다 서로 다른 인증로직을 부여하기 위해서 입니다. 예를 들면 https://api.pick-git.com/api/post 와 같은 URL에 대하여 POST 의 경우는 사용자의 인증을 진행하고, 적절한 인가 과정을 거쳐 로직을 실행해야 하지만 GET 의 경우는 로그인 하지 않은 사용자의 경우에도 게시물을 불러올 수 있도록 해야합니다. 일반적으로 인증 로직은 interceptor에 위치하게 되는데, 스프링에서 제공하는 기본 interceptor register의 경우 url path를 이용하여 interceptor에 등록, 제외할 수는 있지만 동일한 URL에 대한 METHOD 분기는 지원하고 있지 않습니다. 깃-들다 팀은 이를 해결하기 위해 데코레이터 패턴을 활용한 멋진 방법을 사용했습니다. 하지만 이를 이용하다 보니 몇가지 불편함이 생겼습니다. 위와 같이 매번 interceptor configuration을 일일이 적용시켜줘야 한다는 것이었습니다. 이를 깜빡하고 테스트코드를 작성하다가 테스트가 터지는 날에는 몇시간씩 디버거를 붙잡으면서 머릿속에 물음표를 띄우는 경우도 있었죠.. 그러다 문득 인터셉터를 등록하는 과정을 어노테이션 기반으로 리팩토링 하는게 어떨까? 하는 생각이 들었습니다. 그렇게 모든 Interceptor 등록 로직을 어노테이션 기반으로 마이그레이션에 성공했고, 이번 글에서는 이에 대한 지식을 공유하고자 합니다. 제가 했던 기본적인 구상은 다음과 같습니다. 생성해야 하는 어노테이션 @ForLoginAndGuestUser @ForGuest 위 어노테이션을 새로 생성하는 Controller Method에 적용하면, 자동으로 Custom Interceptor에 등록을 시켜주는 형태인 것이죠.. 즉, 최종적 목표는 아래와 같은 로직을 완성하는 것 입니다. @ForOnlyLoginUser
@PostMapping(""/posts/{postId}/comments"")
public ResponseEntity<CommentResponse> addComment()
...
}

@ForLoginAndGuestUser
@getMapping(""/posts/{postId}/comments"")
public ResponseEntity<CommentResponse> addComment()
...
}
기존 방식처럼 intercepter관련 로직을 신경쓰는 것 보다 훨씬 깔끔하고, 팀원들이 비즈니스 로직에 더 집중할 수 있겠죠? 기능 요구 사항 구현하고자 하는 기능의 요구사항은 다음과 같았습니다. 어노테이션이 붙은 Method로 부터 URI 파싱. 어노테이션이 붙은 Method로 부터 HttpMethod 파싱. 파싱한 데이터를 기반으로 특정한 기능 수행 (인증 로직 Interceptor에 파싱한 URI와 HttpMethod를 등록한다). 하지만, 이번 글에서는, 예제의 간소화를 위해 요구사항을 아래와 같이 변경하겠습니다. 어노테이션이 붙은 Method로 부터 URI 파싱. 어노테이션이 붙은 Method로 부터 HttpMethod 파싱. 파싱한 데이터를 기반으로 특정한 기능 수행 (특정 URI로 Get 요청 시 어노테이션이 붙은 Method들로 부터 파싱한 URI와 매칭되는 HttpMethod들을 반환한다). Annotation 가장 먼저 해야할 것은 Anntation을 생성 하는 것입니다. @Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface ForLoginUser {
}

---------------------------------------

@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface ForGuest {
}
간단하게 Method에만 적용 가능하고, RUNTIME에 동작하도록 작성했습니다 데이터 파싱 (URI and HttpMethod) 가장 먼저 어노테이션 기반으로 필요한 정보를 파싱해 보도록 하겠습니다. 파싱할 정보는 URI , HttpMethod 정도가 되겠네요. 현재 저희는 Spring boot Framwork를 사용하며 어노테이션 기반으로 코드를 작성하기 때문에 이를 기반으로 작성하겠습니다. Controller 클래스 추출 [https://github.com/bperhaps/ComponentScan/tree/extract_controller] 가장 먼저 해야하는 것은, 수 많은 클래스들 중에 Controller 클래스만을 파싱하는 것 입니다. 먼저, 모든 컨트롤러들이 공통으로 가지고 있는것은 무엇일까요? 바로 @Controller, @RestController 입니다. 따라서 이 어노테이션을 가지고 있는 Class들을 파싱함으로써 Controller 클래스를 파싱할 수 있습니다. 해당 클래스에 특정 어노테이션이 있는지 확인하는 방법은 간단합니다. 리플렉션에서 제공하는 isAnnotationPresent() 메서드를 이용하여 간단히 해당 타입에 특정 어노테이션이 있는지 확인할 수 있습니다. @Controller
public class ScanController1 {}

@RestController
public class ScanController1 {}

public class ScanController1 {}
테스트를 위해 위와같이 3개의 Controller 클래스를 생성하였습니다. 또한 아래와 같이 프로덕션 코드를 작성하여 Controller클래스를 분류해 보았습니다. 또한 아래와 같이 테스트 코드를 작성하여 제대로 동작하는지 확인하였습니다. 컨트롤러 어노테이션이 붙은 클래스인 ScanController1, ScanController2가 잘 추출됐는지 확인하는 테스트가 잘 돌아가는것을 확인할 수 있습니다. 메소드 추출 [https://github.com/bperhaps/ComponentScan/tree/extract_method] 컨트롤러를 추출했으니, 다음으로 할 것은 특정한 어노테이션이 붙은 메서드를 추출하는 것 입니다. 이번예제의 경우 @ForLoginUser , @ForLoginAndGuest 가 붙은 메서드를 추출하는게 목표가 되겠네요. 먼저 컨트롤러에 예시로 사용할 메서드를 생성하도록 하겠습니다. (자세한 소스는 github를 참조해 주세요) @Controller
public class ScanController1 {

@ForLoginAndGuest
@GetMapping(""/getTestGuest"")
public void getTest() {

}

@ForLoginUser
@PostMapping(""/postTstLogin"")
public void postTest() {

}

@PutMapping(""/putTestNop"")
public void putTest() {

}
}
메서드 추출 프로덕션은 다음과 같습니다 테스트 코드도 작성해 줍니다. 소스를 한번 읽어보면, 각 컨트롤러들의 typetoken으로부터 메서드 타입들을 불러옵니다(getMethods(), getDeclaredMethod를 사용하지 않은 이유는, 어쩌피 스프링의 Mapping 어노테이션이 public method에서만 동작하기 때문에 굳이 private method까지 불러오는 수고를 할 필요가 없기 때문입니다). 그리고 각 메서드에 ForLoginAndguest, ForLoginUser 어노테이션이 있는지 확인하고 파싱합니다. 제가 예제로 만들어 놓은 클래스에서는 파싱되는 메서드의 갯수가 5개이기 때문에 전체 파싱되는 메서드의 개수가 5개인지 확인하는 assertion을 추가하였습니다(github 코드를 참조 하랍니다). HttpMethod, URI 파싱 [https://github.com/bperhaps/ComponentScan/tree/URI_parse] 다음으로 진행해야 할 것은, mapping annotation들로부터 uri를 파싱하는 것 입니다. 저희가 만들어야 할 요구사항이기 때문이죠😉 그 전에, 저희 팀에서는 RequestMapping의 경우는 전역으로 공통되는 부분을 추려내기 위해 Controller쪽에서만 사용하고 있고, 메소드단위에서는 GetMapping, PostMapping…. 등을 사용하고 있습니다. 따라서 이러한 컨벤션을 지킨다는 가정하에 나머지 구현을 진행하도록 하겠습니다. 먼저 Spring에서 URI은 RequestMapping에 정의된 URI와 Method단위에 매핑된 Mapping 어노테이션이 합쳐져서 만들어집니다. @RequestMapping(""/controller2"")
@RestController
public class ScanController2 {

@ForLoginUser
@GetMapping(""/getTestLogin"")
public void getTest() {

}
}
따라서 위와같이 있다면, /controller/getTestLogin 과 같이 URI가 생성되어야 하죠. 기본적으로 위와 같은 Mapping 어노테이션들에 적힌 URI는 values()라는 메서드에 정의되어 있습니다. 어노테이션으로 부터 값을 불러오는 방식은 간단합니다. 해당 메서드(value())를 호출하기만 하면 되는데요, 위에서 메서드와 컨트롤러 파싱까지 성공했으니 파싱한 컨트롤러와 메서드에서 Mapping 어노테이션을 추출하여 value를 추출해 보도록 합시다. 먼저, 각 메서드가 어떤 Mapping Annotation을 가지고 있는지 확인해야하고, 해당 어노테이션으로 부터 value를 추출해야 합니다. 일단 간단하게 enum을 이용해서 각 HttpMethod들을 컨트롤 할 수 있도록 하였습니다. typeToken은 method에 정의된 annotation과의 타입비교를 위해 있고, httpMethod는 추후에 해당 method가 어떤 HttpMethod에 매핑되어 있나 반환해주기 위해 , 마지막 values는 어노테이션으로 부터 value를 추출하기 위해 존재합니다. 지금은 이해가 잘 안가실 수도 있으니 프로덕션을 통해서 한번 살펴봅시다. 먼저, method에 있는 Mapping어노테이션을 파싱하여 value로 정의된 URI를 파싱하는 로직입니다. 간단히 method를 인자로 받고, value를 추출하는 로직으로 작성했습니다. 또한 매칭되는것이 없다면 빈 문자열이 들어있는 리스트를 반환하도록 했습니다. 이때 Function<> values가 왜 존재하는지 궁금해 하실수도 있는데, 어노테이션은 기본적으로 상속이 불가하기 때문에 다형성을 이용하지 못합니다. 따라서 모든 Mapping annotation이 동일하게 value() 메서드를 가지고 있더라도 Class<? extends Annotation> 타입을 통해 value() 메서드를 호출시키는 게 불가합니다. 따라서 위와같이 약간은 정적으로 코드르 작성해 주셔야 합니다(혹시 리플랙션으로 value를 추출시킬 수 있는지 시도해 보았지만 Annotation이 인스턴스가 객체가 아니기 때문에 invoke가 불가능 한것 같더군요). 이젠, 마찬가지로 Controller쪽에 RequestMapping이 있다면 값을 추출하는 로직도 작성하도록 하겠습니다. 간단하군요! 모든 준비과정은 끝났습니다. 지금까지 만든 객체들을 조합하여 URI를 파싱하는 클래스를 작성하도록 하겠습니다. 소스가 조금 길어서 메인 로직만 추출해 왔습니다. 자세한 소스는 깃허브를 참조해 주세요! 컨트롤러로 부터 URI를 추출하고, 메소드로부터 URI를 추출하고, 둘을 조합하여 최종 URI를 만들어내는 로직입니다. 테스트를 한번 작성해 봅시다. 멋지게 통과하는군요! 이로서 URI 파싱로직이 완성되었습니다. httpMethod 파싱 [https://github.com/bperhaps/ComponentScan/tree/extract_httpMethod] 요구사항의 마지막, HttpMethod를 파싱해 보도록 하겠습니다. 모든 준비물은 완성되어있으니 또 다시 조합하면 끝입니다. 우리는 위에서 URI를 파싱로직을 완성시켰는데요, 해당 URI에 매핑된 HttpMethod 또한 가지고 오기 위해서는 두 데이터를 담을 자료구조가 필요합니다. 간단하게 자바 bean규약에 의거하여 생성해 주었습니다. 위에서 우리가 HttpMethods enum을 생성할 때 필드로 HttpMethod를 넣어주었습니다. 바로 지금 이 기능을 위해 생성해둔 것 인데요, Method를 인자로 받고 매핑된 HttpMethod를 찾아 반환하는 로직을 구현해 보겠습니다. enum 내부에 구현하였습니다. 간단하죠? 이젠 위에서 만들었던 URIScanner가 기존처럼 URI만을 반환하는것이 아닌, Method까지 함께 반환하도록 로직을 변경해 보겠습니다. 이 부분 또한 메인로직만 들고왔습니다 기존 URI String만 반환했던것과는 다르게 위에서 만든 data structure를 반환하는것을 볼 수 있습니다. 테스트 코드또한 위와같이 변경하였고 정상적으로 동작하는것을 볼 수 있습니다. 이로써 우리가 원하는 URI와 HttpMethod 파싱 로직을 완성하였습니다!! (와~~) 하지만, 이 기능만 가지고는 뭔가를 하기가 애매합니다. 우리가 원하는 것은 프로그램이 로딩되면, 자동으로 모든 class들을 파싱한 뒤 파싱한 데이터를 가지고 특정 행동을 하는것 이기 때문입니다. Component Scan 필요한 정보들은 Reflection을 사용하여 간단하게 추출 가능한데, 문제는 ClassLoader의 동작 특성상 모든 클래스들의 Type Token을 불러올 수 없습니다(기본적으로 필요한 클래스를 동적으로 불러오는 방식을 사용하니까요). 따라서 Class.forName() 을 이용하여 클래스의 canonical path를 통해 Type Token을 가지고 와야 하는데요. 이를 위해서 Spring의 Component Scan과 비슷한 역할을 하는 모듈을 구성해야 합니다. Reflections[https://github.com/bperhaps/ComponentScan/tree/using_reflections] 가장 간단한 방법은 Reflections 라이브러리[https://github.com/ronmamo/reflections]를 사용하는 것 입니다. 이미 충분히 오랜 기간 검증받고 많은 곳에서 사용하는 라이브러리임만큼 마음놓고 사용할 수 있는데요. 한번 Reflections를 이용해서 구현해 보도록 하겠습니다. 먼저, Reflections 라이브러리에 대한 의존성을 만들어줍니다. 그런 뒤 reflections를 이용하여 모든 Class들에 대한 typeToken을 받아올 수 있도록 합니다. basePackage는 파싱할 package의 최상위 경로를 넣어주면 됩니다. 테스트 코드를 작성해 보겠습니다. 테스트에는 처음에 만들었던, controller들이 있는 패키지를 기준으로 하였습니다. 정상적으로 3개의 컨트롤러를 파싱하는것을 볼 수 있습니다. 그러면 이 기능을 이용하여 기능을 구현해 보도록 하겠습니다. 요구사항은 아래와 같이 구현할 것 입니다. GET /getUriAndMethods 를 수행하는 컨트롤러 메서드를 생성한다. ArugumentResolver를 통해서 스프링부트 부팅 시 초기화 한 어노테이션이 붙은 controller method들의 URI와 HttpMethod들을 반환받는다. 이를 반환 시킨다. 먼저 ArgumentResolver를 생성하겠습니다. 생성자로 초기화 시점에 파싱한 데이터를 받아올 수 있도록 하고, 이를 반환하도록 구성하였습니다. 다음은 WebMvcConfiguration입니다. 리졸버를 등록해 주었고, getUriAndMethods() 를 통해 원하는 데이터를 파싱하도록 하였습니다. 마지막으로 Controller 부분입니다. 마지막으로 테스트를 통해 정상적으로 동작하는지 확인해 보도록 하겠습니다. 정상적으로 동작하는것을 볼 수 있습니다. 이를통해 우리는 전체 클래스를 스캔하고, 어노테이션을 통해 필요한 데이터만 추출한 뒤 이를 사용하는 방법에 대하여 알아봤습니다. 위 예제를 응용하면 여러가지 행위들을 할 수 있겠죠? 하지만 이렇게 끝낸다면 재미가 없습니다. 이번에는 Reflections를 사용하기 않고 전체 클래스를 파싱하는 방법에 대해 알아보겠습니다. Class parsing without Reflections[https://github.com/bperhaps/ComponentScan/tree/using_raw] 위에서 우리는 클래스로더의 동작방식때문에 모든 클래스들을 typeToken을 런타임에서 가지고 오는것이 불가능 하다고 했습니다(이때 런타임에서 불가능하다는 것은, getAllClasses같은 메서드를 통해 메모리상에 있는 모든 클래스들을 가지고 오는것을 의미합니다). 따라서 Class.forName() 을 이용해야 한다고 했는데요. 전체 클래스들의 canonical path를 어떻게 가지고 올 수 있을까요? 방법은, 파일시스템을 이용하면 됩니다. 자바 프로젝트는 package경로와 파일시스템의 폴더구조가 동일합니다. 따라서 이를 이용하여 파일시스템을 탐색하면, 각 클래스들의 package canonical path를 구할 수 있습니다. 가장 먼저 해야할 일은 ClassL1oader가 참조하는 class파일들의 위치를 찾는 일입니다. 이 부분에서 저는 참 많은 애를 먹었는데요. 운영체제, 빌드 방식마다 소스들이 다르게 동작했기 때문입니다. 이번 글에서는 그 중에서 제가 성공한 방식을 설명드리겠습니다. 가장 먼저 알아내야 할 정보는 classPath입니다. ClassLoader가 참조하는 classPath를 알아내면, 해당 위치의 파일시스템을 통해 전체 class를 순회할 수 있습니다. 방법은 위와 같습니다. 현재 동작중인 클래스의 class파일이 위치하는 곳을 찾아서 classPath를 유추할 수 있습니다. (이 외에도 ClassLoader.getResource() 등의 방법을 통해 얻을 수 있지만, 위와같이 유추하는 이유는 뒤에서 설명하겠습니다). 위와같이 소스를 작성하면 아래와 같은 출력을 볼 수 있습니다. build/classes/java/main 으로 classPath를 찾은것을 확인할 수 있습니다. 위 코드가 프로덕션에 위치하기때문에 이러한 경로가 도출됩니다. 하지만 스캔의 범위를 test폴더까지 늘려야 할 필요가 있을수도 있습니다. 이를 위해 저는 아래와 같은 로직을 추가적으로 넣어줬습니다. 파싱한 경로의 마지막 부분을 하나 제거하여 하나 상위 폴더를 basePath로 만들어냈습니다. classes폴더 주고 상 java 폴더 내부에 main과 test(windows의 경우 production과 test)가 위치함으로 이렇게 하여 test와 main까지 탐색 범위를 늘릴 수 있습니다. 메인 로직입니다. 마지막에는 추출한 경로를 package 형식에 맞게 만들어 주는 로직입니다. 천천히 읽어보면 이해가 될겁니다. 추출한 basePath부터 시작하여 파일시스템 탐색을 진행합니다. 파일 순회에는 nio에서 제공하는 walkFileTree를 이용하였습니다. 위 소스는 walkFileTree를 이용할때 필요한 FileVisitor의 구현체 소스입니다. 파일에 접근하였을 때, 파일이 class파일인지, 또한 제가 정의한 baseRoot를 포함 하는지 확인한 후 모두 부합하다면 내부 컬렉션에 전체 경로를 넣어줍니다. 그리고, getAllCanonicalPaths() 메서드에서 이 컬렉션을 반환받게 됩니다. 이제 테스트를 한번 돌려보겠습니다. 테스트를 위에 테스트 폴더 내부에 위와같이 패키지를 하나 만들었습니다. 테스트코드는 위와 같이 작성하였고, 정상적으로 동작하는것을 볼 수 있습니다. 마지막으로 이전에 Reflections를 이용하여 작성한 로직을 지금까지 만든 객체로 교체해보겠습니다. ClassesScanner 로직을 위와같이 변경하였고, 이와 함께 변하는 다른 부분도 모두 변경시켰습니다. 이제 한번 이전에 작성한 인수테스트를 돌려보겠습니다. 정상적으로 동작하는걸 볼 수 있습니다. 성공? 자, 여기까지 왔으면 과연 성공일까요? 아닙니다. 막상 프로젝트를 빌드하여 jar파일로 추출한 뒤 동작시키면 제대로 동작하지 않는것을 알 수 있습니다. 모든 테스트를 통과하고 빌드도 성공하지만 jar 파일을 실행시키면 터집니다! 왜 이런일이 발생할까요? 바로, 우리가 canonical path를 추출하는데 있어 파일시스템을 사용했기 때문입니다. 우리가 ide를 이용하여 빌드를 진행했다면, 당연히 build/classes에 클래스파일이 있습니다. 하지만 jar파일을 어떤가요? jar라는 압축파일 내부에 class파일이 존재합니다. 따라서 파일 순회가 불가능한 문제점이 발생합니다. 이 문제를 해결해 봅시다. 위에서 제가 classPath를 찾기위해서 source파일의 위치를 확인하는 방식을 사용했었습니다. 바로 지금 이 문제를 해결하기 위함이었습니다. 먼저, 실행중인 jar파일의 위치를 찾는게 중요합니다. 인터넷에 있는 수 많은 방법을 시도했지만 운영체제 등 환경의 영향을 많이 받았습니다. 그 중, 모든 환경에서 동작하던 방식이 위 방식이라 채택했습니다. 방법은 다음과 같습니다. jar파일의 위치를 찾았다면, 임시 폴더에 jar폴더를 풉니다. 그리고 그 폴더의 경로에서 class들을 파싱하면 위 로직을 동일하게 사용할 수 있습니다. 일단, jar파일에서 실행했을 때 경로가 어떻게 보이는지 확인해봅시다. 이전 소스와 달라진 점이 있다면, 마지막이 getPath()가 아닌 toString()으로 변경되었습니다. 운영체제마다 다른데, getPath()를 사용하면 path가 null로 나옵니다. 보이시는것 처럼 경로가 위와같이 도출됩니다. 위 형식은 모든 운영체제에서 동일합니다. 따라서 이에 맞게 경로를 파싱해주면 됩니다. 파싱 로직은 위와 같습니다. jar파일인 경우 경로의 시작이 jar로 시작하기 때문에 위와같이 분기를 주어 처리하였습니다. 변경 후에도 모든 테스트가 정상적으로 통과 하는군요. 이제 jar파일일 때 압축을 해제시키고 이를 basePath로 사용하는 로직을 작성하겠습니다. CanonicalPathScanner 의 로직이 이렇게 변경되었습니다! 간단하지요? 임시폴더에 zip 해제하는 소스는 인터넷에 많으니 설명 생략하도록 하겠습니다. 이제 한번 잘 돌아가는지 jar빌드 후 확인해 보겠습니다. 정상적으로 동작합니다!! 이로서 모든 구현이 완료되었습니다. 깃들다 팀에서는 위와같이 모든 구현을 진행하고, interceptor 자동등록을 구현했습니다. 여러분도 이 지식을 활용하여 여러 기능들을 만들어보길 바랍니다 감사합니다🙂 Twitter Facebook Google+ # 백엔드 # interceptor # 자동화",BE
638,"우리 프로젝트의 정적 분석 리포트를 만든 과정을 남겨보도록 한다.


## 정적 분석이란?

소프트웨어를 개발할 때 우리가 유념해야 할 것들이 있다.

• 컨벤션을 잘 지키고 있는가?
• 내 코드 안에 보안 취약점이 있을까?
• 코드가 반복되고 있지는 않을까?
• 의도하지 않은 버그가 발생하는가?
• 새로 작성한 코드를 위한 테스트가 잘 마련되어 있나?

그런데 여러 팀원이 다양한 기능을 구현하다보면 다양한 이유로 그 요소들을 고려하지 못하고 넘어갈 수 있다. 시간이 없어서 자세히 못 볼 수도 있고, 개발도 결국 사람이 하는 일이라 예상하지 못한 문제가 발생할 수 있기 마련이다. 프로젝트의 규모가 커질수록, 그리고 함께 작업하는 팀원의 수가 많아질수록 이런 상황이 발생할 확률이 높아진다.

문제점이 생겼을 때 확실히 발견하기 위해 개발자는 정적 분석 도구를 이용할 수 있다. '정적'이라는 표현에서 알 수 있듯 '동적 분석'도 있는데, 이 둘의 가장 큰 차이점은 문제점을 찾기 위해 소프트웨어를 실행하는지에 있다. 정적 분석은 소프트웨어를 실제로 실행하지 않고 소스코드를 이용하여 분석하는 방법이다.


## Sonarqube(소나큐브) 란?

정적 분석 툴의 종류로 여러가지가 있는데, 그 중 프로젝트에 사용한 것은 Sonarqube다. 위키피디아의 말을 빌리자면 Sonarqube는 20개 이상의 프로그래밍 언어에서 버그, 코드 스멜, 보안 취약점을 발견할 목적으로 정적 코드 분석을 하고 지속적으로 코드 품질을 검사한다. 검사 후 보고서를 제공하는데, 앞서 말한 보안 취약점, 중복코드, 그리고 테스트 커버리지와 같은 개발자가 유념해야할 것들에 관한 정보가 보고서에 포함된다.


## 소나큐브 설치

```
docker run -d --name sonarqube -p 9000:9000 sonarqube
```










• 도커를 이용하여 EC2에 소나큐브를 설치했다.
• 참고로 소나큐브는 일부 기능만 무료로 제공한다. 그래서 어떤 에디션의 소나큐브를 다운받을지 선택해야 하는데, 도커에서도 에디션을 골라서 이미지를 받아올 수 있다. 위의 커맨드를 실행하면 Community Edition, Developer Edition, 그리고 Enterprise Edition이 모두 포함된 이미지를 pull한 뒤 컨테이너를 생성한다.
• 컨테이너가 생성되면 [EC2 public ip]:9000을 통해 소나큐브에 접속할 수 있다. 로그인을 해야하는데, 기본 계정은 다음과 같다.
login = admin
password = admin
• login = admin
• password = admin


## 토큰 및 프로젝트 생성

Gradle 프로젝트에서 방금 만든 소나큐브 서버에 접근할 수 있도록 토큰을 생성한다.

```
우측 상단의 프로필 -> My Account -> Security -> 토큰 이름 입력 -> Generate
```










생성한 토큰은 안전한 곳에 저장한다. 모두 완료 했으면 다시 소나큐브 메인 화면으로 돌아와 프로젝트를 생성한다.

```
Create Project -> manually -> 프로젝트 이름 및 Project Key 입력
```










여기서 Project Key는 우리 Gradle 에서 소나큐브 서버에 연결할 때 해당 소스코드가 어떤 프로젝트와 관련된 것인지 명시할 때 쓰인다.


## build.gradle 설정

이제 우리 프로젝트의 소스코드를 분석하고 정적 분석 리포트를 만들어야 한다. 정적 분석을 할 프로젝트의 build.gradle 파일로 간다.


### 1. plugin

```
plugins {
id ""org.sonarqube"" version ""3.3""
id 'jacoco'
}
```










위와 같이 플러그인에 소나큐브와 jacoco(자코코)를 추가해야 한다. 그런데 자코코가 뭐지?


#### Jacoco 란?

소나큐브가 테스트 커버리지에 대한 정적분석을 하기 위해서는 테스트 리포트를 전달받아야 한다. 자코코는 테스트를 실행할 때 이 리포트를 만들어주는 라이브러리로, 다음과 같은 기능을 제공한다.

• 바이트 단위, 라인 단위, 그리고 브랜치(조건문의 분기) 단위 테스트 커버리지 검사
• 검사 후 html, xml, 그리고 csv 형태로 리포트 생성 (선택 가능)
• 커버리지가 일정 수준을 만족하지 못하면 빌드를 금지


### 2. tool version

```
jacoco {
toolVersion = '0.8.5'
}
```










플러그인에 자코코를 추가했다면 이제 자코코 툴 버전을 명시한다. 0.8.0 버전 이상을 사용하는 것이 권장되는데, 롬복 생성 코드에 대한 커버리지를 제외하기 위해서다. 일단 build.gradle 설정을 다 끝내고 롬복 관련 설정을 마무리 짓도록 한다.

이제 jacocoTestCoverageVerification과 jacocoTestReport를 사용할 수 있다.


### 3. test

```
test {
jacoco {
destinationFile = file(""$buildDir/jacoco/jacoco.exec"")
}
useJUnitPlatform()
finalizedBy 'jacocoTestReport'
}
```










테스트가 테스트가 실행될 때 jacoco설정을 명시한다. 우리 프로젝트에서는 커버리지 결과 데이터를 저장할 경로를 지정하는 설정을 명시했다. 그리고 finalizedBy 'jacocoTestReport' 를 추가하여 test 태스크 이후 jacocoTestReport 태스크가 실행되도록 한다. 명시하지 않은 설정들은 아래와 같은 디폴트 값이 들어간다.

```
enabled = true
destinationFile = file(""$buildDir/jacoco/$.exec"")
includes = []
excludes = []
excludeClassLoaders = []
includeNoLocationClasses = false
sessionId = ""<auto-generated value>""
dumpOnExit = true
classDumpDir = null
output = JacocoTaskExtension.Output.FILE
address = ""localhost""
port = 6300
jmx = false
```











### 4. jacocoTestReport

```
jacocoTestReport {
reports {
html.enabled true
xml.enabled true
csv.enabled false
}
finalizedBy 'jacocoTestCoverageVerification'
}
```










해당 태스크를 이용하여 커버리지 결과를 저장할 방식을 정할 수 있다. 개발자가 보기 쉬운 형태인 html 형태와 소나큐브에 전송하기 위한 xml 형태로 저장되도록 했고, finalizedBy 'jacocoTestCoverageVerification'을 추가해서 해당 태스크 이후 실행될 태스크를 명시했다.


### 5. jacocoTestCoverageVerification

해당 태스크를 이용하여 커버리지 기준을 세우고, 코드가 그 커버리지를 만족하는지 확인할 수 있다.

```
jacocoTestCoverageVerification {
violationRules {
rule {
enabled = true
// 아래의 커버리지 기준을 클래스 단위로 검사하겠다는 설정
element = 'CLASS'

limit {
// 분기에 대한 커버리지 기준 : 최소 80%
counter = 'BRANCH'
value = 'COVEREDRATIO'
minimum = 0.80
}

limit {
// 코드 라인에 대한 커버리지 기준 : 최소 80%
counter = 'LINE'
value = 'COVEREDRATIO'
minimum = 0.80
}

// 커버리지 기준을 만족하는지 검사할 때 제외할 패키지
excludes = [
'botobo.core.infrastructure.**',
'botobo.core.exception.**',
'botobo.core.dto.**',
'botobo.core.DataLoader',
'botobo.core.BotoboApplication'
]
}
}
}
```










자코코 설정에 대한 정보는 이 곳을 참고했다.


### 6. sonarqube

build.gradle 파일에 소나큐브 관련 설정도 추가한다.

```
sonarqube {
properties {
// 소나큐브 프로젝트 생성할 때 만들었던 프로젝트 키
property ""sonar.projectKey"", ""botobo-develop""
// 소나큐브가 실행되고 있는 인스턴스의 public ip
property ""sonar.host.url"", System.getenv('SONAR_URL')
// 소나큐브에서 만든 토큰
property ""sonar.login"", System.getenv('SONAR_LOGIN')
property ""sonar.language"", ""java""
property ""sonar.binaries"", ""$buildDir/classes""
property ""sonar.sources"", ""src/main""
property ""sonar.tests"", ""src/test/java""
property ""sonar.junit.reportsPath"", ""$buildDir/test-reports""
property ""sonar.java.coveragePlugin"", ""jacoco""
property ""sonar.coverage.jacoco.reportPaths"", ""$buildDir/jacoco/jacoco.exec""
// 소나큐브에서 볼 리포트에서 제외하고 싶은 디렉토리
property ""sonar.exclusions"", ""**/exception/**, "" +
""**/dto/**, "" +
""**/DataLoader.java, "" +
""**/BotoboApplication.java""
}
}
```










ip 주소나 토큰과 같은 민감한 정보는 젠킨스 서버의 환경변수에 값을 등록한 뒤 System.getenv를 이용하여 사용하도록 했다.


### 7. 롬복 설정

롬복으로 생성된 코드에 대한 커버리지는 제외하고 싶다면 백엔드 프로젝트의 root 디렉토리에 설정파일을 하나 추가해주면 된다.

```
lombok.addLombokGeneratedAnnotation = true
```











## 젠킨스 파이프라인

이제 젠킨스가 프로젝트를 테스트하고 빌드할 때 소나큐브에서 정적 분석 리포트를 만들도록 해야한다. 미리 만들어놓은 파이프라인에 아래의 스테이지를 추가하면 된다.

```
stage('SonarQube Analysis') {
steps {
withSonarQubeEnv(credentialsId: 'sonar_jenkins', installationName: 'botobo-develop') {
dir('backend') {
sh ""./gradlew sonarqube""
}
}
}
}
```










그리고 젠킨스에서 새로운 빌드를 실행하면 소나큐브 단계가 포함되는 것을 볼 수 있다. 소나큐브에서 해당 프로젝트 페이지에 들어가보면 리포트가 생성된 것을 볼 수 있다. 이 곳에서 현재 프로젝트에 어떤 취약점이 있는지, 개선할 수 있는 부분이 있는지, 그리고 중복되는 코드가 있는지와 같은 정보를 확인할 수 있다.",BE
639,"## 1. 서브모듈이 될 repo 만들기



### 서브모듈을 관리할 Private 저장소 추가




### Private 레포에 필요한 파일 생성




## 2. 메인 프로젝트에 서브모듈 추가하기

• backend/src/core/main/resources/dev 디렉토리에 서브모듈을 넣고싶은 상황



### 메인 프로젝트에서 Terminal 실행

```
cd backend/src/core/main/resources
git submodule add -b master [https://github.com/botobo-team/dev.git](https://github.com/botobo-team/dev.git) dev
```











• 서브모듈을 만들기 싶은 경로(resources)로 가서 dev라는 서브모듈을 만들었다. 메인 프로젝트의 Repo에 서브모듈이 추가된 것을 확인할 수 있다.





## 3. 젠킨스 설정



### Source Code Management 구간

• 아래 두가지를 추가한다. (소스코드 레포지토리와 서브모듈 모두에 권한이 있는 계정을 사용하는 것을 추천한다.)
• 이미 젠킨스를 사용하여 CI/CD를 구축한 상태라면 이 두 정보는 이전에 추가했을 것이다.

• 서비스의 코드가 있는 레포지토리 정보
• 그 레포지토리에 권한이 있는 계정 정보


### 조금 아래에 있는 Additional Behaviours에서 Add를 누르고 Advanced sub-modules behaviours를 선택한다. 서비스 코드 레포지토리에 서브모듈이 있을 경우, 젠킨스에서 빌드할 때 추가 옵션을 선택하게 해주는 기능이다.





### 아래의 사진에서 보이는 세 체크 박스를 선택한다. 체크 박스의 의미는 다음과 같다:

• 레포지토리에 있는 서브모듈 모두 업데이트 (서브모듈을 pull하겠다는 의미)
• 업데이트 할 때 서브모듈이 바라보고 있는 브랜치의 가장 최신 커밋을 가지고 오기
• 서브모듈 레포지토리의 권한을 확인할 때 서비스 코드 레포지토리 권한 계정 사용


## 4. 서버


• 서버의 애플리케이션 실행 스크립트에 설정 파일 경로를 명시하는 플래그 추가
• 위와 같이 설정하고 새로운 빌드를 실행시켜보자.
• 젠킨스가 새로운 jar 파일을 빌드할 때마다 서브모듈의 변경사항을 확인하고, 변경된 부분이 있을 경우 자동으로 반영해줄 것이다.
• 여기서 명심해야할 것은 서브모듈이 있는 Private 저장소에 push를 해도 메인 프로젝트의 서브모듈은 이전 커밋을 바라보고 있기때문에 젠킨스 배포가 시작되지 않는다. (2021-botobo는 변화하지 않았기 때문)
• 젠킨스 배포를 시작하려면 서브모듈의 변화를 git submodule update --remote --merge의 명령어를 통해 메인 프로젝트로 가져온 뒤 push 해야한다.


## 5. 앞으로 프로젝트에서 서브모듈 활용법

• 이미 메인프로젝트의 develop의 서브모듈관련 설정은 되어있다.
• 하지만 자신의 로컬에 서브모듈 설정이 되어있지 않은 팀원들은 간단한 설정을 해야한다. (pull을 한다고해서 서브모듈의 데이터는 가져와지지 않는다.)


### 데이터를 가져오는 상황


#### 로컬에 서브모듈이 처음이라면 git submodule update --init --remote


#### 이후 서브모듈의 정보를 가져오고 싶다면 git submodule update --remote --merge (merge를 꼭 붙이자!)




### 데이터 푸시하는 상황 (로컬에서 서브모듈 안의 내용을 수정하는 경우)


#### 1. 메인 프로젝트의 경로에서 싱크를 맞춘다.

```
git submodule update --remote --merge
```











#### 2. 서브모듈 안의 내용을 수정한다.


#### 3. 로컬에서 서브모듈의 경로로 이동하여 커밋, 푸시한다. (Private 레포의 서브모듈에 직접 푸시하는 것)

```
cd backend/src/main/resources/dev
```











#### 4. 로컬의 메인프로젝트의 경로로 이동하여 커밋, 푸시한다. cd .. (2021-botobo 레포에 푸시하는 것)


### 클론하는 상황

```
git clone —-recurse—submodules https://xxxx.git
```












## 서브모듈이 최상위에 있을 때

build.gradle에 다음 내용을 작성한다.

```
task copyDev(type: Copy) {
from '../dev/application-dev.yml'
into './src/main/resources'
}

build {
dependsOn copyDev
}
```











### git에 남아있는 캐시를 확인할 때

https://stackoverflow.com/questions/4185365/no-submodule-mapping-found-in-gitmodule-for-a-path-thats-not-a-submodule/13394710#13394710",BE
640,"> 해당 글은 블로그에도 작성해두었습니다.

우선 원하는 구조는 위와 같으며, 흐름은 다음과 같다.

• nginx는 proxy역할을 하며, 별도의 인스턴스로 구성한다.
• WAS는 Spring Application이며, 총 두 대로 구성한다.
• 두 대의 WAS는 하나의 Database를 바라본다.
• 클라이언트는 정해진 도메인으로 접속하면, nginx에서 round-robin 방식으로 적절한 WAS로 요청을 분배하며, 각 WAS는 동일한 DB를 바라보고 있기 때문에 동일한 데이터를 응답할 수 있다. 사실 너무 간단해서 글로 쓰기도 뭣하지만.. 나중을 위해!


## WAS 구축하기

Spring Application을 띄울 EC2 인스턴스를 하나 생성한다.

생성된 EC2에 접속한 뒤, 스프링을 띄울 수 있는 환경설정을 한다.

```
sudo apt update
sudo apt install default-jre
sudo apt install default-jdk
```










이후, 소스코드를 github에서 클론한다.

```
git clone -b {branch명} --single-branch {레포지토리 주소}

// 서브모듈이 포함되어있으며 레포지토리가 바라보고 있는 서브모듈을 가져올 경우
git clone -b {branch명} --single-branch {레포지토리 주소} --recurse-submodules

// 서브모듈이 포함되어있으며 최신의 서브모듈을 가져올 경우
git clone -b {branch명} --single-branch {레포지토리 주소} --remote-submodules
```










클론한 레포지토리에서 build.gradle이 있는 디렉토리로 이동한 뒤 소스코드 배포, 빌드를 진행한 뒤 실행한다.

```
./gradlew clean build

cd build/libs
java -jar -Dspring.profiles.active={activeProfile} *.jar &
```










이 과정을 두 개의 인스턴스에서 각각 진행한다.

이후 pull request 및 push에 따라 자동으로 빌드 및 실행이 진행되도록 하기 위해서는 jenkins에 연동하는 것이 좋다.

```
ps -ef | grep java
```










위 명령어를 통해 java -jar .. 이 프로세스로서 실행되고 있는지를 확인하는 것이 좋다. 빌드는 됐지만 실행이 안될수도..


## nginx에 로드밸런싱 설정하기

두 대의 WAS를 모두 띄웠다면, nginx 역할을 담당할 인스턴스를 생성한다. 인스턴스가 생성되었다면, 먼저 도커를 설치한다.

```
sudo apt-get update && 
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common && 
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - && 
sudo apt-key fingerprint 0EBFCD88 && 
sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"" && 
sudo apt-get update && 
sudo apt-get install -y docker-ce && 
sudo usermod -aG docker ubuntu && 
sudo curl -L ""https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)"" -o /usr/local/bin/docker-compose && 
sudo chmod +x /usr/local/bin/docker-compose && 
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
```










내가 진행할 nginx는 도커에서 설치하며, 우아한 테크코스에서 제공해주는 docker image를 사용한다. 나중에 서버에 직접 nginx를 설치해서 하는 방법도 시도해보아야겠다.

아무쪼록 도커가 설치되었다면, 이제 http://내도메인.한국 에 접속해서 사용할 무료 도메인을 생성한다. 그리고 TLS 설정을 진행한다.


## TLS 설정

서버의 보안과 별개로 서버와 클라이언트간 통신상의 암호화가 필요하기에 TLS를 설정해준다. 평문으로 통신할 경우, 패킷을 스니핑할 수 있다.

letsencrypt를 통해 무료로 TLS 인증서를 사용해보자.

```
docker run -it --rm --name certbot 
-v '/etc/letsencrypt:/etc/letsencrypt' 
-v '/var/lib/letsencrypt:/var/lib/letsencrypt' 
certbot/certbot certonly -d '[도메인 주소]' --manual --preferred-challenges dns --server https://acme-v02.api.letsencrypt.org/directory
```










위 명령어를 통해 인증서를 생성하는 중, Please deploy a DNS TXT record under the name 이라는 문구가 나오면 다음을 진행한다.

TXT는 내도메인.한국의 도메인 설정에서 진행하는 것이며, 내도메인.한국에서 입력을 모두 마치고 저장한 뒤 약 5초정도 기다린 다음 letsencrypt 설정을 완료해주면 정상적으로 완료된다.

생성한 인증서를 현재 경로로 이동한다.

```
sudo cp /etc/letsencrypt/live/[도메인주소]/fullchain.pem ./
sudo cp /etc/letsencrypt/live/[도메인주소]/privkey.pem ./
```










vi Dockerfile 이라는 명령어를 통해 Dockerfile을 생성하고, 다음과 같이 작성한다.

```
// Dockerfile
FROM nginx

COPY nginx.conf /etc/nginx/nginx.conf
COPY fullchain.pem /etc/letsencrypt/live/[도메인주소]/fullchain.pem
COPY privkey.pem /etc/letsencrypt/live/[도메인주소]/privkey.pem
```










같은 디렉토리에서 vi nginx.conf 파일을 생성한 뒤, 다음과 같이 작성한다.

```
events {}

http {
# 생성한 두 대의 WAS private IP를 upstream app 아래에 작성한다.
upstream app {
server {WAS private IP}:{포트번호};
server {WAS private IP}:{포트번호};
}

# Redirect all traffic to HTTPS
server {
listen 80;
return 301 https://$host$request_uri;
}

server {
listen 443 ssl;
ssl_certificate /etc/letsencrypt/live/[도메인주소]/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/[도메인주소]/privkey.pem;

# Disable SSL
ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

# 통신과정에서 사용할 암호화 알고리즘
ssl_prefer_server_ciphers on;
ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5;

# Enable HSTS
# client의 browser에게 http로 어떠한 것도 load 하지 말라고 규제합니다.
# 이를 통해 http에서 https로 redirect 되는 request를 minimize 할 수 있습니다.
add_header Strict-Transport-Security ""max-age=31536000"" always;

# SSL sessions
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;

location / {
proxy_pass http://app;
}
}
}
```










nginx가 여러 서버에 분배해주므로 upstream 서버에 작성한다. 로드 밸런싱은 요청을 고루 분배해서 서버로 넘겨주는데, 이 때 어떻게 분배할 지 규칙을 정해줄 수 있다.

지금 위 nginx.conf처럼 아무 규칙도 upstream에 적지 않고 server ip만 작성한다면 default인 round-robin 방식으로 동작하게 된다.

```
upstream app {
ip_hash;
server {WAS private IP}:{포트번호};
server {WAS private IP}:{포트번호};
}
```










ip_hash를 적어주게 되면, ip를 해시값으로 나눠주게 된다. 이렇게 분배하게 되면 한번 접속했던 ip는 같은 해시값을 가지므로 매번 동일한 서버에게 요청을 보내게 된다. 세션으로 무언가를 관리하는 경우 이 방법을 사용하면 좋을 것 같다. (약간의 뇌피셜)

이 외에도 여러가지 방법이 있다.

• round-robin(default)
• hash - hash <키> 형태로 작성하며 해시한 값으로 분배한다. ex)hash $remote_addr (= 이는 ip_hash와 같다)
• ip_hash - 아이피를 해싱해 요청을 분배한다.
• random
• least_conn - 연결수가 가장 적은 서버에 가중치를 고려해서 분배
• least_time - 연결수가 가장 적으면서 평균 응답시간이 가장 적은 쪽을 선택해서 분배 자세한 내용은 nginx-stream-upstream-module 에서 확인할 수 있다.

방법을 설명하던 중 가중치라는 말이 나왔는데 특정 서버에 2배의 가중치를 주고 싶다면 다음과 같이 작성하면 된다.

```
upstream app {
server {WAS private IP}:{포트번호} weight=2;
server {WAS private IP}:{포트번호};
}
```










위와 같이 작성하면 규칙은 라운드 로빈으로 유지하되 첫번째 서버를 2배 더 많이 사용하게 된다.

nginx의 configuration을 통해 로드밸런싱 설정을 모두 마쳤으니, 도커 컨테이너를 띄우면 정상적으로 로드밸런싱이 가능해진다.

```
docker build -t nextstep/reverse-proxy:0.0.2 .
docker run -d -p 80:80 -p 443:443 --name proxy nextstep/reverse-proxy:0.0.2
```










끗",BE
641,"# AWS 인스턴스 배포 및 젠킨스를 활용한 CI/CD 설정


## AWS 인스턴스 배포하기

• 인스턴스를 생성한다.
• 도커를 설치한다.

```
sudo apt-get update && 
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common && 
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - && 
sudo apt-key fingerprint 0EBFCD88 && 
sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"" && 
sudo apt-get update && 
sudo apt-get install -y docker-ce && 
sudo usermod -aG docker ubuntu && 
sudo curl -L ""https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)"" -o /usr/local/bin/docker-compose && 
sudo chmod +x /usr/local/bin/docker-compose && 
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
```










• 내도메인.한국을 이용해 도메인을 생성한다.

```
// 고급 설정
IP연결(A)에 연결할 인스턴스의 public IP를 작성한다.
```










• TLS를 설정한다.

```
sudo docker run -it --rm --name certbot 
-v '/etc/letsencrypt:/etc/letsencrypt' 
-v '/var/lib/letsencrypt:/var/lib/letsencrypt' 
certbot/certbot certonly -d 'yourdomain.com' --manual --preferred-challenges dns --server https://acme-v02.api.letsencrypt.org/directory


TLS 명령 실행 중 나오는 값을 고급 설정>TXT 필드에 각각 구분하여 입력한다.
```










• 인증서를 현재 경로로 옮긴다.

```
cp /etc/letsencrypt/live/[도메인주소]/fullchain.pem ./ &&
cp /etc/letsencrypt/live/[도메인주소]/privkey.pem ./
```










• DockerFile을 생성한다.

```
vi DockerFile

// 내용
FROM nginx

COPY nginx.conf /etc/nginx/nginx.conf
COPY fullchain.pem /etc/letsencrypt/live/[도메인주소]/fullchain.pem
COPY privkey.pem /etc/letsencrypt/live/[도메인주소]/privkey.pem
```










• nginx.conf 파일을 작성한다.

```
vi nginx.conf

// 내용
events {}

http {
upstream app {
server 172.17.0.1:8080;
}

# Redirect all traffic to HTTPS
server {
listen 80;
return 301 https://$host$request_uri;
}

server {
listen 443 ssl;
ssl_certificate /etc/letsencrypt/live/[도메인주소]/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/[도메인주소]/privkey.pem;

# Disable SSL
ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

# 통신과정에서 사용할 암호화 알고리즘
ssl_prefer_server_ciphers on;
ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:DH+3DES:!ADH:!AECDH:!MD5;

# Enable HSTS
# client의 browser에게 http로 어떠한 것도 load 하지 말라고 규제합니다.
# 이를 통해 http에서 https로 redirect 되는 request를 minimize 할 수 있습니다.
add_header Strict-Transport-Security ""max-age=31536000"" always;

# SSL sessions
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;

location / {
proxy_pass http://app;
}
}
}
```










• 도커 컨테이너를 띄운다.

```
sudo docker build -t nextstep/reverse-proxy:0.0.2 . && 
sudo docker run -d -p 80:80 -p 443:443 --name proxy nextstep/reverse-proxy:0.0.2
```










• 운영 DB를 설치한다.

```
sudo apt update && sudo apt install mysql-server
```










• 운영 DB에 사용자 및 데이터베이스를 등록한다.

```
sudo mysql -uroot -p

create user '{MySQL 유저명}'@'{WAS Server Private IP}' identified by '{MySQL 유저 password}';
CREATE DATABASE {schema} DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;
grant all privileges on {schema}.* to {MySQL 유저명}@'{WAS Server Private IP}';
flush privileges;
```










• 자바 설치

```
sudo apt update && sudo apt install default-jre && sudo apt install default-jdk
```










• 빌드

```
./gradlew clean build

// jar file 찾기
find ./* -name ""*jar""

// 실행
nohup java -jar -Dspring.profiles.active=dev [jar파일명] 2>&1 &
```











## 젠킨스 CI/CD 설정하기

• 보또보 팀 Jenkins url: http://52.79.46.9:8080/

> 상황 가정: 젠킨스 인스턴스와 서버 인스턴스가 구분되어있다.
> 디렉토리 구조가 gradle이 최상위가 아닌 backend 폴더에 포함되어있다.

젠킨스 자동배포 1편까지는 동일하게 진행한다.
젠킨스 자동배포 2편에서는 다음과 같은 부분에서 변경 사항이 발생한다.


## 변경 사항 - 아이템 설정


### 소스코드 관리 - Add Credentials

아이템의 소스코드 관리에서 Credentials를 Add할 때 다음과 같이 진행한다.

• Kind에서 Username with password를 선택한다.
• Username에는 깃허브 아이디, Password에는 깃허브 패스워드를 기입한다.


### Build

Use Gradle Wrapper를 선택한 뒤, Make gradlew executable을 체크하고 Wrapper location에 다음과 같이 기입한다.

```
${workspace}/backend
```










Tasks에는 clean build를 작성한다.


### 빌드 후 조치 - Transfers

```
// Source files
backend/build/libs/*.jar

// Remove prefix
backend/build/libs

// Remote directory
/deploy (작성하지 않으면 최상위 폴더로 jar이 온다)

// Exec command
sh /home/ubuntu/{실행할 스크립트 명}.sh

고급을 누른다.
// Exec timeout
0
```",BE
644,"# 기존 검색 API

/api/search/workbooks?type=name&criteria=date&order=desc&keyword=JAVA&start=0&size=10

• type: name, tag, user 중 택 1 (검색할 때 종류)
• criteria: date, name, count, like 중 택 1 (조회할 때 정렬 기준)
• order: asc (오름차순), desc (내림차순)
• keyword: 검색어
• start: 페이징 시작점
• size: 가져올 문제집 개수

보또보 기존 검색 api이다.

문제집 이름으로 검색, 태그로 검색, 유저명으로 검색 이렇게 3개로 나누어져있었고 검색을 실시간으로 할 수 있었다.

태그나 유저로 검색된 결과에서 최신순, 이름순, 카드개수순, 좋아요순으로 정렬이 가능했다.




# 새로운 검색 API

/api/search/workbooks?keyword=JAVA&tags=1&users=1&criteria=date&start=0&size=10

• criteria: date, name, count, like 중 택 1 (조회할 때 정렬 기준)
• keyword: 검색어
• tags: 태그 id (원하는 태그들로 필터링)
• users: 유저 id (원하는 유저들로 필터링)
• start: 페이징 시작점
• size: 가져올 문제집 개수

보또보 새로운 검색 api이다.

다른 검색 사이트 + 프롤로그 검색을 참고했다.

프롤로그의 태그로 필터링 하는 기능을 보고 우리도 태그 또는 유저 검색 기능을 필터링으로 바꾸자는 의견이 나와서 이를 적용시키고 싶었다.

하지만 실시간으로 검색된 결과에 태그나 유저로 필터링을 하는 것이 뭔가 어색한 것 같았고 실제 다른 검색 사이트에서도 검색된 결과 페이지에서 필터링을 주로 한다는 것을 발견할 수 있었다.

그래서 검색어에 대한 결과를 실시간으로 보여주는 것이 아닌 검색을 한 결과 페이지에서 보여주기로 했으며 태그, 유저 검색은 삭제하고 문제집 검색만 남기기로 했다. 문제집 이름으로 검색된 결과를 바탕으로 해당 문제집들의 태그, 유저로 필터링을 할 수 있고 최신순, 이름순, 카드개수순, 좋아요순으로 정렬이 가능하도록 했다.




# Criteria? QueryDSL?

기존의 검색 기능은 동적 쿼리 구현을 위해 JPQL 빌더로 criteria를 사용하고 specification을 이용하여 쿼리 조건을 처리하였다.


## Criteria + Specification 특징


### Criteria

Criteria는 JPQL을 자바 코드로 작성하도록 도와주는 빌더 클래스 API다.

Criteria를 사용하면 문자가 아닌 코드로 JPQL을 작성하므로 문법 오류를 컴파일 단계에서 잡을 수 있고 문자 기반의 JPQL보다 동적 쿼리를 안전하게 생성할 수 있다는 장점이 있다. 하지만 코드가 복잡하고 장황해서 직관적으로 이해가 힘들다는 단점도 있다.

뿐만 아니라 문법 오류를 컴파일 단계에서 잡을 수 있다곤 하지만 사용하다보면 필드명을 문자로 적어야 해서 실수를 한다면 컴파일 단계에서 잡을 수가 없게 된다.



물론 이를 위해 메타 모델 API를 사용하면 된다고 한다.

다만, 메타 모델 API를 사용하려면 메타 모텔 클래스를 만들어야 하는데 이는 코드 자동 생성기가 있어 만들어 준다고 한다. 이러한 코드 생성기는 빌드 도구를 사용해서 실행한다.


### Specification

Specification은 검색 조건을 추상화한 객체다.

즉, 검색 조건에 대해 Specification을 생성하고, 이를 통해 다양한 조건의 검색을 할 수 있다는 뜻이다.

우리 프로젝트로 예를 들면 어떤 타입의 검색어인지 어떤 순서로 정렬할 것인지를 Criteria를 이용해 만들고 이러한 다양한 검색 조건을 Specification으로 생성한 뒤 Repository 메서드 인자로 넘겨주어서 사용했다.


## QueryDSL 특징

Criteria는 위에서 말한 특징들의 장점을 확실하게 가지고 있다.

하지만 너무 복잡하고 어렵다는 가장 큰 단점이 있다.

작성된 코드를 보면 그 복잡성으로 인해 어떤 JPQL이 생성될지 파악하기가 쉽지 않다.

쿼리를 문자가 아닌 코드로 작성해도, 쉽고 간결하며 그 모양도 쿼리와 비슷하게 개발할 수 있는 JPQL 빌더가 바로 QueryDSL이다.


## Why use QueryDSL?

일단 어떤 기술을 적용하기 전에 반드시 '왜 적용하는가?' 에 대해 짚고 넘어가야 한다고 생각한다.

사실 이미 중간곰이랑 피케이가 Criteria, Specification으로 검색 기능을 잘 구현 해놓았기에 처음에는 QueryDSL을 적용할 필요가 있을까 싶었다.

하지만 곰곰히 생각한 끝에 QueryDSL로 변경하기로 했다.

그 이유는 두 가지이다.

• 기존의 Criteria, Specification를 적용한 코드를 내가 구현한게 아니기에 어차피 처음부터 공부했어야 했다. 둘 다 그렇게 해야하는거면 QueryDSL을 적용해보는 것도 괜찮겠다고 생각했다.
• 가독성 및 컴파일 오류다.

아무래도 Criteria로 작성한 코드는 한번에 의미를 파악하기 어려웠고 따라서 기존 코드에서 새롭게 바뀐 API대로 적용시키는게 생각보다 어려울 것 같았다.

또한 메타 모델 API를 사용하지 않는 이상 문자열 그대로 코드를 작성해야 하니 컴파일 단계에서 오류를 완전히 잡아주지 못하였다.

이러한 이유들로 Criteria에서 QueryDSL로 변경하며 새로운 검색 API를 적용시키기로 하였다.


# QueryDSL 설정

```
plugins {
//1
id ""com.ewerk.gradle.plugins.querydsl"" version ""1.0.10""
}

dependencies {
//2
implementation 'com.querydsl:querydsl-jpa'
}

//3
def querydslDir = ""$buildDir/generated/querydsl""

//4
querydsl {
jpa = true
querydslSourcesDir = querydslDir
}

//5
sourceSets {
main.java.srcDir querydslDir
}

//6
configurations {
querydsl.extendsFrom compileClasspath
}

//7
compileQuerydsl {
options.annotationProcessorPath = configurations.querydsl
}
```










빌드툴로 gradle을 사용했다.

신기하게도 QueryDSL 공식 문서를 보면 maven이나 ant로 설정하는 방법은 나오지만 gradle에서 설정하는 방법은 나오지 않는다. 그래서 검색을 통해 해결하였다.

설정들을 위에서부터 하나씩 설명하자면 이렇다.

• Q클래스 생성을 위한 QueryDSL 플러그인을 추가한다.
• QueryDSL 의존성을 추가한다.
• Q클래스가 저장되는 위치를 뜻한다. Q클래스는 자동으로 생성되는 파일들이라 .gitignore에 추가하여 깃헙에 올리지 않도록 하는것이 좋은데 build 디렉토리에 있는 파일들은 모두 올라가지 않고 있는 중이라 이렇게 설정하였다.
• jpa true로 하면 빌드할 때 자동으로 Q클래스가 생성된다. querydslSourcesDir는 Q클래스가 어디에 생성할지 결정한다.
• 빌드할 때 Q클래스가 생성된 위치를 나타낸다.
• gradle 6.x 버전에서 이 코드를 작성해야 정상작동한다고 한다. compile로 걸린 JPA 의존성에 접근하도록 해준다.
• annotation processor의 경로를 설정해주어 빌드 시 Q클래스가 생성되도록 해준다.

complieQuerydsl을 실행해주면 이렇게 원하는 위치에 Q클래스가 잘 생성된 모습을 볼 수 있다.




# 기존 코드와 변경된 코드 비교

```
@EqualsAndHashCode
@Getter
@NoArgsConstructor
@AllArgsConstructor(access = AccessLevel.PRIVATE)
public class WorkbookSearchParameter {

private static final int MINIMUM_START_PAGE = 0;
private static final int DEFAULT_START_PAGE = 0;
private static final int MINIMUM_PAGE_SIZE = 1;
private static final int MAXIMUM_PAGE_SIZE = 100;
private static final int DEFAULT_PAGE_SIZE = 20;

private SearchKeyword searchKeyword;
private SearchCriteria searchCriteria;
private int start;
private int size;

private WorkbookSearchParameter(SearchCriteria searchCriteria, SearchKeyword searchKeyword, int start, int size) {
this.start = start;
this.size = size;
this.searchKeyword = searchKeyword;
this.searchCriteria = searchCriteria;
}

public static WorkbookSearchParameter ofRequest(String searchCriteria,
String searchKeyword,
String start, String size) {
return of(
SearchCriteria.of(searchCriteria),
SearchKeyword.of(searchKeyword),
initializeStartValue(start),
initializeSizeValue(size)
);
}

public static WorkbookSearchParameter of(SearchCriteria searchCriteria,
SearchKeyword searchKeyword,
int start, int size) {
return new WorkbookSearchParameter(
searchCriteria,
searchKeyword,
start,
size
);
}

private static int initializeStartValue(String start) {
try {
int value = Integer.parseInt(start);
if (value < MINIMUM_START_PAGE) {
throw new InvalidPageStartException();
}
return value;
} catch (NumberFormatException e) {
return DEFAULT_START_PAGE;
}
}

private static int initializeSizeValue(String size) {
try {
int value = Integer.parseInt(size);
if (value < MINIMUM_PAGE_SIZE || value > MAXIMUM_PAGE_SIZE) {
throw new InvalidPageSizeException();
}
return value;
} catch (NumberFormatException e) {
return DEFAULT_PAGE_SIZE;
}
}

public PageRequest toPageRequest() {
return PageRequest.of(start, size);
}
}
```










기존 코드에서는 검색 조건 파라미터 값을 가지고 있으며 Specification을 이용해 검색 조건을 처리하는 클래스인 WorkbookSearchParameter이다.

여기에 타입, 정렬 기준 등을 처리해주는 SearchCriteria, SearchType, SearchOrder 등이 있다.

```
@EqualsAndHashCode
@Getter
@NoArgsConstructor
@AllArgsConstructor(access = AccessLevel.PRIVATE)
public class WorkbookSearchParameter {

private static final int MINIMUM_START_PAGE = 0;
private static final int DEFAULT_START_PAGE = 0;
private static final int MINIMUM_PAGE_SIZE = 1;
private static final int MAXIMUM_PAGE_SIZE = 100;
private static final int DEFAULT_PAGE_SIZE = 20;

private SearchKeyword searchKeyword;
private SearchCriteria searchCriteria;
private int start;
private int size;

@Builder
private WorkbookSearchParameter(String searchCriteria, String searchKeyword, String start, String size) {
this.start = initializeStartValue(start);
this.size = initializeSizeValue(size);
this.searchKeyword = SearchKeyword.of(searchKeyword);
this.searchCriteria = SearchCriteria.of(searchCriteria);
}

private int initializeStartValue(String start) {
try {
int value = Integer.parseInt(start);
if (value < MINIMUM_START_PAGE) {
throw new InvalidPageStartException();
}
return value;
} catch (NumberFormatException e) {
return DEFAULT_START_PAGE;
}
}

private int initializeSizeValue(String size) {
try {
int value = Integer.parseInt(size);
if (value < MINIMUM_PAGE_SIZE || value > MAXIMUM_PAGE_SIZE) {
throw new InvalidPageSizeException();
}
return value;
} catch (NumberFormatException e) {
return DEFAULT_PAGE_SIZE;
}
}

public PageRequest toPageRequest() {
return PageRequest.of(start, size);
}
}
```










```
@RequiredArgsConstructor
@Repository
public class WorkbookSearchRepository {

private final JPAQueryFactory jpaQueryFactory;

public Page<Workbook> searchAll(WorkbookSearchParameter parameter,
List<Long> tags,
List<Long> users,
Pageable pageable) {
QueryResults<Workbook> results = queryBy(parameter, tags, users)
.offset(pageable.getOffset())
.limit(pageable.getPageSize())
.fetchResults();
return new PageImpl<>(results.getResults(), pageable, results.getTotal());
}

public List<Workbook> searchAll(WorkbookSearchParameter parameter) {
return queryBy(parameter)
.limit(parameter.getSize())
.fetch();
}

private JPAQuery<Workbook> queryBy(WorkbookSearchParameter parameter) {
return queryBy(parameter, Collections.emptyList(), Collections.emptyList());
}

private JPAQuery<Workbook> queryBy(WorkbookSearchParameter parameter, List<Long> tags, List<Long> users) {
return jpaQueryFactory.selectFrom(workbook)
.innerJoin(workbook.user, user).fetchJoin()
.innerJoin(workbook.cards.cards, card)
.leftJoin(workbook.workbookTags, workbookTag)
.leftJoin(workbookTag.tag, tag)
.leftJoin(workbook.hearts.hearts, heart)
.where(containKeyword(parameter.getSearchKeyword()),
containTags(tags),
containUsers(users),
openedTrue())
.groupBy(workbook.id)
.orderBy(findCriteria(parameter.getSearchCriteria()), workbook.id.asc());
}

private BooleanExpression containKeyword(SearchKeyword searchKeyword) {
if (searchKeyword == null) {
return null;
}
String keyword = searchKeyword.getValue();
return containsKeywordInWorkbookName(keyword)
.or(equalsKeywordInWorkbookTag(keyword));
}

private BooleanExpression containsKeywordInWorkbookName(String keyword) {
return workbook.name.lower().contains(keyword);
}

private BooleanExpression equalsKeywordInWorkbookTag(String keyword) {
return workbook.workbookTags.any().tag.tagName.value.eq(keyword);
}

private BooleanExpression containTags(List<Long> tags) {
if (tags == null || tags.isEmpty()) {
return null;
}
return workbookTag
.tag
.id
.in(tags);
}

private BooleanExpression containUsers(List<Long> users) {
if (users == null || users.isEmpty()) {
return null;
}
return user
.id
.in(users);
}

private BooleanExpression openedTrue() {
return workbook.opened.isTrue();
}

private OrderSpecifier<?> findCriteria(SearchCriteria searchCriteria) {
if (searchCriteria == SearchCriteria.DATE) {
return workbook.createdAt.desc();
}
if (searchCriteria == SearchCriteria.NAME) {
return workbook.name.asc();
}
if (searchCriteria == SearchCriteria.COUNT) {
return card.countDistinct().desc();
}
return heart.countDistinct().desc();
}
}
```










QueryDSL로 바꾸면서 기존의 WorkbookSearchParameter는 파라미터로 들어온 값만 가지고 있고 이 값을 이용해 QueryDSL 전용 Repository에서 조회를 처리했다.

configuration으로 JPAQueryFactory를 빈으로 등록한 뒤 이를 이용해 동적 쿼리를 작성하는 방식이다.

디테일한 부분은 설명하기 아직 지식이 부족하지만 대략적으로 JPAQueryFactory를 사용해 동적 쿼리를 만드는데 파라미터로 넘겨온 값을 where 내에 있는 메서드들을 이용해 BooleanExpression을 반환받도록 하여 조건에 맞게 쿼리를 만들 수 있다.

물론 이와 함께 paging도 가능하다!

Criteria와 비교하면 상대적으로 가독성이 좋다는 것을 알 수 있다.


# 주의할 점

적용하다 생긴 트러블 슈팅을 공유한다.

• 소나큐브와 함께 사용하다 보니 테스트 커버리지를 통과하지 못해 빌드할 때 에러가 발생했다.

Q클래스도 테스트 커버리지 검증을 하다보니 발생하는 에러였는데 이를 위해 테스트 커버리지 검증에서 Q클래스를 제외시켰다.

역시나 검색을 해보니 이런 상황을 맞이한 분들이 있으셔서 이 블로그를 참고하였다. https://velog.io/@lxxjn0/코드-분석-도구-적용기-2편-JaCoCo-적용하기

jacocoTestCoverageVerification {
def Qdomains = []

for (qPattern in '*.QA'..'*.QZ') {
Qdomains.add(qPattern + '*')
}

violationRules {
rule {
enabled = true
element = 'CLASS'

limit {
counter = 'BRANCH'
value = 'COVEREDRATIO'
minimum = 0.80
}

limit {
counter = 'LINE'
value = 'COVEREDRATIO'
minimum = 0.80
}

excludes = [
//제외될 다른 클래스들
] + Qdomains
}
}
}
• distinct와 orderBy가 동시에 적용되지 않는 현상이 발생했다.

찾아보니 H2 문법 문제였고 MySQL에서는 같이 사용해도 적용이 되었다.

그래서 일단은 workbook의 id로 groupBy를 해서 distinct를 사용하지 않는 방향으로 수정했다. 프로젝트를 진행하며 Flyway를 적용할 때도 그렇고 이번에도 그렇고 local과 test에서는 H2를 사용하고 dev와 prod에서는 Mariadb를 사용하다 보니 H2와 MySQL의 문법 차이 때문에 여러 번 syntax 에러를 경험하는 일이 많았다.

그리하여 다음 스프린트 때는 이 차이를 해소시킬 방법을 찾아볼 생각이다.
• 이건 우리가 겪은 트러블 슈팅은 아니지만 gradle에서 QueryDSL 설정과 관련해서 많이들 이슈가 있다고 한다. 현재 우리가 적용시킨 설정 방법은 플러그인을 사용해서 Q클래스를 만드는데 이때 gradle 버전이 업그레이드 됨에 따라 여러 가지 설정을 추가해주어야 했다. 그래서 이 플러그인을 걷어내기 위해 gradle AnnotationProcessor를 사용하여 처리하는 방법이 있다고 한다. 이게 궁금하다면 http://honeymon.io/tech/2020/07/09/gradle-annotation-processor-with-querydsl.html 블로그를 참고하도록 하자.",BE
645,"# 무중단 배포 도입기

보고 또 보고 서비스의 백엔드 서버는 로드 밸런서 뒤에서 두 인스턴스로 운영되고 있다.
로드 밸런서는 두 인스턴스를 upstream 서버로 보고, 요청을 보낸 클라이언트의 ip를 기반으로 어떤 서버에 해당 요청을 보낼지 결정한다.
그런데 만약 요청을 보낼 서버가 내려가 있는 상태면 다른 서버로 그 요청을 전달한다.

Jenkins는 배포를 진행할 때 이 두 서버에 차례대로 들어가서 기존의 애플리케이션을 종료하고, 새로 만든 jar 파일을 이용해서 애플리케이션을 실행한다.
이것을 Rolling Deployment 방식이라고 한다. (Rolling Deployment 방식은 다양한 방법으로 구현할 수 있다.)

즉 지금 우리 인프라 구조는 사실 이미 무중단 배포가 자연스럽게 적용된 상태였다.


### 고려한 부분

Rolling Deployment 방식으로 배포를 진행하면 어느 한 서버가 잠시 내려가 있어도 다른 서버가 부하를 감당하기 때문에 다운 타임이 없다.
하지만 이 Rolling Deployment 방식에는 몇 가지 아쉬운 점이 있다:

• 특정 서버에서 새로운 애플리케이션을 실행할 때 다른 서버에 부하가 집중된다.
• 애플리케이션이 실행되지 않고 문제가 발생하면 롤백을 할 수 없다. 기존에 돌아가고 있던 애플리케이션을 종료하기 때문이다.
• 런타임 문제가 발생하면 애플리케이션을 직접 다시 실행해야한다.

그래서 Rolling Deployment 방식을 조금 변형해서 도입해보았다.


## 도입 후 현재 배포 흐름

• Jenkins가 jar 파일을 빌드한다.
• 첫번째 서버에 ssh 프로토콜로 들어가서 현재 애플리케이션이 실행되고 있지 않은 다른 포트에 새로운 애플리케이션을 실행한다.
• 잘 실행되고 있는지 health check를 한다.
• Health check에 성공하면 다른 서버로 ssh 한다.
• 두 번째 서버에서도 기존 애플리케이션이 실행되고 있지 않은 포트에 새로운 애플리케이션을 실행한다.
• 잘 실행되고 있는지 health check를 한다.
• Health check에 성공하면 로드 밸런싱을 하고 있는 리버스 프록시로 ssh 한다.
• 리버스 플록시가 바라보고 있는 포트를 바꾸고, 바뀐 설정을 적용하기 위해 reload 한다.

즉, Rolling Deployment 방식인데 따로 애플리케이션을 실행하고, 모두 잘 작동하는지 확인 후에 포트를 바꾼다는 점이 특이한 부분이다.


#### 해당 방식을 사용하면 다음과 같은 장점이 있다:

• 새로운 애플리케이션에 문제가 발생해도 기존 애플리케이션이 잘 돌아가고 있는 상태다.
• 다른 서버가 부하를 모두 감당하지 않아도 된다.
• 기존 애플리케이션으로 쉽게 롤백 할 수 있다.


## 구체적인 방법


### /infra/port api

현재 애플리케이션이 어떤 포트에서 실행되고 있는지 알려주는 api를 만든다.
8080 포트에서 애플리케이션이 실행되고 있으면 8000 포트를 쓰고,
8000 포트에서 실행되고 있으면 8080 포트에 새로운 애플리케이션을 실행한다.


### /infra/health api

현재 애플리케이션이 잘 실행되고 있는지 확인할 수 있는 api를 만든다.
애플리케이션을 실행시키고 10초후부터 30초동안 1초 간격으로 해당 api에 요청을 보낸다.
만약 'UP’이라는 응답이 돌아오면 애플리케이션이 잘 작동되고 있다고 보고 Jenkins에서 해당 서버와 ssh 를 종료한다.


### nginx 리버스 프록시

리버스 프록시 역할을 하고 있는 엔직엑스 설정을 두 개 준비한다. (nginx.conf, nginx2.conf)
하나의 설정 파일은 두 upstream 서버의 8080 포트를 바라보도록 하고, 다른 하나는 8000번 포트를 바라보도록 한다.
그리고 배포할 때마다 알맞는 설정 파일을 사용하여 nginx를 reload 한다. (nginx는 동적으로 reload 할 수 있어서 변경된 설정이 바로 적용된다.)

• 8080 포트와 8000 포트를 사용한 이유는 현재 ec2 보안 설정에서 이 두 포트에 대한 접근을 허용한 상태이기 때문이다.
• 설정 파일을 두 개로 나누고 번갈아 가며 사용한 이유는 Nginx 설정을 변경하는 것 중, upstream 서버들의 포트 번호를 바꾸는 것보다 이렇게 두 개의 파일을 바꿔가며 쓰는 것이 훨씬 빠르고 간편해서다.


## develop 서버

develop 서버에는 production 환경에 도입하기 전에 시험삼아 적용했고,
실제 production 환경에 도입하기 전에 시험삼아 적용했던 것이라 다시 되돌려 놓았다.
develop 서버의 역할이 예상하지 못한 에러를 찾는 것이라고 생각했기 때문이다.
그리고 부하를 분산시킬 이유도 없기 때문에 오버엔지니어링 느낌도 있다.",BE
646,#ERROR!,BE
647,"# Flyway란?

• Flyway란 DataBase Migration Tool로 DataBase Migration을 손쉽게 해결해주는 도구 중 하나이다.
그렇다면 DataBase Migration은 무엇일까?

DataBase Migration
마이그레이션(migration)이란 한 운영환경으로부터 다른 운영환경으로 옮기는 작업을 뜻하며, 하드웨어, 소프트웨어, 네트워크 등 넓은 범위에서 마이그레이션의 개념이 사용되고 있다.
데이터베이스에서 데이터 마이그레이션이란 데이터베이스 스키마의 버전을 관리하기 위한 하나의 방법이다.
예를 들어 dev 환경에서 데이터베이스 스키마가 변경되었지만, prod 환경에서 데이터베이스 스키마가 변경되지 않았을 경우 마이그레이션을 수행한다.
데이터베이스 마이그레이션은 개별 sql 파일을 데이터베이스 콘솔에서 직접 실행하지 않고 프레임워크의 특정 명령어를 통해 실행하고 이 결과를 별도의 테이블에 버전 관리를 하는 기법이다.
• 마이그레이션(migration)이란 한 운영환경으로부터 다른 운영환경으로 옮기는 작업을 뜻하며, 하드웨어, 소프트웨어, 네트워크 등 넓은 범위에서 마이그레이션의 개념이 사용되고 있다.
• 데이터베이스에서 데이터 마이그레이션이란 데이터베이스 스키마의 버전을 관리하기 위한 하나의 방법이다.
예를 들어 dev 환경에서 데이터베이스 스키마가 변경되었지만, prod 환경에서 데이터베이스 스키마가 변경되지 않았을 경우 마이그레이션을 수행한다.
• 데이터베이스 마이그레이션은 개별 sql 파일을 데이터베이스 콘솔에서 직접 실행하지 않고 프레임워크의 특정 명령어를 통해 실행하고 이 결과를 별도의 테이블에 버전 관리를 하는 기법이다.
• Flyway를 통해 손쉽게 각각의 dev, prod 환경에 동일한 데이터베이스 스키마를 적용시킬 수 있었고 변경 이력 또한 남아 관리하기 편하다는 장점이 있었다.



# Flyway를 왜 적용시켰는가?

• 사실 처음부터 Flyway를 찾아보고 적용시킨 것이 아니다.
(초반에 스키마가 변경되고 조앤이 코기 블로그 글을 보내줬을 때나 심지어 코기가 flyway 테코톡을 했을 때도 별 생각 없었다..)
• 그러던 중 스프린트가 진행됨에 따라 데이터베이스 스키마가 변경되고 dev, prod 환경에서 ddl-auto가 update로 설정되어 있다보니 DB가 꼬이는 현상이 발생하였다.
게다가 4차 데모데이 요구사항 중 하나가 prod 환경에 DB를 drop 하지말라는 것이 있었기에 이참에 flyway를 적용시켜보자 해서 진행하게 되었다.
조금 더 빨리 적용시켰으면 dev 환경에 DB 또한 drop 하지 않았을텐데.. 아쉽다..



# 간단한 사용법

flyway를 적용시켜보자.
당시 상황은 이러하다.

• User라는 테이블에 github_id라는 컬럼의 이름이 social_id로, 타입이 varchar(255)로 바뀌어야한다.
social_type이라는 컬럼이 추가되어야 한다.
• Heart라는 테이블이 추가되어야 한다.

지금 방법은 이미 DB에 데이터가 있을 때 적용시킨거라 처음부터 적용시킨 것과 조금은 다를 수도 있다는 것에 주의하자.

• dependency를 build.gradle에 추가한다. maven은 maven repository를 찾아보자!

implementation('org.flywaydb:flyway-core:6.4.2')
• application.yml 또는 application.properties에 다음과 같은 옵션을 추가한다. (yml 기준)

spring:
flyway:
enabled: true // true면 flyway를 활성화시킨다.
baseline-on-migrate: true
// default는 false로 true로 설정하면 히스토리 테이블이 없으면 새로 만든다.
• resources 디렉터리 밑에 db/migration 디렉터리를 만들고 적용할 스크립트를 작성해야 한다.
V1__init.sql 안에 테이블 스키마를 작성해두면 스크립트가 적용이 되어 테이블이 생성될 것이다.
하지만 이미 우리 프로젝트 DB에는 테이블이 만들어져있다.
그렇다면 V1__init.sql이 적용이 되는것인가?
이미 테이블이 있는 상태에서 스크립트가 적용이 된다면 에러가 발생하지 않을까라는 생각을 했다.
결론부터 말하면 baseline-on-migrate: true 설정에 의해서 이미 DB가 존재하고 V1__init.sql을 작성하였다면 V1__init.sql의 설정은 무시하고 기존의 DB 스키마를 baseline으로 잡게 된다.
그래서 사실 V1__init.sql의 내용을 비워놔도 무방하지만 스키마가 변경되기 전의 DDL로 작성해도 괜찮을 것 같다고 생각을 하여 작성해놓았다.






CREATE TABLE user(
id BIGINT AUTO_INCREMENT PRIMARY KEY,
github_id BIGINT NOT NULL,
user_name VARCHAR(255) NOT NULL,
profile_url VARCHAR(255) NOT NULL,
role VARCHAR(255) NOT NULL,
created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

CREATE TABLE workbook(
id BIGINT AUTO_INCREMENT PRIMARY KEY,
name VARCHAR(30) NOT NULL,
opened TINYINT(1) NOT NULL,
deleted TINYINT(1) NOT NULL,
user_id BIGINT not null,
created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
FOREIGN KEY(user_id) REFERENCES user(id) ON UPDATE CASCADE ON DELETE RESTRICT
);

CREATE TABLE card(
id BIGINT AUTO_INCREMENT PRIMARY KEY,
question BLOB NOT NULL,
answer BLOB NOT NULL,
encounter_count INT NOT NULL,
bookmark TINYINT(1) not null,
next_quiz TINYINT(1) not null,
workbook_id BIGINT not null,
deleted TINYINT(1) not null,
created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
FOREIGN KEY(workbook_id) REFERENCES workbook(id) ON UPDATE CASCADE ON DELETE RESTRICT
);

CREATE TABLE tag(
id BIGINT AUTO_INCREMENT PRIMARY KEY,
name VARCHAR(30) UNIQUE NOT NULL,
created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

CREATE TABLE workbook_tag(
id BIGINT AUTO_INCREMENT PRIMARY KEY,
workbook_id BIGINT not null,
tag_id BIGINT not null,
deleted TINYINT(1) not null,
created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
FOREIGN KEY(workbook_id) REFERENCES workbook(id) ON UPDATE CASCADE ON DELETE RESTRICT,
FOREIGN KEY(tag_id) REFERENCES tag(id) ON UPDATE CASCADE ON DELETE RESTRICT
);
• User 테이블 변경 내용을 V2__change_user_column.sql, Heart 테이블 추가 내용을 V3__add_heart_table.sql 라는 적당한 이름의 파일로 만들어 작성한다.
순서대로 적용시키기 위해 숫자를 1씩 증가시켜야 한다.
사실 이미 변경이 되었기에 V2를 작성할 때 한꺼번에 DDL을 작성해도 되지만 분리해서 히스토리로 남기기 위해 이렇게 작성하였다.
V2, V3를 한꺼번에 만들어도 V2와 V3가 순서대로 적용이 되었다!






alter table user change github_id social_id varchar(255) not null;
alter table user add column bio varchar(255) not null default '';
alter table user add column social_type varchar(255);

CREATE TABLE heart(
id BIGINT AUTO_INCREMENT PRIMARY KEY,
workbook_id BIGINT not null,
user_id BIGINT not null,
created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
FOREIGN KEY(workbook_id) REFERENCES workbook(id) ON UPDATE CASCADE ON DELETE RESTRICT
);
• 결과적으로 기존의 prod 환경의 DB 데이터는 유지한 채로 User 테이블의 기존 컬럼명 및 타입을 수정, 새로운 컬럼을 추가할 수 있었다!
• 이 후에 스키마가 변경이 되면 V4부터 스크립트를 작성해서 추가하면 된다.



# 주의할 점

적용하면서 애먹었던 부분을 공유한다.

• local 환경에서 H2 DB를 사용하게 되는 경우가 많을텐데 mariadb의 쿼리문을 이용한 스크립트와 같은 스크립트를 적용하다 syntax 에러가 많이 발생하였다. 이를 주의하도록 하자.
혹시나 DB마다 다른 스크립트를 적용하려면 flyway에서 벤더사마다 다른 스크립트를 적용하는 방법도 존재하니 그걸 찾아보도록 하자!
우리는 팀원들끼리 이야기한 결과 굳이 local에선 flyway를 사용할 필요가 있을까하여 flyway.enabled를 false로 설정하여 사용했다.
• 최신 버전의 flyway를 적용하면서 mariadb를 사용한다면 꼭 10.2 버전 이상의 mariadb를 사용하자.
10.1 버전의 mariadb를 사용하다 이런 에러를 맞이하게 되어 기존 mariadb를 10.4로 upgrade하는 상황이 발생하게 되었다.
조앤이 upgrade 하는 방법을 금방 찾아서 다행이었지만 생각보다 할게 많아 까다로웠다..
아니면 에러에 적힌대로 flyway pro edition을 사용하면 괜찮을지도..?",BE
693,"Gradle Test Fixture 사용해서 테스트 중복코드를 줄일 수 있어요. 멀티모듈 구조에서 다른 모듈의 Test 폴더를 사용할 수 없으니, 중복되는 클래스가 각각의 모듈의 Test 폴더에 포함되었어요. 저는, Gralde 멀티모듈 구조에서, 의존을 하는 모듈의 Test 폴더까지 사용하고 싶었어요. 이때 사용할 수 있는 것이 Gradle Test Fixture입니다 ㅎㅎ TestFixutre 적용 먼저 test fixture를 사용할 모듈에서 testfixture 의존성을 넣어요. (domain-cvi/build.gradle) plugins{
id 'java-library' //**new**!!
id 'java-test-fixtures' //**new**!!
id 'maven-publish' //**new**!!
}

bootJar { enabled = false }
jar { enabled = true }

dependencies {
implementation project(path:':common-cvi', configuration: 'default')
implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
runtimeOnly 'com.h2database:h2'
runtimeOnly 'mysql:mysql-connector-java'
implementation 'org.hibernate.validator:hibernate-validator:6.2.0.Final'

//migration
implementation 'org.flywaydb:flyway-core:7.11.2'

//testFixture
testFixturesImplementation 'org.springframework.boot:spring-boot-starter-test' //new!!
}
... testFixturesImplementation에 의존성이 추가된 것을 유의해주세요 (testFixture에서 사용할 라이브러리를 지정해줘야 합니다. ) testfixtures 폴더를 생성한 후, 멀티모듈 구조에 맞게 클래스를 생성해요. (com/cvi/CustomParameterizeTest) 이제 이를 사용할 모듈에서 아래와 같이 의존성을 추가해주면 끝! (app-cvi/build.gradle) version = '0.0.1-SNAPSHOT'

dependencies {
implementation project(path:':common-cvi', configuration: 'default')
implementation project(path:':domain-cvi', configuration: 'default')
testImplementation(testFixtures(project("":domain-cvi""))) **//new!!
...** 일줄 알았더니... Could not determine the dependencies of task ':app-cvi-api:test'.
> Could not resolve all task dependencies for configuration ':app-cvi-api:testRuntimeClasspath'.
> Could not resolve project :domain-cvi.
Required by:
project :app-cvi-api
project :app-cvi-api > project :domain-cvi-oauth-service
project :app-cvi-api > project :domain-cvi-publicdata-service
project :app-cvi-api > project :domain-cvi
> Module 'com.backjoongwon:domain-cvi' has been rejected:
Cannot select module with conflict on capability 'com.backjoongwon:domain-cvi:0.0.1-SNAPSHOT' also provided by [com.backjoongwon:domain-cvi:0.0.1-SNAPSHOT(default), com.backjoongwon:domain-cvi:0.0.1-SNAPSHOT(runtimeElements)]
> Could not resolve project :domain-cvi.
Required by:
project :app-cvi-api
> Module 'com.backjoongwon:domain-cvi' has been rejected:
Cannot select module with conflict on capability 'com.backjoongwon:domain-cvi:0.0.1-SNAPSHOT' also provided by [com.backjoongwon:domain-cvi:0.0.1-SNAPSHOT(default), com.backjoongwon:domain-cvi:0.0.1-SNAPSHOT(runtimeElements)]

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. 위와 같은 예외가 발생하네요... 예외의 이유 멀티모듈 구조를 완전히 잘못잡고 있었어요. 이전까지는 implementation project(path:':domain-cvi', configuration: 'default') 이렇게 다른 멀티모듈의 의존성을 끌어오고 있었는데, 이는 하위 의존성을 모두 다 가져오게 돼요. 즉, app-cvi-api 에서 많은 모듈을 의존하고 있는데, 여기서, app-cvi-api가 의존하고 있는 모듈들 또한 domain-cvi를 의존하며, 의존성이 엄청나게 겹쳤던 것이에요!!! 전체 모듈의 모듈 관련 의존성을 # app-api-cvi
dependencies {
implementation(project("":common-cvi""))
implementation(project("":domain-cvi""))
implementation(project("":domain-cvi-oauth-service""))
implementation(project("":domain-cvi-publicdata-service""))
implementation(project("":domain-cvi-scheduler""))
implementation(project("":domain-cvi-aws-s3-service"")) 혹은 # domain-cvi
dependencies {
compile(project("":common-cvi""))
compile 'org.springframework.boot:spring-boot-starter-data-jpa'
runtimeOnly 'com.h2database:h2'
runtimeOnly 'mysql:mysql-connector-java'
implementation 'org.hibernate.validator:hibernate-validator:6.2.0.Final' 이렇게 바꿨어요. 정리 그렇다면, 차이점이 뭘까요? 클라이언트 모듈 A(app-api-cvi), 의존 모듈 B(domain-cvi)라 정할게요. A 모듈에서 B 모듈을 implementation으로 의존했다 가정하면(implementation(project("":domain-cvi""))),B 모듈만 의존한다는 것이에요. 만약 C라는 모듈이 A를 의존하려 하려 한다면, B 모듈을 가져오지 못해요. 즉, implementation이라 선언된 것은, 모듈 내에서만 의존해서 쓰고, 자신을 의존하려는 모듈에는 제공하지 않아요. B 모듈 내의 의존성 (jpa)은 comile이라 선언되어 있어요. 이는, A 모듈이 B 모듈을 의존할 때, B 모듈은 jpa의존성도 제공해주겠다는 말이에요. (A 모듈은 B 모듈을 의존하며 JPA의존성도 사용할 수 있다. ) 정말 어려웠어요... 지금까지 대충 의존성을 주입하려 했었어요(반성합니다 ㅠㅠ) compile과 implementation의 차이를 알아가게 된 계기라 생각해요. testFixure의 자세한 내용은 꼭 공식문서를 통해 확인해주세요 :) Refer https://docs.gradle.org/6.8.3/userguide/java_testing.html#sec:java_test_fixtures https://bottom-to-top.tistory.com/58",BE
694,"반복 테스트(ParameterizedTest)에 대한 출력을 통일성 있게 하고싶어 아래와 같은 옵션들을 지정했어요. @ParameterizedTest(name = ""{displayName} {index}, args = {arguments}"") @DisplayName(""사용자 생성 - 성공"")
@ParameterizedTest(name = ""{displayName} {index}, args = {arguments}"")
@ValueSource(strings = {""ㅁㅇㄹㅁㅇㄹ"", ""ㅓㅓㅓㅓ"", ""adfdf"", ""검프"", ""검프23213""})
void save(String name) {
...
} 테스트 출력시 위와 같이 나오며, ""반복 테스트의 이름, 실행 번호, 파라미터""와 같은 형식으로 나오게 했어요. 정말 좋은데, 이러한 형식을 사용하기 위해 @ParameterizedTest를 사용하는 모든 곳에 위와 같은 옵션을 줘야해요. 귀찮지 않나? 맞아요. 테스트가 한 두개면 모르겠는데, 100개 이상인 반복 테스트에서 일일이 위와 같은 형식으로 작성하는 것은 문제가 있어요. 애노테이션 선언 (테스트 폴더에서) #test/java/com/cvi
package com.cvi;

import org.junit.jupiter.params.ParameterizedTest;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target({ ElementType.ANNOTATION_TYPE, ElementType.METHOD })
@Retention(RetentionPolicy.RUNTIME)
@ParameterizedTest(name = ""{displayName} {index}, args = {arguments}"")
public @interface CustomParameterizedTest {
} 커스텀한 ParameterizeTest를 만들었어요. 자세히 보면, 선언 위치는 애노테이션 타입과, 메소드에서, 실행시점은 런타임 시간에 작동하기로 선언하고, @ParameterizedTest와 옵션을 함께 정의했어요. 이게 되는 이유는 JUnit5에서 애노테이션을 조합하여 커스텀한 애노테이션을 만들 수 있게 지원하기 때문이에요. 여튼 이제, 반복 테스트 즉, @ParameterizedTest를 사용하는 곳에서는 동일한 출력을 보장할 수 있게 됐어요 😂 @DisplayName(""사용자 생성 - 성공"")
@CustomParameterizedTest
@ValueSource(strings = {""ㅁㅇㄹㅁㅇㄹ"", ""ㅓㅓㅓㅓ"", ""adfdf"", ""검프"", ""검프23213""})
void save(String name) {
...
}",BE
707,"DB(MySQL) Replication with Spring Data JPA 업데이트: November 11, 2021 On This Page Master DB 서버 생성 및 설정 EC2 이름 출력 설정 MySQL server 설치 root 계정 비밀번호 변경 문자 인코딩 utf8mb4 설정 & 테이블 명 대소문자 구분 X 설정 Ubuntu Port Redirect 설정 Database와 USER MySQL Replication용 USER 생성 Slave DB 서버 생성 및 설정 Master & Slave 스키마 동기화 Master-Slave 연결 실패 Spring Boot Datasource 설정 Properties 클래스 RoutingDataSource 클래스 DataSourceConfig 클래스 정리 참고자료 우테코 팀 프로젝트의 백엔드 성능 개선을 하고 있다. VUser = 500일 때 WAS를 5대까지 Scale-out을 하면서 59%의 요청 처리가 실패되던 것을 100%의 요청 처리 성공으로 성능 개선을 했다. 하지만, 95% 요청의 응답시간이 17.24s -> 29s로 속도가 느려졌다. ‘WAS가 1개일 때는 애플리케이션 단에서 트랜잭션 관리를 어느 정도 해 주는데, Scale-out을 하니 별도의 WAS 5개가 하나의 DB를 동시에 찔러서 과부하가 걸리나?’라는 생각이 들었다. 그래서 우테코 크루 현구막의 블로그 포스팅을 보며 DB Replication을 적용해보려 한다. 여기에서 DB Replication의 목적은 Scale-out을 통한 성능(응답 속도) 향상이다. Master DB 서버 생성 및 설정 AWS에서 Master DB를 위한 EC2를 새로 생성하겠다. (과정 설명은 생략한다.) EC2 이름 출력 설정 AWS에서 처음 EC2를 생성해 터미널로 접속하면 위와 같이 나온다. 내가 지금 어느 EC2 인스턴스에 접속해있는지 쉽게 구분하기 위해, IP 부분을 이름으로 바꿔보자. $ sudo vim ~/.bashrc
위의 명령어로 파일을 연다. USER=${설정할 이름}
PS1='[e[1;31m$USERe[0m][e[1;32m e[0m][e[1;33mue[0m@e[1;36mhe[0m w] 
$ [33[00m]'
위의 내용을 파일의 맨 아래에 붙여넣고 저장한 뒤, 홈 화면으로 나온다. 지금은 Mater DB 서버에 접속해 있기 때문에, ${설정할 이름}에 DB-MASTER이라고 입력하겠다. $ source ~/.bashrc
위의 명령어를 입력해, 변경된 설정을 반영한다. 그러면 위와 같이, DB-MASTER이라는 이름이 앞에 출력된다. MySQL server 설치 현재 EC2의 OS는 Ubuntu이다. Ubuntu에서 mysql-server를 설치하는 법을 소개한다. $ sudo apt update
먼저, 위의 명령어를 입력해 업데이트한다. $ sudo apt install mysql-server
그다음, 위의 명령어를 입력해 mysql-server를 설치한다. root 계정 비밀번호 변경 $ sudo mysql -u root -p
설치가 완료되었으면, 위의 명령어를 입력해 root 계정으로 접속한다. 그러면 비밀번호를 입력하라고 Enter password: 가 나올텐데, 맨 처음에는 바로 엔터를 치면 접속된다. 이제 root 계정의 비밀번호를 변경하자. mysql> ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '${새로운 비밀번호}';
위의 명령어를 입력해 root 계정의 비밀번호를 변경한다. 문자 인코딩 utf8mb4 설정 & 테이블 명 대소문자 구분 X 설정 Ubuntu home으로 나온다. $ sudo vim /etc/mysql/my.cnf
위의 명령어를 입력해 MySQL 설정 파일을 연다. [client]
default-character-set = utf8mb4

[mysql]
default-character-set = utf8mb4

[mysqld]
character-set-client-handshake = FALSE
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci

lower_case_table_names=1
파일의 맨 마지막에 위의 내용들을 붙여넣고, 저장한 후에 나온다. $ sudo service mysql restart
변경된 설정을 반영하기 위해, 위의 명령어를 입력해 MySQL을 재시작한다. $ sudo mysql -u root -p
잘 반영되었는지 확인하기 위해, 위의 명령어로 MySQL에 로그인한다. 비밀번호는 앞에서 변경한 비밀번호로 입력해야 한다. mysql> SHOW VARIABLES LIKE 'lower_case_table_names';
위의 명령어를 입력한다. 위와 같이 Value 값이 1이 나오면 테이블 명의 대소문자 구분을 하지 않는 설정이 잘 적용된 것이다. mysql> SHOW VARIABLES LIKE 'c%';
위의 명령어를 입력한다. 위처럼 utf8mb4, utf8, utf8mb4_unicode_ci가 나오면, 이모지도 인식할 수 있는 문자 설정이 잘 된 것이다. Ubuntu Port Redirect 설정 우테코 AWS 보안그룹에 3306은 Private IP 사이에서만 열려있다. Public으로 열려있는 것은 9000번 포트이다. 하지만, mysql-server는 3306으로 실행 중이다. 따라서 외부에서 9000번 포트로 접속 -> EC2에서 3306포트로 REDIRECT -> mysql-server 에 3306포트로 접속 위의 과정으로 mysql-server에 접속하려 한다. 이 설정은 EC2 Ubuntu에서 아래의 명령어를 입력하면 된다. sudo iptables -A PREROUTING -t nat -i eth0 -p tcp --dport ${외부에서 들어오는 포트 번호} -j REDIRECT --to-port ${REDIRECT 되어 바뀐 결과의 포트 번호}
즉, 실제 명령어는 아래와 같다. sudo iptables -A PREROUTING -t nat -i eth0 -p tcp --dport 9000 -j REDIRECT --to-port 3306
Database와 USER 이 글은 아래의 내용들을 전제로 하고있다. 기존에 사용중인 Database가 존재한다. Database에 SELECT, INSERT, UPDATE, DELETE 등 적절한 권한이 부여되어있는 USER 계정이 존재한다. MySQL Replication용 USER 생성 mysql> CREATE USER '${생성할 Replication용 USER 이름}'@'%' IDENTIFIED BY '${설정할 비밀번호}';
위의 명령어를 입력해, 새로운 Replication용 USER를 생성한다. mysql> GRANT REPLICATION SLAVE ON *.* TO '${새로 생성한 Replication용 USER 이름}'@'%' IDENTIFIED BY '${비밀번호}';
위의 명령어를 입력해, 새로 생성한 USER에게 모든 DB에 대한 Replication 권한을 부여한다. mysql> exit;
위의 명령어를 입력해, Ubuntu home으로 나온다. $ sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf
위의 명령어를 입력해 MySQL 설정 파일을 연다. 위와 같이 bind-address 부분을 127.0.0.1에서 0.0.0.0으로 변경해, Slave DB 서버와 WAS 서버로부터의 접근을 허용한다. 설정 파일의 아랫부분을 보면, 위 빨간 네모 부분이 주석처리 되어있을 것이다. 이 주석들을 해제한다. 파일의 변경사항을 저장하면서 밖으로 나온다. $ sudo service mysql restart
위의 명령어를 입력해, MySQL을 재시작한다. MySQL에 재접속한 뒤, mysql> SHOW MASTER STATUSG;
위의 명령어를 입력한다. 위와 같이 출력되면, 성공한 것이다. Slave DB 서버 생성 및 설정 아래의 과정들은 중복되므로 생략한다. EC2 생성 EC2 이름 표시 설정 mysql-server 설정 root 계정 비밀번호 변경과정 문자 인코딩 utf8mb4 설정 & 테이블 명 대소문자 구분 X 설정 mysql> CREATE USER '${생성할 USER 이름}'@'%' IDENTIFIED BY '${설정할 비밀번호}';
위의 명령어를 입력해, 새로운 USER를 생성한다. mysql> CREATE DATABASE ${생성할 DB 이름};
그리고, 위의 명령어를 입력해 새로운 DB를 생성한다. ${생성할 DB 이름}은 Master DB와 동일하게 생성한다. mysql> GRANT ALL PRIVILEGES ON ${새로 생성한 DB 이름}.* TO '${새로 생성한 USER 이름}'@'%';
위의 명령어를 입력해, 새로 생성한 USER에게 새로 생성한 DB에 대한 모든 권한을 부여한다. mysql> exit;
위의 명령어를 입력해, Ubuntu home으로 나온다. $ sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf
위의 명령어를 입력해 MySQL 설정 파일을 연다. 위와 같이 bind-address 부분을 127.0.0.1에서 0.0.0.0으로 변경해, Slave DB 서버와 WAS 서버로부터의 접근을 허용한다. 설정 파일의 아랫부분을 보면, 위 빨간 네모 부분이 주석처리 되어있을 것이다. 이 주석들을 해제한다. 위의 이미지에서 server-id 값을 Master DB에서 1로 설정한 것과 다르게 2로 설정했다. server_id 값은 복제 구성에 포함된 MySQL 서버들이 각자 고유하게 갖고있는 식별 값이다. 이 값은 바이너리 로그에 쌓이는 이벤트들의 이벤트가 최초로 발생한 MySQL이 어디인지 식별하기 위해 사용되고, 기본값은 모두 1이다. 만약 Master DB와 Slave DB의 server-id 값이 동일한 경우, Master DB에서 발생한 이벤트라도 해당 바이너리 로그에 적혀있는 server-id가 Slave DB와 같으므로, 해당 Slave DB는 자기 자신이 발생시킨 이벤트로 보고 동기화를 진행하지 않는다. 따라서, 복제 구성에 포함된 MySQL 서버들은 각자 고유한 server-id를 갖도록 설정해야 한다. 파일의 변경사항을 저장하면서 밖으로 나온다. $ sudo service mysql restart
위의 명령어를 입력해, MySQL을 재시작한다. Master & Slave 스키마 동기화 Replication 연결을 하려면, Master DB와 Slave DB의 스키마 및 내부 데이터가 모두 동일해야 한다. 이를 위해 Slave DB를 덤핑하자. 기존 운영 DB는 Slave DB였다. 즉, 기존의 DB 스키마와 데이터가 존재하는 DB는 Slave DB이다. Slave DB 서버에 접속하고, 다음 명령어를 입력한다. $ sudo mysqldump -u ${계정 이름} -p ${덤프할 DB 스키마 이름} > ${내보낼 덤프 파일명}.sql
ls 명령어를 입력해 잘 덤핑이 됐는지 확인하자. scp 명령어를 통해, 덤프 파일을 Slave DB에서 Master DB 서버로 보내자. Slave DB 서버에서 Master DB 서버로 덤프 파일을 보내려면 Slave DB 서버 -> 로컬 PC -> Master DB 서버 의 경로로 덤프 파일을 전송해야 한다. Public Key를 사용해 Slave DB 서버 EC2 -> Master DB 서버 EC2로 직접 보낼수도 있지만, 현재 우테코 AWS EC2 보안그룹에 22번 포트가 일부 막혀있어서, 로컬 PC를 통한 방법으로 진행하겠다. scp 명령어를 사용한다. 로컬 PC에서 터미널을 켜고, 아래의 명령어를 입력한다. $ scp -i ${AWS EC2 접속용 private 키페어 경로} ubuntu@${Slave DB 서버 퍼블릭 IP}:${Slave DB 상에서 덤프 파일 위치} ${로컬 PC에서 덤프 파일을 받을 위치}
덤프 파일을 로컬 PC에 받았으면, 아래의 명령어를 입력해 다시 Master DB로 전송한다. $ scp -i ${AWS EC2 접속용 private 키페어 경로} ${전송할 덤프 파일 경로} ubuntu@${Master DB 서버 퍼블릭 IP}:${Master DB 상에서 덤프 파일을 받을 위치}
Slave DB 서버에 접속한다. $ sudo mysql -u root -p ${덤프 파일을 적용할 DB 이름} < ${덤프 파일명}.sql
위의 명령어를 입력해 덤프 파일에 있는 데이터를 Master DB에 주입한다. 주입이 완료되었으면, DB 안에 접속해서 주입이 잘 됐는지 확인한다. 주입이 잘 됐으면, Slave DB에 접속한다. mysql> USE mysql;
위의 명령어를 입력해, mysql DB를 선택한다. mysql> CHANGE MASTER TO MASTER_HOST='${Master DB IP}', MASTER_PORT=${Master DB 포트번호}, MASTER_USER='${Master DB USER 이름}', MASTER_PASSWORD='${Master DB USER의 비밀번호}', MASTER_LOG_FILE='${Master DB의 바이너리 로그 파일명}', MASTER_LOG_POS=${POS 값};
MASTER_USER와 MASTER_PASSWORD는 앞서 Master DB에서 생성해둔 Replication 전용 계정, 비밀번호를 입력한다. MASTER_LOG_FILE은 앞서 MASTER DB에서 SHOW MASTER STATUS 명령어를 입력해 확인한 바이너리 로그 파일의 이름을 명시한다. (ex. mysql-bin.0.000001) MASTER_LOG_FILE은 앞서 MASTER DB에서 SHOW MASTER STATUS 명령어를 입력해 확인한 바이너리 로그 파일 위치를 명시한다. (ex. 154) 위의 설명에 따라 명령어를 입력한다. mysql> START SLAVE;
위의 명령어를 입력해 SLAVE를 시작시키고, mysql> SHOW SLAVE STATUSG;
위의 명령어를 입력해 상태를 확인한다. 위와 같이 출력되어야 성공한 것이다. 임의의 값을 Master DB에 INSERT 해서, 정말 SLAVE DB들에도 INSERT 되는지 테스트해 보자. Master-Slave 연결 실패 만약 Slave_IO_State, Slave_IO_Running, Slave_SQL_Running 등이 다르게 출력될 경우 연결에 실패한 것이다. Master 서버의 IP, 포트 번호, replication 용 계정, 비밀번호를 잘못 입력했을 가능성이 있다. 쉽고 빠르게 확인해볼 방법으로는 Slave 서버 터미널에서 아래 명령을 통해 replication 용 계정으로 직접 Master DB에 접속해보는 것이다. $ mysql -h ${Master DB 서버 IP} -u ${Replication 권한이 부여된 Master DB의 계정} -p
Enter password: ${비밀번호}
접속이 성공하는 경우, Slave DB와 Master DB의 스키마 구조가 다른 것이다. dump 적용이 잘 되었는지 다시 확인한다. 원인을 해결했다면, 아래의 과정을 따라 Slave DB에서 Master DB로 연결을 재시도한다. mysql> RESET SLAVE ALL;
위의 명령어를 입력해 모든 SLAVE를 리셋한다. mysql> SHOW SLAVE STATUSG;
모든 SLAVE들이 제대로 RESET 됐다면, 위의 명령어를 입력했을 때 Empty set (0.00 sec)가 출력되어야 한다. mysql> CHANGE MASTER TO MASTER_HOST='${Master DB IP}', MASTER_PORT=${Master DB 포트번호}, MASTER_USER='${Master DB USER 이름}', MASTER_PASSWORD='${Master DB USER의 비밀번호}', MASTER_LOG_FILE='${Master DB의 바이너리 로그 파일명}', MASTER_LOG_POS=${POS 값};
Master DB 정보를 다시 설정하고, mysql> START SLAVE;
SLAVE를 시작한다. Spring Boot Datasource 설정 기존 Spring Boot properties 파일에서 spring.datasource 에 관련 정보를 기입하면 자동으로 datasource가 등록되었다. 하지만 Replication을 이용하려면 여러 개의 datasource를 동시에 등록해야 하므로, 자동 등록 기능을 이용할 수 없다. 직접 각각의 datasource들을 적용해주어야 한다. 즉, properties 파일에 기재한 datasource 내용을 코드로 직접 읽어서 등록해야 한다. spring:
# 새로운 커스텀
# 어떤 양식을 이용하든지 본인 자유
datasource:
driver-class-name: com.mysql.cj.jdbc.Driver
url: jdbc:mysql://${Master DB 서버 IP}:${포트 번호}/${DB 이름}?serverTimezone=Asia/Seoul&characterEncoding=UTF-8
username: ${Master DB 계정 이름}
password: ${Master DB 계정 비밀번호}

slaves:
slave1:
name: slave1
driver-class-name: com.mysql.cj.jdbc.Driver
url: jdbc:mysql://${Slave DB 서버 IP}:${포트 번호}/${DB 이름}?serverTimezone=Asia/Seoul&characterEncoding=UTF-8
username: ${Slave DB 계정 이름}
password: ${Slave DB 계정 비밀번호}

# 여기부턴 JPA가 읽고 해석하므로 자유 아님
jpa:
hibernate:
ddl-auto: validate
generate-ddl: false
properties:
hibernate:
show_sql: false
format_sql: false
physical_naming_strategy: org.springframework.boot.orm.jpa.hibernate.SpringPhysicalNamingStrategy
jdbc:
lob:
non_contextual_creation: true
physical_naming_strategy의 값 SpringPhysicalNamingStrategy는 네이밍 전략 설정이다. 설정을 생략할 경우 테이블/칼럼 명이 자바에서 사용하는 camel case 그대로 사용된다. (datasource 자동 등록 시 얼마나 많은 기본설정들이 포함되어 있는지 알 수 있는 대목) non_contextual_creation 설정은 createClob() 메서드를 구현하지 않았다는 Hibernate의 에러 로그를 보여주지 않는 설정이다. Spring Boot properties 설정 작성이 완료되었다면, 임의로 작성한 DataSource를 @ConfigurationProperties 애노테이션을 통해 객체로 매핑해주자. Properties 클래스 @Setter
@Getter
@ConfigurationProperties(prefix = ""spring.datasource"")
public class ReplicationDataSourceProperties {

private String driverClassName;
private String url;
private String username;
private String password;
private final Map<String, Slave> slaves = new HashMap<>();

@Setter
@Getter
public static class Slave {

private String name;
private String driverClassName;
private String url;
private String username;
private String password;
}
}
필드 변수명 하나하나가 yml 파일에서 설정한 이름과 매칭되어야 함을 주의하자. @ConfigurationProperties 애노테이션을 처음 사용하면 에러를 마주할 수 있는데, 공식 문서를 확인해보면 의존성 추가를 통해 해결할 수 있음을 알려준다. dependencies {
annotationProcessor 'org.springframework.boot:spring-boot-configuration-processor'
}
다음은 @Transactional(readOnly=true) 분기 처리를 위한 로직을 작성해야 한다. 서비스 로직을 수행할 때 메서드에 붙은 애노테이션이 @Transactional(readOnly=true)인 경우 Slave Datasource로, 나머지는 Master Datasource로 분기 처리를 하기위한 RoutingDataSource 클래스를 생성한다. RoutingDataSource 클래스 @Slf4j
public class ReplicationRoutingDataSource extends AbstractRoutingDataSource {

private static final String DATASOURCE_KEY_MASTER = ""master"";
private static final String DATASOURCE_KEY_SLAVE = ""slave"";

private DataSourceNames<String> slaveNames;

@Override
public void setTargetDataSources(Map<Object, Object> targetDataSources) {
super.setTargetDataSources(targetDataSources);

List<String> replicas = targetDataSources.keySet()
.stream()
.map(Object::toString)
.filter(string -> string.contains(DATASOURCE_KEY_SLAVE))
.collect(toList());

this.slaveNames = new DataSourceNames<>(replicas);
}

// 요청에서 사용할 DataSource Key 값 반환
@Override
protected Object determineCurrentLookupKey() {
boolean isReadOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly();

if (isReadOnly) {
final String nextSlaveName = slaveNames.getNext();
log.info(""Connection Slave : {}"", nextSlaveName);
return nextSlaveName;
}

log.info(""Connection Master"");
return DATASOURCE_KEY_MASTER;
}

public static class DataSourceNames<T> {

private final List<T> values;
private int index = 0;

public DataSourceNames(List<T> values) {
this.values = values;
}

public T getNext() {
if (index + 1 >= values.size()) {
 index = -1;
}
return values.get(++index);
}
}
}
ReplicationRoutingDataSource 클래스는 AbstractRoutingDataSource를 상속하여 구현해야 한다. AbstractRoutingDataSource는 spring-jdbc 모듈에 포함되어 있는 클래스로, 복수의 datasource를 등록하고 상황에 맞게 원하는 datasource를 사용할 수 있도록 추상화한 클래스이다. determineCurrentLookupKey() 메서드 오버라이딩을 통해 현재 요청에서 필요한 Master/Slave 분기를 진행하고 사용할 datasource의 key 값을 반환해준다. 여러 개의 Slave 서버를 골고루 사용해서 부하를 분산시킬 수 있도록 원형 연결리스트 형태의 클래스도 사용한다. 이를 통해 요청마다 인덱스가 증가/감소하면서 모든 Slave 서버를 순회할 수 있게 된다. 다음은 실제 datasource를 스프링부트에 등록하기 위한 config 클래스를 생성한다. DataSourceConfig 클래스 @Profile({""local"", ""prod""})
// @EnableAutoConfiguration(exclude = DataSourceAutoConfiguration.class)
@EnableConfigurationProperties(ReplicationDataSourceProperties.class)
@Configuration
public class ReplicationDataSourceConfig {

private final ReplicationDataSourceProperties dataSourceProperties;
private final JpaProperties jpaProperties;

public ReplicationDataSourceConfig(ReplicationDataSourceProperties dataSourceProperties,
 JpaProperties jpaProperties) {
this.dataSourceProperties = dataSourceProperties;
this.jpaProperties = jpaProperties;
}

// DataSourceProperties 클래스를 통해 yml 파일에서 읽어 들인 DataSource 설정들을 실제 DataSource 객체로 생성 후 ReplicationRoutingDataSource에 등록
@Bean
public DataSource routingDataSource() {
DataSource masterDataSource = createDataSource(
dataSourceProperties.getDriverClassName(),
dataSourceProperties.getUrl(),
dataSourceProperties.getUsername(),
dataSourceProperties.getPassword()
);

Map<Object, Object> dataSources = new LinkedHashMap<>();
dataSources.put(""master"", masterDataSource);

for (Slave slave : dataSourceProperties.getSlaves().values()) {
DataSource slaveDatSource = createDataSource(
 slave.getDriverClassName(),
 slave.getUrl(),
 slave.getUsername(),
 slave.getPassword()
);

dataSources.put(slave.getName(), slaveDatSource);
}

ReplicationRoutingDataSource replicationRoutingDataSource = new ReplicationRoutingDataSource();
replicationRoutingDataSource.setDefaultTargetDataSource(masterDataSource);
replicationRoutingDataSource.setTargetDataSources(dataSources);

return replicationRoutingDataSource;
}

// DataSource 생성
public DataSource createDataSource(String driverClassName, String url, String username, String password) {
return DataSourceBuilder.create()
.type(HikariDataSource.class)
.url(url)
.driverClassName(driverClassName)
.username(username)
.password(password)
.build();
}

// 매 쿼리 수행마다 Connection 연결
@Bean
public DataSource dataSource() {
return new LazyConnectionDataSourceProxy(routingDataSource());
}

// JPA에서 사용하는 EntityManagerFactory 설정. hibernate 설정을 직접 주입한다.
@Bean
public LocalContainerEntityManagerFactoryBean entityManagerFactory() {
EntityManagerFactoryBuilder entityManagerFactoryBuilder = createEntityManagerFactoryBuilder(jpaProperties);
return entityManagerFactoryBuilder.dataSource(dataSource())
.packages(""{프로젝트 패키지 경로 ex) gg.babble.babble}"")
.build();
}

private EntityManagerFactoryBuilder createEntityManagerFactoryBuilder(JpaProperties jpaProperties) {
JpaVendorAdapter vendorAdapter = new HibernateJpaVendorAdapter();
return new EntityManagerFactoryBuilder(vendorAdapter, jpaProperties.getProperties(), null);
}

// JPA에서 사용할 TransactionManager 설정
@Bean
public PlatformTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) {
JpaTransactionManager tm = new JpaTransactionManager();
tm.setEntityManagerFactory(entityManagerFactory);
return tm;
}
}
가장 첫 줄을 보면 주석 처리가 되어있다. exclude 옵션을 이용해 DataSource 자동설정(DataSourceAutoConfiguration)을 제외시킬 수 있으나, Spring 공식문서에서 DataSourceAutoConfiguration에 대해 침투적이지 않다(Auto-configuration is non-invasive)고 평가한다. 때문에 굳이 제외시킬 필요 없다고 생각해서 포함하지 않았다. 위 코드에서 눈에 띄는 부분은 LazyConnectionDataSourceProxy()일 것이다. 우선 Spring은 트랜잭션에 진입하는 순간 이미 설정된 DataSource의 커넥션을 가져온다. TransactionManager가 트랜잭션을 식별하면 DataSource의 커넥션을 가져오고, 트랜잭션의 동기화가 시작되어버린다. 이럴 경우 다중 DataSource 환경에서는 DataSource를 선택하는 분기가 불가능하다. 따라서 미리 DataSource를 정하지 않도록 LazyConnectionDataSourceProxy를 사용하여 실제 쿼리가 실행될 때 Connection을 가져오도록 한 것이다. 설정을 모두 마쳤으면 애플리케이션을 실행해보자. 다른 이슈가 없다면 정상적으로 서비스가 진행되며 로거를 통해 Master/Slave 데이터베이스를 번갈아서 사용하고 있음을 확인할 수 있다. 정리 스프링 프로필 파일에 작성된 datasource 정보들을 DataSourceProperties 클래스를 통해 수동으로 매핑한다. isReadOnly 옵션값에 따라 Master/Slave DB 서버를 선택한다. 리스트를 순환하면서 Slave 서버를 선택하도록 해서, Slave 서버 부하를 분산시킨다. 매핑된 DataSource 설정들을 실제 DataSource 객체로 생성 후 ReplicationRoutingDataSource에 등록한다. JPA가 사용할 EntityManagerFactory를 수동으로 설정한다. JPA가 사용할 TransactionManager를 수동으로 설정한다. 서비스 로직 메서드마다 datasource가 바뀌어야하므로, LazyConnectionDataSourceProxy를 통해 proxy datasource를 연결하도록 설정한다. 참고자료 MySQL(MariaDB) Replication with JPA 태그: DB, MySQL, Replication, 성능 개선, 테스트 업데이트: November 11, 2021 이전 다음 댓글남기기",BE
708,"백엔드 성능 개선 리포트 (WAS Scale-out + DB Replication) 업데이트: November 11, 2021 On This Page 목푯값 설정 계산 예시 트래픽(일)이란? 일 트래픽 계산 방법 VUser 구하기 VUser가 100명, 500명일 때 각각 트래픽 계산 VUser = 100 VUser = 500 테스트 기록 Smoke Test 개선 전 : AWS ELB + WAS 총 1개 계획 결과 로드밸런싱 적용 (WAS 총 5개) DB Replication 적용 (Master 1대 / Slave 4대) 최종 결과 Load Test 개선 전 : AWS ELB + WAS 총 1개 계획 (VUser = 100 = 1일 사용자 약 32,000명) 결과 로드밸런싱 적용 (WAS 총 5개) DB Replication 적용 (Master 1대 / Slave 4대) 최종 결과 Stress Test 개선 전 : AWS ELB + WAS 총 1개 계획 (VUser = 500 = 1일 사용자 약 162,000명) 결과 로드밸런싱 적용 (WAS 총 5개) DB Replication 적용 (Master 1대 / Slave 4대) 최종 결과 최종 결과 요약 [WAS Scale-out(WAS 총 5개) + DB Replication(DB 총 4개)] 참고자료 SQL INSERT 반복문 목푯값 설정 계산 예시 우선 예상 1일 사용자 수(DAU)를 정해 본다. : 6,000명 피크 시간대의 집중률을 예상해 본다. (최대 트래픽 / 평소 트래픽) : 200GB / 22.5GB = 8.889 1명당 1일 평균 접속 혹은 요청 수를 예상해 본다. : 50번 이를 바탕으로 Throughput을 계산한다. Throughput : 1일 평균 rps ~ 1일 최대 rps 1일 사용자 수(DAU) x 1명당 1일 평균 접속 수 = 1일 총접속 수 6,000 x 50 = 300,000 1일 총접속 수 / 86,400 (초/일) = 1일 평균 rps 300,000 / 86,400 = 3.472 1일 평균 rps x (최대 트래픽 / 평소 트래픽) = 1일 최대 rps 3.472 x 8.889 = 30.863 Latency : 일반적으로 50 ~ 100ms 이하로 잡는 것이 좋다. 50ms 사용자가 검색하는 데이터의 양, 갱신하는 데이터의 양 등을 파악해 둔다. 트래픽(일)이란? 인터넷 사용자에게 사이트의 정보를 노출 시킬 수 있는 하루 동안의 용량을 말한다. 하루 동안 10M짜리 동영상이 10번 노출되면 트래픽(일)은 100M(10M*10) 일 트래픽 계산 방법 방문 수 * 평균 페이지뷰 * 1페이지당 평균 바이트 방문 수 : 하루 동안 사이트를 방문한 횟수 300,000회 1,000,000회 평균 페이지뷰 : 방문한 사람에게 노출되는 평균 페이지 수 50개 100개 1페이지당 평균 바이트 : 노출되는 페이지의 평균 용량 1.5KB 2KB VUser 구하기 Request Rate: measured by the number of requests per second (RPS) VU: the number of virtual users R: the number of requests per VU iteration : 12 T: a value larger than the time needed to complete a VU iteration : 1 T = (R * http_req_duration) (+ 1s) ; 내부망에서 테스트할 경우 예상 latency를 추가한다.

VUser = (목표 rps * T) / R
예를 들어, 두 개의 요청 (R=2)이 있고 왕복시간이 0.5s, 지연시간이 1초라고 가정할 때 (T=2) 계산식은 아래와 같다. ex) VU = (300 * 2) / 2 = 300 T = (12 * 1) = 12 평균 VU = (3.472 * 12) / 12 = 3.472 -> 4 최대 VU = (30 * 12) / 12 = 30.863 -> 31 VUser가 100명, 500명일 때 각각 트래픽 계산 R = 11 (아래의 smoke.js 파일 등에 있는 VU iteration 당 요청 수가 총 11개이다.) T = (11 * 1) = 11 VUser = (목표 rps * 11) / 11 VUser = 목표 rps = 1일 최대 rps VUser = 100 100 = 목표 rps 1일 평균 rps * 8.889 = 100 1일 평균 rps = 11.25 1일 총접속 수 / 86,400 (초/일) = 11.25 1일 총접속 수 = 972,000 1일 사용자 수 * 30 = 972,000 1일 사용자 수 = 32,400명 VUser = 500 500 = 목표 rps 1일 평균 rps * 8.889 = 500 1일 평균 rps = 56.249 1일 총접속 수 / 86,400 (초/일) = 56.249 1일 총접속 수 = 4,859,913 1일 사용자 수 * 30 = 4,859,913 1일 사용자 수 = 161,997명 테스트 기록 Thread Pool 설정 Nginx 설정 비동기 로깅 적용 Nginx -> ELB 교체 스케일 아웃, 로드밸런싱 : WAS를 5개로 늘림. DB Replication 적용 (Master 1대 / Slave 4대) Smoke Test 최소한의 부하로 구성된 테스트로, 테스트 시나리오에 오류가 없는지 확인할 수 있다. 최소 부하 상태에서 시스템에 오류가 발생하지 않는지 확인할 수 있다. VUser를 1 ~ 2로 구성하여 테스트한다. smoke.js import http from 'k6/http';
import {check, sleep} from 'k6';

export let options = {
stages: [
{duration: '1m', target: 1}, // ${duration} 동안 현재 사용자를 ${target}명으로 유지한다.
],
thresholds: {
http_req_duration: ['p(99)<50'], // 99% 개의 요청이 50ms 미만으로 완료되어야 한다.
},
};

function getPage(APIS) {
for (const api of APIS) {
const response = http.get(api);
check(response, {
'response code was 200': (response) => response.status == 200,
});
sleep(4);
}
}

export default function () {
// 메인 페이지 접속
// 인기글 조회
const GET_PAGE_APIS = [
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=20&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/12461',
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/18553',
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=20&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/10847'
];

getPage(GET_PAGE_APIS);
};
개선 전 : AWS ELB + WAS 총 1개 계획 30초 동안 VUser 0명 → 1명 증가. 1분 동안 VUser 1명 유지. 30초 동안 VUser 1명 → 0명 감소. 99%의 요청이 각각 50ms 미만으로 완료되어야 한다. 결과 모든 요청 성공. 95%의 요청 응답시간 173ms. 로드밸런싱 적용 (WAS 총 5개) 95%의 요청 응답시간 252ms로 응답 속도 느려짐. 추측 : 1개의 WAS일 때는 애플리케이션 단에서 트랜잭션 관리를 어느 정도 해 주는데, 로드밸런싱을 적용하니 별도의 WAS 5개가 하나의 DB를 동시에 찔러서 과부하가 걸리나? DB Replication 적용 (Master 1대 / Slave 4대) 95%의 요청 응답시간 176ms로 원상복구 됨. 최종 결과 성능 동일. Load Test 테스트 기간 일반적으로 Load Test는 보통 30분 ~ 2시간 사이로 권장한다. 부하가 주어진 상황에서 DB Failover, 배포 등 여러 상황을 부여하며 서비스의 성능을 확인한다. 애플리케이션 배포 및 인프라 변경(scale out, DB failover 등) 시에 성능 변화를 확인한다. 외부 요인(결제 등)에 따른 예외 상황을 확인한다. 말 그대로 시스템이 얼마만큼의 부하를 견뎌낼 수 있는가에 대한 테스트를 말한다. 보통 Ramp up 이라 하여 낮은 수준의 부하부터 높은 수준의 부하까지 예상 트래픽을 꾸준히 증가시키며 진행하는 테스트로, 한계점의 측정이 관건이며, 그 임계치를 높이는 것이 목적이라 할 수 있다. 일반적으로 “동시접속자 수” 와 그 정도의 부하에 대한 Response Time으로 테스트를 측정한다. 예를 들어 1분동 안 10만 명의 동시접속을 처리할 수 있는 시스템을 만들 수 있는지가 테스트의 주요 관건이 된다. load.js import http from 'k6/http';
import {check, sleep} from 'k6';

export let options = {
stages: [
{duration: '1m', target: 100}, // ${duration} 동안 현재 사용자를 ${target}명으로 유지한다.
],
thresholds: {
http_req_duration: ['p(99)<50'], // 99% 의 요청이 50ms 미만으로 완료되어야 한다.
},
};

function getPage(APIS) {
for (const api of APIS) {
const response = http.get(api);
check(response, {
'response code was 200': (response) => response.status == 200,
});
sleep(4);
}
}

export default function () {
// 메인 페이지 접속
// 인기글 조회
const GET_PAGE_APIS = [
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=20&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/12461',
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/18553',
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=20&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/10847'
];

getPage(GET_PAGE_APIS);
};
개선 전 : AWS ELB + WAS 총 1개 계획 (VUser = 100 = 1일 사용자 약 32,000명) 30초 동안 VUser 0명 → 100명 증가. 1분 동안 VUser 100명 유지. 30초 동안 VUser 100명 → 0명 감소. 99%의 요청이 각각 50ms 미만으로 완료되어야 한다. 결과 모든 요청 성공. 95%의 요청 응답시간 1.86s. 로드밸런싱 적용 (WAS 총 5개) 95%의 요청 응답시간 2.61s로 응답 속도 느려짐. 추측 : 1개의 WAS일 때는 애플리케이션 단에서 트랜잭션 관리를 어느 정도 해 주는데, 로드밸런싱을 적용하니 별도의 WAS 5개가 하나의 DB를 동시에 찔러서 과부하가 걸리나? DB Replication 적용 (Master 1대 / Slave 4대) 95%의 요청 응답시간 180.73ms로, Smoke Test만큼 빨라짐. 맨 처음보다 속도 약 10배배 빨라짐. 최종 결과 속도 약 10배 빨라짐. Stress Test 서비스가 극한의 상황에서 어떻게 동작하는지 확인한다. 장기간 부하 발생에 대한 한계치를 확인하고 기능이 정상 동작하는지 확인한다. 최대 사용자 또는 최대 처리량을 확인한다. 스트레스 테스트 이후 시스템이 수동 개입 없이 복구되는지 확인한다. stress.js import http from 'k6/http';
import {check, sleep} from 'k6';

export let options = {
stages: [
{duration: '1m', target: 500}, // ${duration} 동안 현재 사용자를 ${target}명으로 늘린다.
],
thresholds: {
http_req_duration: ['p(99)<50'], // 99% 의 요청이 50ms 미만으로 완료되어야 한다.
},
};

function getPage(APIS) {
for (const api of APIS) {
const response = http.get(api);
check(response, {
'response code was 200': (response) => response.status == 200,
});
sleep(4);
}
}

export default function () {
// 메인 페이지 접속
// 인기글 조회
const GET_PAGE_APIS = [
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=20&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/12461',
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/18553',
'{접속 URL}/api/v1/posts/paging?offset=0&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=10&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/paging?offset=20&size=10&sort=LIKE_COUNT_DESC',
'{접속 URL}/api/v1/posts/10847'
];

getPage(GET_PAGE_APIS);
};
개선 전 : AWS ELB + WAS 총 1개 계획 (VUser = 500 = 1일 사용자 약 162,000명) 30초 동안 VUser 0명 → 500명 증가. 1분 동안 VUser 500명 유지. 30초 동안 VUser 500명 → 0명 감소. 99% 의 요청이 각각 50ms 미만으로 완료되어야 한다. 결과 58%의 요청 처리 실패. 95%의 요청 응답시간 17.24s. 로드밸런싱 적용 (WAS 총 5개) 모든 요청 처리 성공. 95%의 요청 응답시간 29s로 응답 속도 느려짐. 추측 : 1개의 WAS일 때는 애플리케이션 단에서 트랜잭션 관리를 어느 정도 해 주는데, 로드밸런싱을 적용하니 별도의 WAS 5개가 하나의 DB를 동시에 찔러서, 과부하가 걸리나? DB Replication 적용 (Master 1대 / Slave 4대) 95%의 요청 응답시간 4.88s로 속도 빨라짐. 맨 처음보다 속도가 약 3.5배 빨라짐. 최종 결과 58%의 요청 처리 실패 -> 모든 요청 처리 성공. 속도 약 3.5배배 빨라짐. 최종 결과 요약 [WAS Scale-out(WAS 총 5개) + DB Replication(DB 총 4개)] Smoke Test 성능 동일 Load Test 속도 약 10배 빨라짐. Stress Test 58%의 요청 처리 실패 -> 모든 요청 처리 성공. 속도 약 3.5배 빨라짐. 참고자료 동시 초당 접속자 - RPS(Request Per Second)란? load testing 과 stress testing, performance testing 에 대한 비교 “Too many open files” Error는 어떻게? [Logging] Logback을 이용해 logging하기 SQL INSERT 반복문 drop procedure if exists myFunction;
DELIMITER $$
CREATE PROCEDURE myFunction() -- myFunction이라는 이름의 프로시져
BEGIN
DECLARE i INT DEFAULT 1; -- i변수 선언, defalt 값으로 1 설정

DECLARE min_user_id INT DEFAULT 8002;
DECLARE max_user_id INT DEFAULT 16002;

DECLARE min_post_id INT DEFAULT 10001;
DECLARE max_post_id INT DEFAULT 20011;

WHILE (i <= 48000) DO -- for문 작성(i가 48,000 이하인 동안 반복)

insert ignore
into
likes
(likes_id, created_at, last_modified_at, post_id, user_id)
values
(null, now(), now(), FLOOR(RAND() * (max_post_id - min_post_id + 1)) + min_post_id, FLOOR(RAND() * (max_user_id - min_user_id + 1)) + min_user_id);

SET i = i + 1; -- i에 1 더하고 WHILE문 처음으로 이동
END WHILE;
END$$
DELIMITER ; -- 구분 기호를 다시 ;로 바꿔주기
CALL myFunction;

-- FLOOR(RAND() * (max_post_id - min_post_id + 1)) + min_post_id
태그: DB Replication, k6, WAS Scale-out, 부하 테스트, 성능 개선 업데이트: November 11, 2021 이전 다음 댓글남기기",BE
709,"S3 vs S3 + CloudFront 이미지 조회 속도 비교 테스트 업데이트: November 10, 2021 On This Page CloudFront란? S3에서 이미지를 바로 조회할 수 있도록 설정 S3 버킷 생성 및 권한 설정 IAM 사용자 권한 생성 S3에 테스트용 이미지 업로드 CloudFront vs S3 이미지 조회 성능 비교 참고자료 현재 팀 프로젝트 서비스는 게시글에 이미지를 첨부할 수 있다. 우테코의 정책으로는 S3에 저장되어있는 객체는 CloudFront로만 조회할 수 있었다. 그래서 처음 인프라 설계 자체를 S3 + CloudFront 구성으로 했다. 그런데 문득, S3에 있는 이미지를 바로 조회할 때와, CloudFront를 통해 조회할 때 성능이 얼마나 차이 나는지 궁금해, 테스트해 보기로 했다. CloudFront란? CloudFront는 CDN이다. AWS S3에 저장되어 있는 이미지 등의 정적 파일을 엣지 로케이션이라고 하는 분산되어있는 데이터 센터에 캐싱해 둔다. 사용자가 요청을 보내면, 원본 이미지 파일이 있는 AWS S3 서버까지 도달할 필요 없이, 요청 위치부터 가장 가까운 엣지 로케이션에 있는 이미지 파일을 가져온다. 그러면, 이미지를 조회하는 데 걸리는 시간을 줄일 수 있다. S3에서 이미지를 바로 조회할 수 있도록 설정 우선, 지금 성능 개선을 위해 구성되어있는 인프라는 우테코에서 교육용으로 제공한 AWS 환경이다. 이 환경에서는 우테코 정책상 S3에서 바로 이미지를 조회할 수 없고, 무조건 CloudFront를 통해서만 조회할 수 있다. 따라서, 개인 계정을 사용해 테스트해야 한다. S3 버킷 생성 및 권한 설정 AWS에 개인 계정으로 로그인해서 S3 콘솔에 접속한 후, 버킷 만들기를 누른다. 버킷 이름을 입력한다. 모든 퍼블릭 액세스 차단의 체크박스를 해제한다. 그러면 맨 아래의 체크박스 부분이 생기는데, 체크한다. 맨 아래로 가서 버킷 만들기 클릭 외부에서 S3의 이미지를 읽어갈 수 있도록 오픈하고, 업로드는 AWS의 IAM 역할을 통해 제한을 둘 것이다. 방금 생성한 버킷 이름을 클릭하고, 권한 탭으로 들어간다. 버킷 정책 항목의 편집 버튼을 누른다. 버킷 정책 편집 항목에서 버킷 ARN 복사 정책 생성기 버튼 클릭 Add Statement 버튼을 누르면 아래와 같은 내용이 밑에 나온다. 입력한 내용들을 확인하고, Generate Policy 버튼을 클릭한다. 그러면 위와 같이 JSON 형태의 정책이 생성되는데, 모두 복사한다. S3 버킷 정책 편집 창으로 돌아와, 복사한 JSON 형태의 정책을 붙여넣고, 변경사항 저장을 누른다. IAM 사용자 권한 생성 AWS의 IAM 대시보드에 접속한다. 메뉴에서 사용자 탭을 클릭하고, 사용자 추가 버튼을 누른다. 사용자 이름은 아무거나 입력해도 된다. 꼭 해주지 않아도 되지만, 나는 태그에 Name 값을 입력했다. 내용을 검토하고, 사용자 만들기 버튼을 누른다. 위와 같이 엑세스 ID, 비밀 엑세스 키가 나온다. 이 값들은 이 창에서만 볼 수 있으니, 메모장 등에 적어놓자. 이 두 값이 있어야 S3에 객체를 업로드 하는 등의 접근을 할 수 있다. S3에 테스트용 이미지 업로드 CloudFront에 성능 테스트 및 개선을 위해 저장되어있는 이미지를 이 S3에 업로드하겠다. 앞에서 생성한 S3에 접속해, 업로드 버튼을 누른다. 업로드할 이미지를 드래그&드롭한 후, 업로드 버튼을 눌러 업로드한다. 퍼블릭 엑세스 차단을 풀어놓았기 때문에, https://taehee-kim-dev-s3-bucket.s3.ap-northeast-2.amazonaws.com/inbi-test-image.jpeg를 브라우저 주소창에 입력하면 이미지를 볼 수 있다. CloudFront vs S3 이미지 조회 성능 비교 이제, 같은 이미지를 CloudFront로 조회하는 것과, S3에서 조회하는 것의 속도를 비교해보자. 동일한 환경에서 진행하기 위해, DB에서 값을 수정하겠다. 첨부된 이미지가 2개 존재하는 게시물 중에, id가 19968인 게시물로 테스트를 해 보겠다. 두 이미지 중 한 이미지의 URL을 방금 S3에 업로드한 이미지의 URL(https://taehee-kim-dev-s3-bucket.s3.ap-northeast-2.amazonaws.com/inbi-test-image.jpeg)로 수정하겠다. Image 테이블의 모습이다. post_id 19968인 게시글에 이미지가 2개 있다. 한 이미지는 CloudFront를 통해 조회된다. (inbi-test-4mb-image.jpeg) 다른 한 이미지는 S3에서 조회된다. (inbi-test-image.jpeg) :bangbanb: 이미지의 이름은 용량 등과 아무 상관이 없으니 신경 쓰지 말자. 이제 id가 19968인 게시글의 이미지 로딩 시간을 크롬 개발자도구의 Network 탭을 통해 확인해 보자. 캐싱 되어있는 이미지를 가져오면 0ms가 나오니, 크롬 개발자도구를 켠 상태에서 캐시 비우기 및 강력 새로고침을 누르자. 네트워크 상황에 따라 결과는 매번 달라졌지만, 가장 평균적이라고 생각하는 결과를 캡처했다. 이미지 파일 이름 조회 방식 조회하는 데 걸린 시간 inbi-test-image.jpeg S3에서 바로 조회 97ms inbi-test-4mb-image.jpeg S3에 저장되어 있는 것을 CloudFront를 통해 조회 74ms 평균적으로 위와 같은 결과가 나왔다. 계산해 보면, 74ms / 97ms = 0.76 다. 즉, CloudFront를 통해 조회하는 것은 S3에서 조회하는 것보다 약 23% 정도 시간이 더 적게 걸린다. 참고자료 [Spring] Spring Boot AWS S3 사진 업로드 하는 법 태그: AWS, CloudFront, S3, 성능 개선, 이미지, 테스트 업데이트: November 10, 2021 이전 다음 댓글남기기",BE
765,"안녕하세요! 조엘입니다! ""처음이라니까요"" 시리즈 아홉 번째 토픽은 웹훅입니다. 🎡🎡 우테코 레벨3는 프로젝트 기획부터 개발까지 경험해보도록 구성이 되어있는데요! 이번 프로젝트를 경험하면서 웹훅이라는 것을 많이 썼어요. ""깃헙에서 PR이 머지되면 젠킨스가 돌아가도록 웹훅을 사용하자!"" ""중요한 에러가 터지면 슬랙에 알림을 주도록 웹훅을 사용하자!"" 팀원들과 함께 웹훅을 사용해봤지만, 사실 전 웹훅이 뭔지 몰랐답니다. 😅 이번 기회에 한 번 정리해보도록 할게요! *** 웹훅이란? *** 웹훅의 정의부터 알아봅시다. 위키피디아가 정의한 웹훅은 다음과 같아요. A webhook in web development is a method of augmenting or altering the behavior of a web page or web application with custom callbacks. These callbacks may be maintained, modified, and managed by third-party users and developers who may not necessarily be affiliated with the originating website or application. The term ""webhook"" was coined by Jeff Lindsay in 2007 from the computer programming term hook.[1] The format is usually JSON. The request is done as an HTTP POST request. 웹훅은 웹 개발 방법으로, 커스텀 콜백을 활용하여 웹 사이트의 행동을 보강하거나 변경하는 방법입니다. 이 콜백들은 웹 사이트와 직접적으로 관련이 없는 제 3자에 의해 유지되거나, 수정되거나, 관리될 수 있습니다. 요청은 HTTP POST 방식으로 이뤄지며, 주로 JSON을 활용합니다. 웹 사이트의 행동을 보강하거나 변경한다는데, 무엇이 부족해서 웹훅이 등장했을까요? 🤔 우리가 일반적으로 사용하는 웹 사이트의 어떤 문제를 해결하고자 등장했을까요? 🤔 *** 왜 필요하지? *** 웹훅은 웹앱에서 발생한 이벤트 정보를 ""실시간""으로 제공하기 위해서 필요해요. REST API로 구축된 웹 서비스의 경우, 하나의 요청에 따라 하나의 응답을 제공해요. 이러한 구조로 인해 피치 못하게 발생하는 문제들이 있었는데요. 어떠한 이벤트가 발생했는지를 조회하기 위해서는 서버로의 요청이 선행되어야 했던 것이죠. 예시를 한 번 들어볼게요. 서버에 예상치 못한 예외가 발생했을 때, 개발자인 제가 어떻게 이를 알 수 있을까요? 1번 방법으로, 예외 상황을 확인하는 백엔드 API를 구축하는 방법이 있어요. 해당 API로 요청을 보내 서버에 예상치 못한 예외가 발생했는지를 조회해볼 수 있어요. 하지만 요청을 보내야 응답을 받을 수 있다는 점이 단점이에요. 궁금할 때마다 서버에 요청을 보내야 하니 비효율적이고요. 2번 방법으로, 예외 상황이 발생하면 웹훅을 통해 알려주는 것이에요. 서버 측에서 직접 개발자가 소식을 받아볼 수 있는 엔드포인트로 요청을 보내는 것이죠. 개발자는 가만히 있다가 서버가 직접 요청을 보내오면 예외 상황을 확인하면 되는 것이에요. 1번 방법 Polling, 2번 방법 Webhook 서버가 요청을 보내는 클라이언트의 역할을 한다고 느껴지기도 하는데요! 그러다 보니 웹훅을 Reverse API, HTTP PUSH API, Web Callback 등으로 부르기도 해요. 많은 사람들이 사용하는 메신저 Slack에서 알맞은 엔드포인트에 요청이 들어오면 알림을 주도록 웹훅을 지원하는데요. 실습을 통해 알아보아요. 👏👏 *** Slack 웹훅 사용해보기 *** 검색 기능을 만들고 있는데, 사용자들이 어떠한 검색어를 입력했는지 실시간으로 받아보고 싶어요. 이를 Slack 웹훅을 사용해서 구현해보도록 할게요! 1. Slack에서 'Slack 찾아보기 > 앱' 선택 2. '내 도구와 연결' 창에서 'Incoming WebHooks' 검색 후 클릭 3. Slack 웹 페이지에서 'Incoming WebHooks' 앱 추가 승인 4. 웹훅 URL을 할당받기 (해당 URL이 외부에 공개되면 Slack 측에서 웹훅 URL을 변경합니다!) 5. 웹훅 URL에 어떠한 방식으로 요청을 보내야 하는지 살펴보기 (JSON 페이로드에 'text'를 담아 POST 방식 전송) 6. 백엔드 로직에 사용자 검색어를 담은 POST 요청을 보내도록 구현하기 @Service
public class WebhookService {
@Value(""${slack.url}"")
private String slackUrl;

private final RestTemplate restTemplate = new RestTemplate();

public void search(String message) {
sendSlackMessage(message);
}

private void sendSlackMessage(String message) {
Map<String, String> body = new HashMap<>();
body.put(""text"", message + "" 검색되었습니다."");
restTemplate.postForObject(slackUrl, body, String.class);
}
} 다음과 같은 스텝을 거치면, Slack 웹훅을 활용한 검색 기록 알리미가 완성됩니다! 참고 - https://leffept.tistory.com/329 - https://sddev.tistory.com/9 - https://simsimjae.medium.com/%EC%9B%B9%ED%9B%85%EC%9D%B4%EB%9E%80-e41cf1ba92f0",NON_TECH
768,"놀토에서는 브랜치 전략으로 git-flow를 사용하고 있는데 메인 서비스용 서버(product server)에 배포시 가끔 혼란이 와서 문서화의 필요성을 느꼈습니다. 우선 놀토의 브랜치의 종류는 다음과 같습니다. main : product 서버 용 코드가 있는 브랜치 develop : 개발용 서버의 코드가 있는 브랜치. 새로운 기능이나 버그 수정등은 이곳에 모입니다. release : product 서버에 배포하기 전 QA를 진행하기 위해 사용하는 브랜치 hot-fix : main 서버에서 발생한 버그를 처리하기 위한 브랜치 feature/backend/작업명 : 기능을 개발하기 위한 브랜치 평소에 main 브랜치에 머지하기 위한 과정은 다음과 같았습니다. develop 브랜치로부터 release 브랜치를 생성한다. release 브랜치에서 버전 tag를 생성하고 main 브랜치에 머지합니다. (이때 버전 tag는 develop 브랜치에도 머지해줘야 합니다.) main 브랜치에서 QA를 진행하고 버그는 develop 브랜치에서 bug-fix 브랜치를 생성합니다. 버그 수정이 완료되면 develop 브랜치에서 main 브랜치로 merge 시킵니다. 현재 release 브랜치를 적극적으로 사용하지 않고 진행하는 데는 몇가지 이유가 있습니다. release 브랜치를 사용해서 QA를 진행하기 위해 release 용 서버를 새로 만들어야 한다. release 브랜치가 아니라도 develop 과 main 브랜치도 충분히 QA를 진행할 수 있지 않은가? 물론 main 에 배포를 한 상태에서 QA를 진행하는 것은 문제가 있다고 생각됩니다. 현재는 작업하는 인원이 많은것도 아니며 QA를 거창하게 진행하기엔 필요없는 리소스가 발생한다고 생각합니다. 따라서 QA는 develop 브랜치와 서버에서 충분히 진행하고 main 브랜치에 머지하도록 합니다. 그럼 git 명령어를 명시하면서 위의 과정을 정리해보겠습니다. 저장소 이름 upstream : 최신의 소스코드가 저장되는 저장소 origin : 개인이 upstream 저장소를 fork 한 저장소 local : 개인 로컬에 있는 저장소 우선 로컬의 develop 브랜치에서 시작합니다. 로컬의 develop 브랜치에서 upstream의 develop 브랜치를 pull --rebase해서 최신화를 합니다. $(develop) > git pull --rebase upstream develop develop 브랜치로부터 release 브랜치를 생성합니다. $(develop) > git checkout -b release-0.0.0v release 브랜치에 태그를 추가합니다. $(release-0.0.0v) > git tag [태그이름] develop 브랜치에서 release 브랜치를 머지합니다. $(release-0.0.0v) > git checkout develop $(develop) > git merge --no-ff release-0.0.0v $(develop) > git push upstream develop main 브랜치에서도 release 브랜치를 머지합니다. $(release-0.0.0v) > git checkout main $(main) > git merge --no-ff release-0.0.0v $(main) > git push upstream main 여기서 upstream에 직접 push하는 방법과 release 브랜치를 upstream에 올려서 pr 생성 후 merge 하는 방법도 있습니다. git-flow 전략을 진행하면서 한가지 헷갈렸던 부분을 말씀드리면 main 브랜치는 develop 브랜치를 Fast-Forward 관계여야 하는가? 즉, main 브랜치에 develop 브랜치를 병합하는 과정에 conflict나 merge commit 이 발생하는 것이 어색한 것인가? 입니다. 답은 아니라고 생각합니다. 두 브랜치의 commit 이력 상태를 완벽하게 일치시킨다는 것은 굉장히 어렵고 번거로운 작업이 될것입니다. 그래서 코드를 같게 유지하는 것이지 커밋 이력까지 같게하는것은 현재는 힘들다 라고 생각합니다.",BE
771,"먼저 이 글에서 main 브랜치는 master 브랜치를 의미합니다! 'main 브랜치 == master 브랜치 ' 로 생각하고 글을 읽어주세요 :) Git-flow Git-flow는 빈센트 드리센(Bincent Driessen)이 2010년에 제안한 브랜치 전략 모델입니다. 우선 알아야 할 것은 빈센트 드리센도 2020년 3월 5일에 2010년에 작성한 글에 코멘트를 작성했습니다. 내용을 요약하면 10년이 지난 모델이고 웹 애플리케이션 같은 경우는 여러 버전의 소프트웨어로 관리될 필요가 없고 지속적으로 제공해도 문제가 되지 않으므로 복잡한 모델인 Git-flow 모델보다 Github-flow 같은 간단한 브랜치 전략 모델을 사용할 것을 추천하고 있습니다. Git-flow 에 대한 내용은 아래부터 시작하겠습니다. 우선은 간단한 용어부터 보고 넘어가겠습니다. upstream Repository / origin Repository / local Repository 저장소 (Repository) upstream : 최신의 소스 코드가 저장되어 있는 Repository origin : upstream 을 Fork 한 원격 개인 저장소 local : 내 컴퓨터에 저장되어 있는 저장소 브랜치 (Branch) Main - 최종 배포 브랜치 Release - 배포버전 QA를 진행하고 발생하는 버그를 처리하는 브랜치 Develop - 개발한 기능들이 모이는 main line 브랜치 Feature - 기능을 개발하는 브랜치 Hot-Fix - 배포한 버전에서 발생한 버그를 수정 Git-flow 모델의 간략한 흐름 태초에 main 과 develop 브랜치가 있었더라 develop 브랜치는 main 브랜치에서 탄생한 브랜치 develop 브랜치에는 상시로 버그를 수정한 커밋들이 추가될 수 있슴당 새로운 추가 작업이 있는 경우 develop 브랜치에서 feature/xxxxx 브랜치를 생성합니다. 기능 추가 작업이 완료됐다면 feature 브랜치는 develop 브랜치로 merge 합니다. develop 에서 이번 버전에 포함할 기능이 모두 merge 됐다면 QA로 가기위해 develop 브랜치로부터 release 브랜치를 생성합니다. QA를 진행하면서 발생한 버그는 release 브랜치에서 수정합니다. (QA에서 발견한 버그를 해결한 commit 은 release 브랜치에 추가) QA를 무사히 마쳤다면 release 브랜치를 main브랜치와 develop브랜치에 merge 시킵니다. 출시된 main 브랜치에 태그를 추가합니다. 이슈(jira 에서는 티켓) 처리하기 '로그인 레이아웃 생성' 이라는 이슈를 처리하려는 상황입니다. 하나의 이슈는 하나의 커밋으로 처리 라는 말은 기능을 구현하기 전에 작업을 여러개의 이슈로 나누는 것을 의미 (기능을 여러개의 이슈로 쪼개서 최대한 작은 작업 단위로 만들자) feature-user 브랜치에서 작업 브랜치 bfm-100_login_layout 를 생성 (feature-user)]$ git checkout -b bfm-100_login_layout --track upstream/feature-user (feature-user)]$ git fetch upstream bfm-100_login_layout 브랜치에서 작업 ⚒️ 작업 내용을 커밋 만약 커밋이 불필요하게 여러개로 나눠졌다면 squash 를 진행 커밋을 왜 하나로 만드나요? 꼭 하나일 필요는 없지만 여러개의 커밋이 나눠질 필요가 없거나 코드리뷰에 나눠진 커밋이 큰 도움이 되지 못할 때 커밋은 굳이 나누지 않고 하나로 합치는 것이 좋습니다. 하나의 이슈에 대한 작업이라도 커밋이 분리되어 있는것이 좋다고 생각하면 2개의 커밋으로 놔둬도 괜찮음 (bfm-100_login_layout)]$ git rebase -i HEAD~2 bfm-100_login_layout 브랜치의 작업 내용을 feature-user 브랜치에 pull —rebase 진행 rebase 를 진행하는 이유 커밋을 순차적으로 만들기 위한 작업 - rebase (bfm-100_login_layout)]$ git pull --rebase upstream feature-user Develop 브랜치에 새로운 기능에 대한 브랜치(Feature)를 합치기 전에 Develop 의 변경사항을 Feature 브랜치에 가져오기 feature-user 브랜치에 upstream/develop 브랜치를 merge 합니다. (feature-user)]$ git fetch upstream (feature-user)]$ git merge –no-ff upstream/develop upstream/develop의 변경사항이 merge된 feature-user를 upstream에 push 합니다. (feature-user)]$ git push upstream feature-user 배포하기 release 브랜치를 최신으로 갱신 (upstream에서 local의 release 브랜치로 push) release 브랜치를 develop 브랜치로 merge하고 upstream의 develop 브랜치로 push release 브랜치를 main 브랜치로 merge하고 upstream의 main 브랜치로 push main 브랜치에 태그를 추가 Github-flow Git-flow는 Github 에서 사용하기에 복잡하다고 생각하여 나온 브랜치 전략이 Github-flow 입니다. main 브랜치에만 엄격한 role을 적용합니다.(""pull request 를 날려서 완벽한 검증을 거쳐야 한다""와 같은 role을 의미) main 브랜치는 어떤때든 Product로써 배포가 가능한 상태여야 한다. (stable 한 상태여야 한다.) main 브랜치에만 엄격한 role 을 적용합니다. (이외의 브랜치는 자유롭게 사용) main 에서 새로운 작업을 시작하거나 버그를 수정한다면 브랜치 이름에 어떤 일을 하는지 명확하게 작성합니다.(ex. user-login) Github-flow 에서는 develop, feature 브랜치가 존재하지 않습니다. 커밋 메세지는 명확하게 작성해야 합니다. main 브랜치에 merge 하기 전에 충분히 테스트해야 합니다. 원격지 브랜치에 수시로 Push 하여 팀원들이 나의 작업을 확인할 수 있도록 해야합니다. 이는 하드웨어에 문제가 생겨 작업한 코드가 소실되도 원상 복구해서 작업할 수 있도록 도와줍니다. 도움이 필요하거나 작업이 완료되면 pull request 를 작성하고 리뷰를 받습니다. main 브랜치로 merge가 되면 자동으로 배포되도록 한다. 과정 요약 1. main 브랜치에서 새로운 기능이나 버그에대한 브랜치 생성(브랜치 이름에 작업내용, 버그내용 명확하게 명시) 2. 도움이 필요하거나 작업을 마치면 main 브랜치로 pull request 생성 3. 코드 리뷰, 피드백, 테스트 진행 4. CI까지 통과하면 main 브랜치에 머지시킨다. 5. main 브랜치는 merge 됐을 때 자동으로 배포되도록 한다. 놀토에서 사용하는 브랜치 전략 Repository(저장소) upstream(최신의 소스코드가 있는 원격 저장소) origin(개인 원격 저장소) local(로컬 저장소) Branch(브랜치) main : Product 로 배포되는 브랜치 release : 다음 배포버전 QA를 진행하고 QA 중 발생하는 버그를 처리하는 브랜치 (아직 release 브랜치의 존재 가치를 느끼지 못함) develop : 개발한 기능들이 모이는 main line 브랜치 feature : 기능을 개발하는 브랜치 hot-fix : 배포한 버전에서 발생한 버그를 수정 정해놓은 규칙 새로운 기능은 develop 브랜치로 부터 feature 브랜치를 만듭니다. feature 브랜치를 만들기 전 develop 브랜치를 rebase 해서 최신의 상태로 만듭니다. 프론트엔드는 feature/frontend/[작업이름] , 백엔드는 feature/backend/[작업이름] 으로 생성합니다. 기능 개발이 끝난 feature 브랜치는 origin 저장소로 push 합니다. (이때도 upstream의 develop 브랜치에 변경이 있을 수 있으므로 pull rebase 또는 pull 을 사용해서 코드를 가져옵니다. 코드 충돌이 있다면 해결합니다. ~feature/backend/xxxx$ git push origin feature/backend/xxxx origin에 올라간 feature 브랜치를 upstream의 develop 브랜치로 pull request 를 작성합니다. pull request 에 대해 코드 리뷰를 요청합니다. 코드 리뷰가 완료되면 github 의 Squash and merge 기능을 사용해서 merge를 진행합니다. 새로운 버전 배포시에는 develop 에서 release-v0.0.0같은 브랜치를 생성 release 에서 태그 작업, QA후에 release 브랜치에서 main 브랜치에 pull request작성하고 merge main으로 merge 시킬때는 github의 create a merge commit 기능 사용 main 에서 최신화된 내용을 develop 에도 적용",BE
772,"우테코 레벨3에서는 팀 프로젝트를 진행했습니다. 제가 참여한 프로젝트는 ""토이프로젝트 커뮤니티 서비스""였습니다. 희망하는 도메인에 1지망 부터 12지망까지 투표하는 방식이었는데 운좋게 참여하고 싶었던 도메인에 당첨이 됐습니다. (원래는 크루원 전체가 각자 1개의 서비스를 기획해서 발표를 했는데 그 중 인기있는 12개의 서비스가 뽑혔던 것입니다.) 이 글은 ""토이프로젝트 커뮤니티 서비스""를 개발하고 있는 놀토(놀러오세요 토이프로젝트)팀에서 협업중에 생각했던 것을 정리한 글입니다. 참 좋은건데 설명할 방법이 없네.. 놀토팀은 백엔드 4인으로 함께모여서 진행을 할 때가 많았습니다. 이유는 전원이 처음해보는 기술에 대해서는 학습한 부분을 공유하거나 실제 코드에 반영하는 과정을 보면서 학습하기 위해서 였습니다. 4인으로 개발을 진행하던중 조회 쿼리에 튜닝이 필요한 부분이 있었습니다. 이 때 JPQL로 진행할 지 Querydsl을 적용할지 또는 전체 데이터 조회 후에 도메인 로직으로 처리할지 토론을 했습니다. 저는 Querydsl을 도입하고 싶었는데 제 자신도 당장 Querydsl에 대한 지식이 없었습니다. (JPQL도 마찬가지였습니다..) 단순히 Querydsl이 좋으니 도입을 해보자! 라고 하기엔 너무나 지식이 부족했습니다. 하지만 저를 포함한 3명의 크루는 Querydsl을 도입해보고 싶었습니다. 이때 나머지 한명의 크루는 이런 기술들에 동의할 수 없는 상태였습니다. 'JPA를 사용하는 이유에는 도메인 로직을 사용해서 좀 더 객체지향적으로 개발하는데 있지않은가요?' 라는 의견이었습니다. 이때 저는 동의하지않는 의견에 반박할 수 없었습니다. 당장 설득할 말이 떠오르지 않았기 때문입니다. 우선 당시에는 한두개의 조회 쿼리를 위해서 Querydsl은 너무 과하다 라는 결론을 내렸습니다.(당장 인터페이스를 만들고 구현체를 만드는것은 과하다! 라는 의견) 그래서 JPQL을 도입하도록 했습니다. 동의하지 않던 크루는 JPQL에 대해 타협은 했지만 여전히 완벽한 동의는 할 수 없는 상태였습니다. (JPQL도 결국 쿼리를 이용한 조회이기 때문에 데이터베이스 의존적이다 라고 생각했는데 저도 여기에 어느정도 동의하고 있었습니다.) 이때 들었던 생각이 전원이 동의하지 않는 기술에 대해 설득할 수 있는 충분한 지식이 없다면 도입하는것은 좋지 않다고 생각했습니다. 우리 서비스(또는 로직)에 어떤 도움을 줄 수 있는지 설득할 수 있을 만큼 충분한 사전 지식이 필요했습니다. 저는 JPQL을 왜 사용하는지에 대해 학습을 시작했고 결론을 내린것은 다음과 같습니다. 데이터는 데이터베이스에 있는데 이것을 도메인 로직으로 처리하기 위해서 데이터베이스의 데이터를 전부 메모리로 올려야 합니다. 데이터베이스의 메모리에 애플리케이션 메모리까지 소모되는 불필요한 로직이라고 할 수 있습니다. 마치 일급 컬렉션을 사용하는데 컬렉션 내부의 데이터를 굳이 밖으로 꺼내와서 컬렉션에 대한 로직을 실행하는것이라고 생각했습니다. 우리가 일급 컬렉션 내부에서 컬렉션에 대한 로직을 처리하는 것처럼 데이터베이스에서 필요한 조회 쿼리를 사용해서 정제된 데이터를 가져오는것이 훨씬 효율적이라서 JPQL을 사용한다고 생각했습니다. 이렇게 결론 내린것을 전원에게 공유했고 전원이 JPQL에 동의할 수 있게 됐습니다. 참 좋은것이면 설명할 줄 알아야한다 아무리 좋은것이라도 충분한 설명이 따라오지 않는다면 그것은 믿을 수가 없습니다. ""이 약을 먹으면 건강에 참 좋은데 설명할 수가 없네요"" 라고 하는것은 약을 판매하는데 큰 도움이 되지 않습니다. ""이 약을 먹으면 호르몬 작용을 해서 탈모가 낫게됩니다."" 라고하면 위의 약보다 훨씬 잘 팔릴것입니다. 개발에서 협업도 마찬가지라고 생각합니다. 좋은 기술이라면 왜 좋은지, 우리 프로젝트에 적용하면 어떤점이 좋은지 설명할 수 있어야 합니다. 비지니스 임팩트까지 설명해준다면 더욱 좋겠지요! 물론 이렇게 설명하는 이유는 모든 팀원이 기술에 동의하고 그 기술에 열정적으로 참여하게 유도할 수 있다는 점이라고 생각합니다. 협업에서 동의하지 않는 사람의 의견을 버리고 간다면 그 사람을 버리는 것과 같다고 생각합니다. 물론 충분한 설명이 뒷받침 되고 비지니스 임팩트까지 분명한데 반대를 하는것은 좀 더 생각을 해봐야겠습니다!!",NON_TECH
773,"송파구에서 일을 '더' 잘하는 11가지 방법 중에서""잡담을 많이 나누는 것이 경쟁력이다""라는 3번 항목이 있습니다. 처음 이 항목에 대해 이해한것은 '지식에 대한 잡담으로 지식을 키우자!' 였습니다. 하지만 유튜브에서 '우아한형제들' 김범준 대표님의 배달의민족 CEO에게 뽑고 싶은 개발자를 물어보았다 라는 영상에서 3번 항목의 취지를 완벽하게 이해했습니다. 평소 친분을 쌓아서 인간대 인간으로 개인의 친숙함이 있는 상황이면 의견에 반대하거나 누군가는 이상하게 생각할 수 있는 의견도 자신있게 낼 수 있는 분위기가 만들어집니다. 영상에는 다음과 같은 예시가 나옵니다. 엘리베이터 사용자가 엘리베이터가 느리다고 항의를 했습니다. 어떤 사람은 100만원을 사용해서 엘리베이터의 성능을 향상시켜야 한다고 할 수 있습니다. 어떤 사람은 사람들이 엘리베이터가 느리다고 느끼는게 문제니까 엘리베이터 앞에 거울을 설치해서 사람들이 지루하지 않게 해주자 라고 합니다. 위의 예시에서 거울을 설치하자는 의견에 누군가는 '무슨소리야? 엘리베이터가 느리다니까?' 라고 할 수 있습니다. 하지만 이런 이야기를 자유롭게 얘기할 수 있는 분위기를 형성하는 것이 훨씬 팀의 발전에 좋다고 생각합니다. 이전에 백기선님의 마이크로 소프트의 개발문화에서 이런 항목을 들었습니다. '사소한 의견이라도 말하지않으면 팀에게 손해라는 생각을 가질 수 있도록 분위기를 형성해라. 실수는 누구나 할 수 있는것이고 팀은 실수를 받아들일 수 있어야합니다. 그렇게해야 자신이 가지고 있는 사소한 생각도 공개적으로 말할 수 있는 분위기가 생깁니다.' 저는 이 항목에 너무 공감하고 있습니다. 개발자에게 문제를 해결하는 방법은 항상 여러가지가 있다고 생각합니다. 그 중 어떤 사람은 큰 비용으로 문제를 해결하는 의견을 가지고 있을것이고 어떤 사람은 작은 비용으로 문제를 해결하는 의견을 가지고 있을 것입니다. 때문에 팀에서 활발하게 의견을 제출하는 것은 문제해결을 여러 측면에서 바라볼 수 있게하고 때로는 다수의 의견을 통합시켜 멋진 방법을 찾아내기도 합니다. 이번 레벨3의 놀토(놀러오세요 토이프로젝트)팀에서 구성원은 이미 친분이 쌓여있는 팀원들이었습니다. 바로 백엔드 구성원 4명은 레벨1에서 같은 데일리조로 활동을 했었습니다. (데일리조는 아침 10시에 함께 모여서 한가지 주제에 대해 얘기를 하던지, 게임을 하던지 뭔가의 활동을 함께 진행하는 조입니다.) 이미 백엔드 크루들은 친분이 쌓여있었기에 프론트엔드 크루들과 금방 친분이 쌓였습니다. 이런 분위기가 형성돼가니 의견을 내는데 주저함이 없어지고 반대의견을 내는데 큰 어려움이 없었다고 생각합니다. 의견이 부딪히는데는 사람과 사람으로 부딪히는게 아니라 의견대 의견으로 부딪혀야 한다는 말이 있습니다. 저도 가끔은 사람과 사람으로 부딪히자는건가? 라고 생각할 때가 있습니다.. 이 부분에 대해서는 많은 연습이 필요하다고 생각해요! 하지만 다른사람의 의견을 단순히 반대의견이라고 생각하고 상대방의 의견에서 바라보면 시야가 더 넓어지는 경험을 합니다. 예를 들어서 저는 통합테스트의 범위를 직접적인 연관관계가 맺어진 부분들만 테스트한다고 생각하고 있을 때 다른 크루는 간접적인 관계까지 묶어서 통합테스트를 진행했습니다. 내 의견이 맞는것 같은데 라고 생각하다가도 단순히 반대의견에 서서 생각을 해보면 좀 더 넓은 시야에서 테스트를 바라볼 수 있었습니다. 팀프로젝트를 어느정도 진행한 지금 협업에서 큰 비중을 차지하는 것은 팀원간의 친분이라고 생각합니다. 친분이 높을 수록 아래의 것들이 쉽게 이뤄질 수 있습니다. 다른사람의 의견에 반대 의견 내기 누군가는 엉뚱하게 생각할지도 모르겠는걸? 이라는 의견 내기 어떤 의견도 무시하지 않는 분위기 서로를 신뢰하는 분위기 반대되는 의견도 합의점이 있을거라 믿고 커뮤니케이션에 임하는 분위기",NON_TECH
775,"현재 놀토(Nolto) 서비스에서는 데이터베이스를 EC2 인스턴스의 Docker Container에 MariaDB를 띄워서 사용하고 있습니다. 하지만 Docker Container에 데이터베이스를 올리는것은 좋은 행위가 아니라고 합니다. 관련 자료들 - https://vsupalov.com/database-in-docker/ - https://patrobinson.github.io/2016/11/07/thou-shalt-not-run-a-database-inside-a-container/ 먼저 데이터베이스를 Docker에 올려서 얻을 수 있는 장점은 어떤것이 있을까요? - 손쉽게 버전을 맞춰줄 수 있습니다. - 환경 설정을 맞춰줄 수 있습니다. - 음.. 그럼 단점은 어떤것이 있을까요? - 데이터베이스의 데이터는 매우매우매우 중요한 자원입니다. Docker에 문제가 생겨서 컨테이너가 사라져버린다면 데이터도 함께 사라지는것을 의미합니다. - Docker의 볼륨 마운팅으로 위의 단점은 극복할 수 있지만 볼륨 마운팅 기술은 불완전하여 어떤 문제가 발생할지 알 수 없습니다. 문제가 생길 여지가 있는곳에 매우매우매우 중요한 자원인 데이터베이스의 데이터를 저장하는것은 위험합니다. - 즉 Docker 컨테이너에 데이터베이스를 띄우는것은 관리 포인트를 증가시키는 것이고 관리하지 않아도 될 리스크가 하나 생기는 것입니다. 이제 도커에 있는 mariaDB를 도커의 바깥인 인스턴스 자체에 설치를 할것입니다. 그러기 위해서는 먼저 도커 내부에 있는 데이터베이스의 데이터를 백업해둬야 합니다. 아래의 dump파일 생성 명령어를 docker container 내부의 CLI 창에서 실행해주세요. (다양한 명령어 옵션은 MariaDB 사이트의 mysqldump 문서를 참고해주세요!) mysqldump -u[username] -p[password] [Option] > [dump파일명].sql 백업 파일이 완성됐습니다. 이제 도커 컨테이너 내부에 있는 dump파일을 인스턴스 내부로 가져와야합니다. 아래 명령어로 가져올 수 있습니다. docker cp [컨테이너 이름]:[파일 경로] [호스트 경로] 이제 dump파일이 준비됐습니다. 이제 MariaDB만 설치하면 간단하게 끝이날거라 생각했지만 버전 문제가 발생했습니다. 우선 EC2 인스턴스는 OS로 Ubuntu 18.04 를 사용하고 있는데 apt-get을 활용해서 mariaDB 설치시 10.1.xx 버전을 설치하고 있었습니다. 하지만 Docker 컨테이너 내부의 MariaDB는 lastest 버전을 설치했는데 이는 10.5.xx 버전이었습니다. 10.5.xx 버전의 dump 파일로 10.1.xx버전에 데이터를 복구하려고 했더니 인덱스 키값이 너무 길다는 에러와 함께 복구가 실행되지 않았습니다. 그래서 apt-get으로 mariaDB 10.5버전을 설치합니다. 설치는 이곳의 글을 참고했습니다. mariaDB 10.5 버전이 있는 저장소를 apt에 추가하는 방법입니다. (18.04 버전은 반드시 add-apt-repository 저장소를 추가할 때 focal 대신 bionic 을 사용하셔야 합니다. sudo apt-get update sudo apt-get upgrade sudo apt-get install software-properties-common sudo apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8 sudo add-apt-repository 'deb [arch=amd64,arm64,ppc64el] http://mirror.lstn.net/mariadb/repo/10.5/ubuntu bionic main' sudo apt update sudo apt install mariadb-server 해당 작업을 하면서 apt의 저장소에 대한 오류가 생겼었는데 글의 주제와 맞지않아 다른 글에서 다루겠습니다. 이제 설치까지 마쳤다면 아래의 명령어를 실행해서 데이터를 복구합니다. mysql -u[사용자] -p[비밀번호] [Option] < [dump 파일이름].sql 이제 모든 준비가 됐습니다! 애플리케이션과 연결만 잘 되면 해피엔딩입니다. 하지만.. Docker DB로 애플리케이션을 연결했을때는 잘 연결되던 것이 이번엔 Connection Fail을 뱉어냅니다. 문제는 mariaDB 설정에 있는 bind-address 입니다. 설치 후 아무런 설정을 하지않았다면 bind-address가 127.0.0.1 로 설정되어 있습니다. 즉, 외부에서 접근을 허용하지 않고있습니다. 이 설정을 바꿔줘야합니다. 설정파일은 우분투기준 /etc/mysql/mariadb.conf.d/50-server.conf 에 있습니다. 이 설정을 주석처리하면 0.0.0.0에 대한 IP를 허용합니다. 즉 전체를 허용하는 것입니다. 여기까지 설정했다면 연결까지 완료됐습니다! 혹시 Character-Encoding 문제가 있다면 이 글을 참고해보세요!",BE
776,"FFmpeg로 gif파일 mp4로 변환하기 - 1 놀토 서비스를 만들면서 프론트엔드 크루가 이미지 저장시 gif 파일 형식을 mp4 파일 형식으로 변환해달라는 요청을 했습니다. 'gif형식이 대중적이지 않은가?' 라고 생각했는데 gif 파일을 mp4 파일 형식으로 변경했을때 용량의 차이가 어마어마했습니다. 이름은 다르지만 mp4로 변환한 것이 resize2.mp4 파일입니다. 대략 50배의 용량 차이가 났습니다. 그래서 당장 'gif파일을 mp4로 어떻게 서버에서 변경해줄 수 있을까?' 고민했습니다. 놀토에서 이미지는 AWS S3 버킷에 저장하고 있었습니다. 그렇다면 S3에 저장하기 전에 gif를 mp4로 변환해야합니다. Java에서 기본적으로 제공해주는 ImageIO는 JPEG, PNG, GIF, BMP 및 WBMP 형식을 기본적으로 제공합니다. 따라서 ImageIO로는 당장 하기에 한계가 있습니다. 대안으로 FFmpeg 프로그램이 있습니다. FFmpeg를 사용하면 비디오, 오디오, 이미지를 쉽게 Encoding, Decoding, Muxing, Demuxing 할 수 있도록 도움을 주는 멀티미디어 프레임워크입니다. FFmpeg 프로그램을 사용하면 gif 파일을 mp4로 변환할 수 있습니다. 하지만 FFmpeg는 기본적으로 Command Line 에서 명령을 사용해서 사용할 수 있습니다. 하지만 우리는 Java에서 이 프로그램의 명령을 실행시켜야합니다. 여기서 사용할 수 있는 ffmpeg-cli-warpper 라는 라이브러리가 있었습니다. 아래와 같이 친숙한 코드로 명령을 내릴 수 있습니다. 게다가 method chaining 을 사용해서 코드를 이해하는데도 어렵지 않습니다. 단 사용하기 위해 조금 까다로운것이 있습니다. FFmpeg가 서버내에 설치돼있어야 합니다. FFmpeg의 설치경로를 사용자가 직접 설정해줘야합니다. -> 테스트가 힘든 환경입니다. MacOS에서 ffmpeg를 설치하면 기본 경로는 /usr/local/bin/ffmpeg 입니다. (homebrew 사용) ubuntu에서 ffmpeg를 설치하면 기본 경로는 /usr/bin/ffmpeg 입니다. (apt 사용) ffmpeg-cli-warpper 라이브러리는 자바코드로 FFmpeg의 Command Line 명령어를 실행하는것일 뿐 FFmpeg프로그램을 대신할 수는 없습니다. 앞으로 ffmpeg 적용 과정은 Mac을 기준으로 설명하겠습니다. 먼저 FFmpeg를 설치합니다. Ubuntu 설치 시 : apt-get install ffmpeg Mac 설치 시 : brew install ffmpeg 윈도우 설치 시 : 홈페이지에서 다운로드 받으실 수 있습니다 :) 프로젝트에 ffmpeg-cli-warpper 라이브러리 의존성을 추가합니다. Maven Repository - ffmpeg-warpper 에서 가져올 수 있습니다! Gradle에서는 아래와 같이 의존성을 추가하시면 됩니다. implementation 'net.bramp.ffmpeg:ffmpeg:0.6.2' 이제 라이브러리의 FFmpeg 와 FFprobe 객체를 생성해야하는데 ffmpeg 와 ffprobe의 경로가 필요합니다. 제가 Mac에 설치했을때 경로는 아래와 같았습니다. ffmpeg path => /usr/local/bin/ffmpeg ffprobe path => /usr/local/bin/ffprobe 추가로 ubuntu에 설치하면 아래와 같습니다. ffmpeg path => /usr/bin/ffmpeg ffprobe path => /usr/bin/ffprobe 경로를 가지고 아래와 같이 객체를 생성할 수 있습니다. final FFmpeg fFmpeg = new FFmpeg(ffmpegPath);
final FFprobe fFprobe = new FFprobe(ffprovePath); FFmpeg : 멀티미디어 간에 파일을 변환하는데 사용하는 Command Line 도구입니다. FFprobe : 간단한 멀티미디어 스트림 분석기 입니다. 이제 FFmpeg 와 FFprobe 객체를 생성했습니다. 명령어를 사용하기 위해서 FFmpegBuilder를 사용해 생성합니다. FFmpegBuilder builder = new FFmpegBuilder()
 .overrideOutputFiles(true)
 .addExtraArgs(""-r"", ""10"")
 .setInput(gifFile.getPath())
 .addOutput(mp4File.getPath())
 .addExtraArgs(""-an"")
 .setVideoPixelFormat(""yuv420p"")
 .setVideoMovFlags(""faststart"")
 .setVideoWidth(240)
 .setVideoHeight(182)
 .setVideoFilter(""scale=trunc(iw/2)*2:trunc(ih/2)*2"")
 .done(); 사용한 명령어 overrideOutputFiles : 변환하려는 파일이 이미 존재하면 덮어쓰기를 할지 여부를 결정 setInput : 변환의 대상 파일을 지정합니다. 즉, 읽어올 파일입니다. addOutput : 변환한 파일을 저장할 파일을 지정합니다. 즉, 쓰기를 실행할 파일입니다. setVideoPixelFormat : 비디오 픽셀 포맷을 설정합니다. setVideoMovFlags : MP4 파일의 구조를 최적화 하여 브라우저가 가능한 한 빨리 파일을 로드할 수 있도록 합니다. setVideoWidth : 변환될 파일의 가로(너비) 사이즈를 정합니다. setVideoHeight : 변환될 파일의 세로(높이) 사이즈를 정합니다. setVideoFilter : 비디오 필터를 설정합니다. 위 명령어에서는 H.264를 사용하는 MP4 비디오는 2로 나눌 수 있는 크기를 가져야 해서 설정한 조건입니다 addExtraArgs : 커스텀한 명령어를 설정 할 수 있습니다. 위의 설정으로 나타낸 명령어는 아래와 같습니다. /usr/local/bin/ffmpeg -y -v error -r 10 -i [input파일경로] -pix_fmt yuv420p -movflags faststart -s 240x181 -an [output파일경로] 이제 생성한 명령어 builder 를 실행합니다. FFmpegExecutor 객체를 생성해서 실행을 합니다. FFmpegExecutor executor = new FFmpegExecutor(fFmpeg, fFprobe);
FFmpegJob job = executor.createJob(builder);
job.run(); 간단하게 구현할 수 있지만 한 가지 문제점이 있습니다. 이미지 변환도 리소스를 꽤 잡아먹는 작업이기 때문에 애플리케이션과 함께 서버에서 돌려도 괜찮을지 고민해봐야합니다. 전체 코드를 보고싶은 분은 여기서 확인할 수 있습니다. (FFmpegTest.java 를 봐주세요) 아래의 문제점은 다음 글을 참고하시면 해결할 수 있습니다 단 사용하기 위해 조금 까다로운것이 있습니다. FFmpeg가 서버내에 설치돼있어야 합니다. FFmpeg의 설치경로를 사용자가 직접 설정해줘야합니다. -> 테스트가 힘든 환경입니다. MacOS에서 ffmpeg를 설치하면 기본 경로는 /usr/local/bin/ffmpeg 입니다. (homebrew 사용) ubuntu에서 ffmpeg를 설치하면 기본 경로는 /usr/bin/ffmpeg 입니다. (apt 사용) 참고자료 - https://github.com/bramp/ffmpeg-cli-wrapper - https://jodu.tistory.com/30 - https://velog.io/@tlatldms/FFmpeg%EA%B3%BC-FFprobe - https://www.ffmpeg.org/ - https://wikidocs.net/78484 - https://unix.stackexchange.com/questions/40638/how-to-do-i-convert-an-animated-gif-to-an-mp4-or-mv4-on-the-command-line - https://velog.io/@ace0390/Java-ffmpeg%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%8F%99%EC%98%81%EC%83%81-%EC%B2%98%EB%A6%AC%ED%95%98%EA%B8%B0",BE
777,"Ubuntu에서는 apt(Advanced Packaging Tool)로 소프트웨어를 설치하거나 제거할 수 있습니다. 얼마전 놀토에서 Ubuntu서버에 도커 컨테이너가 아닌 호스트에 직접 mysql을 설치해야 했습니다. (궁금하신 분은 이 글을 참고해주세요!) Ubuntu 18.04버전을 기준으로 설명합니다. 에러가 났던 상황을 간단하게 설명하면 다음과 같습니다. Ubuntu의 기본 저장소(Repository)는 MariaDB 패키지가 있지만 버전이 오래됐습니다. (10.1x 버전이 존재) 도커 컨테이너에 설치된 MariaDB 버전은 10.5x 버전이라서 dump 파일을 10.1x 버전에서는 읽을 수 없습니다. Ubuntu 에 MariaDB에서 제공하는 저장소를 추가합니다. 추가하는 명령어가 focal(Ubuntu 20.04 버전의 코드네임)으로 된것을 보고 추가진행 중인것을 중단했습니다.(control + c, 강제종료) 저장소 추가 명령어에 bionic으로 다시 추가했습니다. MariaDB 10.5 설치 명령어가 오류가 나면서 설치되지 않습니다. ubuntu@ip-172-31-1-88:~$ sudo apt install mariadb-server-10.5 Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies: mariadb-server : Depends: mariadb-server-10.5 (>= 1:10.5.12+maria~focal) but it is not going to be installed E: Unable to correct problems, you have held broken packages. mariadb-server-10.5 패키지를 설치하려고 했으나 패키지가 손상됐다고 나오네요. 오랜 시간 추적끝에 저장소를 추가하던 도중 멈춰서 문제가 발생했다는걸 알았습니다. 저장소의 리스트가 있는 파일은 다음의 경로에 있습니다. /etc/apt/sources.list sources.list 파일을 열어보면 아래와 같이 다양한 저장소들이 있고 위에서 추가한 저장소들도 있음을 확인할 수 있습니다. deb http://security.ubuntu.com/ubuntu bionic-security main restricted # deb-src http://security.ubuntu.com/ubuntu bionic-security main restricted deb http://security.ubuntu.com/ubuntu bionic-security universe # deb-src http://security.ubuntu.com/ubuntu bionic-security universe deb http://security.ubuntu.com/ubuntu bionic-security multiverse # deb-src http://security.ubuntu.com/ubuntu bionic-security multiverse deb [arch=arm64,amd64,ppc64el] http://mirror.lstn.net/mariadb/repo/10.5/ubuntu focal main # deb-src [arch=arm64,amd64,ppc64el] http://mirror.lstn.net/mariadb/repo/10.5/ubuntu focal main deb [arch=ppc64el,arm64,amd64] http://mirror.lstn.net/mariadb/repo/10.5/ubuntu bionic main # deb-src [arch=ppc64el,arm64,amd64] http://mirror.lstn.net/mariadb/repo/10.5/ubuntu bionic main 여기서는 두가지 선택이 있습니다. 오류가 났던 저장소를 삭제한다. sources.list 파일 자체가 너무 많이 망가져서 어디를 건드려야할지 모르겠다. -> sources.list 파일을 초기화합니다. 1번 방법은 오류가 났던 저장소를 삭제한다는 말 그대로 파일 에디터로 열어서 해당 부분을 삭제하시면 됩니다. 2. sources.list 파일을 처음 상태로 초기화하기 약간 위험한 방법일 수 있어서 1번의 방법이 가능하다면 1번으로 진행해주세요. 1. 혹시 모르니 sources.list 파일을 백업합니다. sudo cp /etc/apt/sources.list /etc/apt/sources.list.back 2. sources.list의 내용을 전부 삭제합니다. 3. https://repogen.simplylinux.ch/ 에 접속합니다. 4. Select your country탭에서 South korea를 선택합니다. 5. Select your release에서 Bionic 18.04 를 선택합니다. (자신에게 맞는 버전을 선택합니다. 사이트는 18.04까지만 지원합니다) 6. Ubuntu Branches탭에서 체크 박스를 전부 체크해주세요 7. Ubuntu Updates탭에서 Proposed 를 제외하고 전체를 선택해주세요. 8. (Optional) 3rd Parties Repos 탭에서는 필요한 서드파티 저장소를 선택합니다. 9. 페이지 가장 하단으로 가면 있는 Generate List 버튼을 클릭합니다. 10. 새로 나오는 페이지에서 Sources List 의 내용을 전부 복사해서 sources.list에 붙여넣습니다. 여기까지 하셨다면 sources.list 파일이 초기화 됐습니다!",BE
822,"놀토 프로젝트
놀토 프로젝트가 드디어 막을 내렸다. 며칠 동안 자잘한 (대부분은 UI) 버그들도 고치고, cookie도 수정하고, 구조 도식화도 하고, 거의 팀원들과 떠들기만 하고, 배포를 시도할 때마다 꼬여버리는 github도 겨우 해결했다. 아! 갈수록 난장판이 되어갔던 노션도 정리했다.

게더타운으로 진행된 데모데이는 생각보다도 훨씬 재밌고 바빴고 알찼다. 데모데이 전날 밤 우리 팀 부스를 화려하게 꾸미다가 혼나고 (…) 다같이 캐릭터 모자를 맞춰쓰고 열심히 뽈뽈 돌아다녔다. 초반 호객행위를 해서인지 많은 분들이 우리 부스에 와주셨고, 리뷰어님들도 많이 놀러와주셨다. 모두 아낌없는 칭찬과 조언을 해주셔서 더욱 자신감과 힘을 얻을 수 있었다.

마지막으로 온라인 회식을 했다. 직접 만나고 싶었으나 한 팀원의 격리 때문에 😂

줌이 너무나도 익숙해져서인지 오디오가 쉴틈없이 이야기를 주고 받았다. 거의 술도 안먹었으면서. 정말 주제없는 이야기들을 주고받으며 많이 웃고, 또 팀원들의 숨겨진 스토리들을 들으며 서로 더 친밀해진 기분이다. 온라인으로 무려 4시간이나 떠들었다.

그동안 함께 해온 미키, 조엘, 아마찌, 포모, 찰리 그리고 항상 든든한 지원군이었던 코치 구구 정말 고마웠어요 ❤️ 우테코 3기중에 제일 귀여웠던 놀토 팀은 영원할거야

01
프론트엔드 공부
자바스크립트 반응형
🍀 여기서 읽기

자바스크립트 Proxy
🍀 여기서 읽기

npm은 어떻게 동작할까
🍀 여기서 읽기

공부하기
구글 SEO 기본 가이드
✅ Google이 내 콘텐츠를 찾을 수 있도록 돕기

sitemap을 제출한다. sitemap은 사이트에 있는 파일로서 새 페이지나 변경된 페이지가 있을 때 이를 검색엔진에 알려준다.
robots.txt를 사용하여 원치 않는 크롤링을 차단한다.
✅ Google 및 사용자가 내 콘텐츠를 이해할 수 있도록 돕기

Google이 사용자와 같은 방식으로 내 페이지를 인식하도록 한다. Googlebot이 웹사이트에서 사용하는 자바스크립트, CSS, 이미지 파일에 항상 액세스할 수 있도록 허용한다.
<title> 요소를 사용하여 고유하고 정확한 페이지 제목을 만든다. <title>은 간단하지만 설명이 담겨 있어야 하며, 각 페이지에 고유한 <title> 요소 텍스트가 있는 것이 좋다.
메타 설명 태그를 사용하여 페이지 내용을 요약하여 제공한다. 설명 메타 태그는 Google 검색 결과에서 페이지의 스니펫으로 사용할 수 있다.
✅ Google 검색결과에 사이트가 표시되는 방식 관리하기

페이지의 구조화된 데이터를 수정하여 Google 검색결과에서 여러 특별한 기능들을 사용해보자.
✅ 사이트 계층 구조 구성하기

검색엔진의 URL 사용 방식을 이해하자. 콘텐츠별로 고유한 URL을 사용하는 것이 좋다.
웹사이트 탐색 기능을 사용하여 방문자가 원하는 콘텐츠를 빨리 찾을 수 있도록 할 수 있다. breadcrumb 등의 UI로 탐색경로의 목록을 사용하거나, 사용자가 사용할 간단한 탐색 페이지를 만들 수 있다.
단순한 URL을 사용하여 콘텐츠 정보를 전달해야 한다.
✅ 콘텐츠 최적화하기

사이트를 재미있고 유용하게 만들어 자연스러운 입소문을 타게 한다(!)
독자가 무엇을 원하는지 이해하고 적절한 콘텐츠를 제공한다. Google Search Console은 인기 검색어를 제공한다.
사용자 신뢰를 구축하는 웹사이트를 만들고, 전문성과 권위를 명확히 드러낸다. 또 주제에 관해 적절한 양의 콘텐츠를 제공하는 것이 좋다.
주의를 분산시키는 광고를 표시하는 것은 지양하며, 사용자와 검색엔진이 탐색하기 쉽도록 링크를 현명하게 사용한다.
✅ 이미지 최적화하기

HTML 이미지를 사용한다. <img> 또../../assets/wtc-week-37/는 <picture> 요소 등의 시멘틱 마크업을 사용하면 크롤러가 이미지를 찾고 처리할 수 있다. 이미지에는 alt 속성을 사용해야 한다.
이미지 사이트맵을 사용하여 검색엔진의 탐색을 도울 수 있다.
표준 이미지 형식을 사용한다.
✅ 사이트를 모바일 친화적으로 만들기

모바일 전략을 선택하자. 반응형 웹 디자인, 동적 게재, 별도 URL 등을 구현할 수 있다.
색인이 정확하게 생성될 수 있도록 모바일 사이트를 구성한다.
웹사이트를 홍보하자. 소셜 미디어 사이트를 이해하고, 검색 실적 및 사용자 행동을 분석한다. 또 사이트 사용자의 행동을 분석한다.
Ref https://developers.google.com/search/docs/beginner/seo-starter-guide?hl=ko https://imweb.me/faq?mode=view&category=29&category2=35&idx=15573

git tag 달기
배포할 때마다 매번 찾아봤던 git tag 달기. 우리는 yarn을 사용하기 때문에 package.json의 버전과 맞춰서 sematic versioning을 진행했다.

일반적으로 git tag를 다는 명령어는 아래와 같지만,

Copy
git tag -a v3.0.1 -m ""nolto version 3.0.1""
yarn을 이용해서 package의 버전을 업데이트할 수 있다.

Copy
yarn version
02
원하는 버전을 입력하면, package.json의 version 항목과 함께 git tag도 새롭게 반영된 것을 확인할 수 있다.

사파리 모바일의 video playsinline 속성
애증의 사파리 모바일 🤯

메인 페이지의 다른 페이지로 이동했다가, 다시 돌아오면 메인 페이지에 로드되는 <video> 태그의 mp4 확장자 파일들이 모두 자동으로 전체화면으로 실행된다. 아래처럼…


이렇게 정신사나울 수가 없다. 알고 보니 사파리 모바일에서는 모든 <video> 태그를 기본적으로 전체화면으로 실행한다고 한다. (대체 왜?)

그래서 <video> 태그에 playsinline 속성을 넣어줘야 하고, 문제는 다행히 해결되었다.


nginx gzip 설정
프론트엔드를 SSR 서버로 운영하게 되면서 더 이상 정적 호스팅을 위한 cloudfront를 사용할 필요가 없게 되고, 그리하여 cloudfront의 기본 gzip 옵션을 사용할 수 없게 되었다. gzip을 이용하지 않으면 자바스크립트 번들 파일의 크기가 전혀 줄어들지 않은 채로 브라우저에 렌더링되기 때문에, 방법을 찾아야 했다.

현재 서버로 사용중인 express에서도 gzip을 설정할 수 있지만, express 공식 문서에서는 만약 nginx를 사용한다면 nginx에서 gzip을 설정하라고 권고하고 있다. 기본적으로 nginx 역시 cloudfront와 같은 캐시 프록시 역할을 하므로, nginx에서 파일 압축을 수행하는 것이 맞다는 생각이 들었다. 또 express 서버에 파일 압축 업무를 추가하는 건 부담이 될 수도 있으니까?

사용중인 ssh에 접속하여 nginx 설정 파일을 수정해주면 된다.

Copy
vim /etc/nginx/nginx.conf
그리고 gzip과 관련된 설정들을 추가한다. 직접 작성하려고 했는데, 주석처리가 되어있어서 필요한 부분들의 주석을 풀어줬다.

Copy
##
# Gzip Settings
##

gzip on;
gzip_proxied any;
gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;
gzip, gzip_proxied, gzip_type 설정을 만져줬다.

gzip: on으로 gzip 설정을 켜준다.
gzip_proxied: proxy나 캐시 서버에서 요청할 경우 동작 여부를 설정한다. any로 지정하면 항상 압축하게 된다.
gzip_type: text/javascript도 gzip 대상 파일로 설정한다.
Ref https://expressjs.com/ko/advanced/best-practice-performance.html http://nginx.org/en/docs/http/ngx_http_gzip_module.html#gzip_types https://www.lesstif.com/system-admin/nginx-gzip-59343019.html

SSL이 도대체 뭔가요?
SSL은 Secure Sockets Layer의 약자로, 인터넷으로 통신을 할 때 보안 통신을 하기 위한 전자 파일이다. SSL 인증서를 서버에 설치하여 SSL 프로토콜을 이용한 보안 통신을 할 수 있다.

최근에는 TLS라는 용어가 더 많이 사용되고 있다. TLS는 Transport Layer Security의 약자로 SSL 3.0 기반의 표준화된 인증 방식이다. 요즘은 SSL보다는 TLS 방식을 더 권장하고 있다.

VeriSign이나 Comodo, 그리고 AWS까지 유료 SSL인증서를 발급해주는 기관도 있고, Let’s Encrypt와 같이 무료 인증서를 발급해주는 곳도 있다. 유료와 무료 SSL 인증서 간에 기술적인 차이점은 없다. 다만 유료 SSL 인증서의 경우 인증서를 사용한 사이트에 인증 관련 문제가 생겼을 시 유료 인증서 발급 기관에서는 일정 부분을 배상하게 된다. 무료 인증서의 경우 이러한 배상 체계가 존재하지 않는다.

Ref https://devlog.jwgo.kr/2019/04/12/what-is-ssl/

scrollIntoView
모든 페이지의 하단에 footer를 추가하니 마이페이지에 스크롤이 생겼고, 그로 인해 새로운 문제를 발견했다.

아래처럼 scrollIntoView를 통해 부드러운 swipe 효과를 내고 있었는데,

Copy
selectedTab.current.scrollIntoView();
수평 방향의 스크롤이 발생함과 동시에 수직 방향으로는 화면의 아래쪽으로 쑥 꺼져버리는 것이었다!


scrollIntoView의 속성을 잘 몰라서 발생한 문제였다.

scrollIntoView의 block 옵션은 요소가 스크롤 가능한 조상에서부터 수직 방향으로 어떻게 정렬될지 결정하는 옵션이다.

{ block: 'start' } - 요소는 조상 요소의 최상단에 위치한다.

{ block: 'center' } - 요소는 조상 요소의 중간에 위치한다.

{ block: 'end' } - 요소는 조상 요소의 최하단에 위치한다.

{ block: 'nearest' }

요소가 조상 요소보다 아래 있다면, 요소는 조상 요소의 최상단에 위치한다.
요소가 조상 요소보다 위에 있다면, 요소는 조상 요소의 최하단에 위치한다.
요소가 이미 보이는 상태라면, 그 자리에 있는다.
기본값이 start로 설정되어 있었기 때문에, 스크롤 가능한 영역에서 수직 방향으로 최상단에 재정렬되어 화면은 아래쪽으로 밀려난 것이었다.

요소가 이미 보이는 상태기 때문에, 수직 방향으로 가만히 있게끔 { block: 'nearest' }를 걸어주었다.

Copy
selectedTab.current.scrollIntoView({ block: ""nearest"" });

문제 해결!

Ref https://stackoverflow.com/questions/48634459/scrollintoview-block-vs-inline

FE Conf - 컴포넌트, 다시 생각하기
당근마켓 원지혁님의 FE Conf 발표영상이다.

컴포넌트의 의존성에 기반하여 React 컴포넌트를 분리하는 하나의 관점을 설명한다.

✅ 비슷한 관심사끼리 함께 두기 비슷한 관심사라면 가까운 곳에 둠으로써 코드의 사용성을 높인다. 대표적으로 하나의 컴포넌트에서 사용하는 스타일(styled-components 등)과 로직(custom hooks)는 컴포넌트 파일에 내재하거나, 동일한 폴더 내에 다른 파일에서 관리할 수 있다.

✅ 데이터를 ID 기반으로 정리하기 특정 interface를 가지고 있는 모델 인스턴스 데이터를 다른 컴포넌트에 통째로 넘겨주는 것이 아니라, 필요한 데이터의 id만 넘겨준다. Global ID(또는 Node ID)는 도메인 내에서 유일성을 갖는 ID 체계로, API 요청 시 Global ID를 전달하여 데이터를 가져올 수 있다. 이러한 방식을 GOI(Global Objet Identification)이라고 부른다.

Copy
interface Props {
articleId: string;
}

const Something: React.ExoticComponent<Props> = (props) => {
const article = useNode({ on: ""Article"" }, props.articleId);

return (
<div>
<h1>{article.title}</h1>
</div>
);
};
useNode와 같은 컨셉은 GraphQL과 Relay에서 차용했다고 한다.

✅ 이름 짓기 - 의존한다면 그대로 드러낸다. 아래와 같은 interface보다는,

Copy
interface Props {
leftImageThumbnailUrl: string;
title: string;
title2: string;
caption: string;
rightDotColor: string;
rightcaption: string;
}
다음 interface가 모델 간의 연결 정보를 더욱 명확하게 드러내고 있다.

Copy
interface Props {
user: {
name: string;
nickname: string;
totalFollowerCount: number;
lastActivityAt: Date;
image: {
thumbnailUrl: string;
};
};
}
그러나 이 경우 props로 데이터를 넘겨주는 상위 컴포넌트가 항상 정확하게 타이핑을 해서 데이터를 보내줘야 한다는 문제가 있다. 이때 다시 한번 전역 ID를 통해 필요한 객체의 레퍼런스만 받아와 의존성이 느슨한, 재사용 가능한 컴포넌트를 만들 수 있다.

4. ✅ 모델 기준으로 컴포넌트 분리하기 얼핏 보면 거의 비슷한 두 컴포넌트가 있다. 이럴 때 기존에 있던 컴포넌트를 재사용할지, 아니면 복사해서 새 컴포넌트로 분리할지 고민이 된다. (실제로 엄청 많이 마주한 문제다!)

발표에서는 다음과 같은 기준을 제시한다.

같은 모델을 의존하는 컴포넌트는 재사용, 다른 모델을 갖는 컴포넌트는 분리하자.

이를 위해 리모트 데이터 스키마(API 데이터)를 주의깊게 관찰해야 한다.

마지막으로 인상 깊었던 조언!

“질문이 정답보다 중요하다” “관점이 기술보다 중요하다” 그리고, 기술 도입이 성공하기 위해서는 유저 경험에 이르기까지 영향을 미칠 수 있어야 한다.

프론트엔드 개발자로서 가져야 할 좋은 마인드!

Ref https://www.youtube.com/watch?v=HYgKBvLr49c

기타
자바스크립트로 오징어게임 만들기
진짜 광기..

Ref https://dev.to/0shuvo0/i-made-squid-game-with-javascript-10j9

개발자 김민준님 인터뷰
모두가 공감하는 말, 좋은 동료들이 최고의 복지다!

Ref https://www.youtube.com/watch?v=pf9Rj3GKhBA

Apple system-ui를 대체할 폰트, Pretendard
데모데이날 리뷰어 발리스타님이 오셔서 추천해주고 가셨다. Apple의 system-ui 폰트들과 Noto Sans의 아쉬운 점들을 보완한, 크로스 플랫폼을 지원하는 폰트다. (윈도우에서도 사용 가능!)

Ref https://cactus.tistory.com/306

development mode는 어떻게 동작하는가?
Ref https://overreacted.io/how-does-the-development-mode-work/

언어별로 부동소수점 알려주는 사이트
도메인이 진짜 광기…

Ref https://0.30000000000000004.com/

A레코드 vs CNAME
도메인 네임을 다는 두 가지 대표적인 방법

A레코드 - EC2 등을 이용한 ip 주소에 직접 걸어준다.
CNAME - AWS cloudfront를 이용하면 https://d2y0p6hv0hkdrd.cloudfront.net과 같은 랜덤 네임을 자동으로 붙여주는데, 그에 CNAME으로 별칭과 같이 이름을 붙여준다.
면접관은 도대체 무슨 생각을 할까?
썸네일은 크게 와닿지 않는데, 최근 본 영상들 중에 가장 인상 깊었다.

Ref https://www.youtube.com/watch?v=7ye03TMi5SU&t=333s

캡틴 로이드의 ‘잘 정리된 이력서보다 중요한 것’
✅ 지원동기가 명확한가 “동기”를 어떻게 만들어낼지 판단하게 된다. ‘동기부여’는 누군가 주입하는 것이 아니라, 스스로 하는 것이다.

✅ 어떻게 일하는가 혼자서 일하는 회사는 없다. 어떤 규모에서, 어떤 사람들과 함께 일했는지를 밝히자.

✅ 퍼포먼스를 내는가 회사는 현재 조직에 없는 사람, 더 나은 성과를 낼 수 있는 사람을 기대한다. 같은 일을 할 때 더 큰 성과를 낼 수 있음을 증명해야 한다.

과정만큼 결과는 중요하고, 퍼포먼스를 ‘내고 싶은 것’과 ‘내왔던 것’은 다르다.

✅ 어려움을 겪었는가 어려움의 경험은 스스로 성찰하는가를 알게 하며, 어려움의 경험 이후로 실제로 성장했는지 알게 한다.

✅ 본인에 대해 잘 이해하는가 팀과 나의 지향점이 맞는지를 판단하게 한다.

그리고 최근 어디선가 들은 질문,

내가 시니어 개발자라면, 누굴 뽑을 것인가?

Ref https://minieetea.com/2021/04/archives/6193

마무리
이렇게 레벨 4가 끝났다. 모든 프로젝트, 미션이 끝났다. 그와 동시에 이제 피할 수 없는 시간이 찾아왔다. 위드코로나가 시작되는 다음주부터는 루터에 등교해도 되고, 이제 2주만 있으면 우형 면접이다. 12월까지 이어지는 면접 전형이 너무 길다고 생각했는데, 생각해보면 어쩔 수 없는 결정이었던 것 같다.

결과에 상관없이 우테코가 끝나면 필히 오열할 것이라고 했는데, 데모데이가 끝난 지금 아직 오열하진 않았다. 조금(많이) 뭉클하긴 했다. 어쩜 조와…",NON_TECH
825,"안녕하세요? 제이온입니다. 과거 힐링페이퍼 면접 이후 이규원 CTO 님께서 제가 가지고 있던 CI의 오해를 바로 잡기를 권유하시며, 한 가지 포스팅을 추천해 주셨습니다. 이번 시간에는 그 포스팅을 제가 번역한 내용을 작성하고, 스스로 느낀 점과 함께 우아한테크코스 다라쓰 팀에는 어떻게 적용하고 있는지 설명드리겠습니다. (번역은 오역이 있을 수 있으니 원본 포스팅도 꼭 정독하시길 바랍니다.) 번역 내용 서론 ThoughtWorks 기술 레이더는 최근 기술 팀 안티 패턴인 ""CI 극장""을 피할 것을 권고했습니다. CI 극장은 실제로 실행하지는 않지만 지속적인 통합(CI)을 실행한다고 착각하는 행위를 뜻합니다. 제 동료인 Emily Luke와 제가 수행한 지속적인 통합 연구를 바탕으로, CI 극장이 어떠한 모습인지, CI 극장을 하면 안되는 이유가 무엇인지, 그리고 CI 극장을 극복하는 방법에 대해 말씀드리겠습니다. 지속적 통합 제가 좋아하는 CI의 정의는 https://continuousdelivery.com/의 Jez Humble에서 따왔습니다. CI 개발자는 정기적으로 (최소한 매일) 모든 작업을 Trunk (master 브랜치라고 이해하면 됩니다.)에 통합합니다. 이 인용구에는 CI 관행의 두 가지 기본 원칙이 숨어 있습니다. 첫 번째는 ""모든 작업을 Trunk에 통합""이고, 두 번째는 ""최소한 매일""입니다. 또한 소스 저장소에서 모든 것을 확인, 모든 커밋 구축, 빌드 자동화, 빌드 속도를 빠르게 유지, 자체 테스트 코드 보유, 눈에 보이는 오류를 표시하고 즉각적인 수정과 같은 CI를 만드는 다양한 원칙과 관행도 있습니다. 디테일한 내용은 마틴 파울러님이 작성하신 포스팅을 참고하시길 바랍니다. 개인적으로 저는 적어도 하루에 한 번 마스터 브랜치를 확인하는 것이 좋은 CI를 위한 기본적인 초석이라고 생각합니다. 그리고 그것이 없다면, 여러분은 CI를 연습하는 것이 아니라 CI 극장을 연습하는 것입니다. CI 극장은 어떻게 생겼는가? 여기 저희 연구에서 나온 이야기가 있습니다. 경험 많은 개발자인 David는 Bay 지역에 있는 중간 규모의 신생 업체 출신으로 일 주일에 두 번 운영 서버에 배포합니다. David는 자신의 조직이 CI를 실천하고 있다고 표현하면서 ""네, Cicle CI를 사용합니다.""라고 말했습니다. 그는 한 branch에서 작업하고, 100개의 파일과 7000줄의 코드를 바꾼 뒤 코드 리뷰에 들어가 동료들에게 자신의 모든 변경 사항을 설명하기 굉장히 어려웠다고 설명했습니다. 그는 ""가장 어려운 것은 한 번의 커밋에 많은 특징들이 쌓이게 되는 것입니다. 왜냐하면 그것들은 모두 한 이야기의 일부이기 때문입니다.""라고 말했습니다. 이어서 그는 ""모든 것을 머릿속에 간직하는 것이 어렵기 때문에 커밋을 분해하는 더 좋은 방법이 있었으면 좋겠습니다.""고 말했습니다. 이 상황이 친숙하게 들린다면, 여러분은 CI 극장의 사례를 경험한 것입니다. 여러분이 아래와 같이 생각하거나 경험했다면, CI 극장을 범한 것입니다 누군가가 CI를 실행하고 있는지 물어봤을 때 다음과 같이 답한다. 우리는 CI 서버를 가지고 있어. 우리는 Github action, jenkins, ...와 같은 CI 툴을 사용하고 있어. 우리의 실험에서 오직 10%의 참가자만이 CI 서버를 갖고 있다는 것이 CI를 실행하는 것과 같지 않다는 사실을 알고 있었습니다. 반대로, 90%의 사람들은 CI의 기본을 실천하고 있는지 여부에 관계없이 CI를 실천하고 있다고 말했습니다. 따라서 CI 서버가 있다는 것만으로 CI를 수행하고 있다고 생각하는 것은 CI 극장의 명확한 지표입니다. 수명이 긴 branch를 사용하고 정기적으로 master branch를 확인하지 않는다. David의 이야기에서, 매일 master 브랜치를 확인하지 않는 것은 CI 극장의 신호였습니다. 이것은 팀에서 master에 대해 CI를 실행하지만 매일 커밋하지 않는 패턴이라고 볼 수 있습니다. 또는, 마스터로의 통합 없이 일반 branch에 대해 CI를 실행하는 패턴이라고 볼 수 있는데, feature 브랜치들에 대해서만 CI를 실행하면 지속적인 통합이 아닌, 지속적인 격리로서의 CI를 수행하는 것입니다. branch들을 병합할 때마다 불안함과 피로를 느낀다. 진정한 지속적인 통합은 코드 소유권을 분산하며, 팀 내 사람들의 시각과 빌드를 실패하는 것에 대한 태도를 변화하게 만듭니다. 더 이상 ""내 소중한 branch"", ""빌드가 실패한 내 잘못""이 아니라, ""우리의 코드""와 ""우리의 실패""라고 인식하게 합니다. David가 불안과 피로를 경험했다는 사실은 그가 CI의 중요한 이점인 지속적인 피드백과 집단적 소유권을 놓치고 있었다는 명백한 증거입니다. 낮은 테스트 커버리지에 대해 빌드를 수행한다. 오랜 기간 빌드가 실패하는 것을 방치한다. 정리하자면, David의 팀은 잘 알려진 CI 툴과 feature 브랜치로 나누는 행위, 코드 리뷰 같은 공통 프로세스를 보유하고 있었지만, 전체 CI 관행을 실천하고 있지 않았기 때문에 그 이점 중 많은 부분을 놓치고 있었습니다. 안타깝게도 저희 연구에 따르면 90%의 조직에서 이런 일이 일어났습니다. CI 연극을 실천하고 있는 조직들이 놓치고 있는 주요 이점 다음과 같습니다. 빠른 피드백, 집단적 소유권, 지속적인 배포로 전환할 준비가 되어있다는 것. 어떻게 CI 극장을 피하고, 극복하고, 해결할 수 있는가? 만약 여러분이 CI 극장으로 인해 어려움을 겪고 있는 경우, 다음과 같은 세 가지 방법으로 문제를 해결하고 지속적인 변화를 이끌어낼 수 있습니다. 자주 커밋해라 기본으로 돌아가서, 자주 커밋하세요. 여러분은 최소한 매일 커밋하는 것을 목표로 삼아야 합니다. CI를 수행하지 않는 경우, 이 단계에서 시작할 수 있으며, CI를 수행하고 있더라도 개선할 여지가 항상 있습니다. 무엇이든 자주하는 것은 어려움을 낮출 수 있습니다. 어떤 일을 더 자주 할 때, 그 일은 더 작아지고, 더 쉽게 성취할 수 있게 만들어 줍니다. 지속적인 통합은 더 자주 수행할수록 더 쉬워지는 중요한 예시입니다. 저는 여러분의 저장소에 코드를 더 자주 커밋하고 적어도 매일 branch들을 master 브랜치에 통합하는 것을 조언합니다. Trunk 기반 개발을 시도해라 Trunk 기반 개발과 branch 기반 개발에 대해 토론하는 포럼이 많지만, 저는 자세한 이야기를 여기서 하고 싶지는 않습니다. 그러나, CI 프로세스에 어려움을 겪고 있는 사람들을 대상으로 한 연구에서 Trunk 기반 개발을 수행하지 않은 팀이 가장 많았습니다. 또한, State of Devops 2016 보고서는 Trunk 기반 개발과 지속적인 통합을 실천하는 것이 지속적인 배포의 중요한 측면이며 팀의 성과 향상에 기여하는 것으로 나타났습니다. Trunk 기반 개발에는 분명 자체적인 어려움이 있지만, 병합, 코드 검토 또는 배포 지연이라는 문제가 덜 일어납니다. 저는 더 나은 CI와 CD를 구현하려면 Trunk 기반 개발을 수행하거나, 원하는 경우 수명이 매우 짧고 총 3개 미만의 branch를 사용하는 것을 추천합니다. 자동화를 구현해라 자동화는 강력한 지속적 통합의 초석이므로 모든 것을 아직 자동화하지 않았다면, 지금이 바로 시작할 때입니다. 만약 여러분이 할 수 있는 모든 것을 자동화했다고 생각한다면, 팀의 모든 사람이 한 번 이상 수동으로 무언가를 할 때마다, 스스로에게 ""왜 이것이 자동화되지 않는가?""라고 물어 보세요. 자동화는 CI를 향상시키는 데 도움이 될 뿐만 아니라 지속적인 배포를 시작하는 데에도 도움이 됩니다 결론 이제 CI 극장이 무엇인지 알게 되었습니다. 팀이 CI 극장을 실행하고 있다면 피할 수 있습니다. 여전히 CI 극장인지 아닌지 헷갈린다면, 마틴 파울러의 블로그에서 ""CI 인증 테스트""를 통해 신뢰할 수 있는 CI를 수행하고 있는지 여부를 확인하는 것이 좋습니다. CI 테스트를 통과하셨다면, 좋습니다. 이제 지속적인 배포를 위한 준비가 되었는지 고려해 봅시다. 느낀 점 이 포스팅을 읽기 전에는 전형적인 Git-flow 방식을 팀에서 적용하고 있었고, CI란 단순히 CI 도구를 사용하여 빌드와 테스트를 거쳐서 코드를 통합하는 행위라고 생각했습니다. 하지만, 제가 지금까지 했던 것은 단순히 통합일 뿐이고 지속이라는 말은 들어가기가 어렵다고 생각합니다. main 브랜치에 비로소 통합되는 주기는 굉장히 길었기 때문이죠. 이와 별개로 규원 님과의 면접에서는 Git-flow 방식을 사용하면서 각 브랜치마다 CI 적용하여 안전성 있고 피드백을 받을 수 있다고 답변했습니다. 그 때는 Git-flow와 CI가 왜 상극인지 이해하지 못했지만, 이제는 그 둘이 상극이라는 것이 잘 느껴집니다. 지금부터는 팀에 진정한 CI를 느끼기 위해 어떤 방식을 도입했는지 단계 별로 소개해 드리겠습니다. 우아한테크코스 다라쓰 팀에 도입한 전략 기존에는 main과 develop 브랜치가 있고, 그 때 그 때 팀원들이 이슈에 맞는 feature 브랜치를 만들어서 develop 브랜치에 먼저 pr을 보내는 방식으로 운영되었습니다. 여기서 중간 브랜치인 develop 브랜치를 없애는 Github-flow 전략으로 변경했습니다. 중간 브랜치에 없어진 것만으로도 운영 서버에 배포한다는 부담감이 줄었고, 배포 과정 자체도 간결해졌습니다. 다만 2가지 단점이 있었습니다. Github-flow 전략을 사용하면서 느낀 문제점 feature 브랜치의 생명 주기가 길어질 수 있다. 팀원이 맡은 하나의 이슈 자체는 하루 만에 끝나면 좋겠지만, 보통 새 기능 개발이면 며칠에서 몇 주까지도 가기도 합니다. 그렇게 되면 팀원에게 빠른 피드백을 받지 못하고, 자신 만의 branch라는 생각 때문에 집단적 코드 소유권 의식도 옅어지며, 코드 리뷰시 굉장히 많은 코드 때문에 팀원들이 효율적인 리뷰를 하기 어렵습니다. QA를 진행하기가 애매하다. 현재 feature 브랜치에서 main 브랜치로의 pr이 merge되면 자동으로 운영 서버로 배포가 됩니다. 그러면 중간에 QA를 진행할 수 없으므로 테스트와 코드 리뷰로는 잡히지 않는 DB 같은 이슈는 찾아내기가 힘듭니다. 수동으로 pr이 merge 되기 전에 팀원과의 의견을 조율하여 개발 서버를 이용하는 방안이 있습니다만, 자동화하는 방법은 찾지 못했습니다. 두 번째 단점보다는 첫 번째 단점이 크리티컬해서 Github-flow에서 좀 더 발전된 Trunk 기반 개발로 기법을 전환했습니다. Trunk 기반 개발은 ""모든 프로그래머가 최소한 하루에 한 번 main 브랜치에 커밋을 한다.""라는 원칙을 가지고 있으며, main 브랜치 외에 브랜치는 굉장히 자유롭고 생명 주기가 매우 짧다는 특징이 있습니다. 다라쓰 팀 같은 경우에는 feature와 hotfix라는 명칭을 붙이기는 하지만, 커밋 단위를 작게 쪼개서 이름을 붙이고 pr을 보내서 코드 리뷰를 받고 있습니다. 아직 초기 단계지만, 한 pr의 커밋 개수가 적어서 팀원이 무슨 일을 하고 있는지 파악하기 쉽고 섬세한 코드 리뷰가 가능하다는 장점을 느끼고 있습니다. Trunk 기반 개발의 단점과 실무 개발자 님들의 조언 그러나, 이 방식도 Github-flow의 2번째 단점이 존재했습니다. 빠른 호흡으로 배포를 가져가는 것은 좋지만, 중간에 최소한 실제 환경에서 테스트를 해야 한다고 생각합니다. 아무래도 다들 실무 경험이 없기 때문에 이러한 고민에 대해서 우아한테크코스 구구 코치님과 힐링페이퍼 이규원 CTO 님께 조언을 구했습니다. 우아한테크코스 구구 코치님께서는 다음과 같이 팀원의 사정에 맞게 조율해야겠지만, QA 과정을 거치는 편이 좋다고 하시면서 어느 단계든 한 번은 수동 배포를 거치라고 말씀해 주셨습니다. 힐링페이퍼 이규원 CTO님께서는 테스트 기법을 더욱 익혀서 테스트를 견고하게 만들고, 코드 리뷰를 통해 버그를 더 잘 잡을 수 있도록 프로세스를 개선하라고 말씀해 주셨습니다. 이어서, 힐링페이퍼는 PR을 날릴 때 커밋이 3개가 넘어가거나 수정된 코드의 양이 200줄이 넘어가면 bot이 경고를 주고 있다고 합니다. QA도 좋지만, 최대한 버그가 적게 발생하도록 커밋 단위를 쪼개고 버그가 발생하더라도 롤백을 할 수 있도록 장치를 만들어두는 편이 바람직해 보입니다. 특히 힐링페이퍼의 bot 시스템이 흥미로워서 팀에도 도입하자고 의견을 낼 것 같습니다. Commit-train 기반 배포 전략 도입 저는 두 분의 조언을 종합하였고, 뱅크 샐러드의 '하루에 1000번 배포하는 조직 되기' 포스팅의 commit-train 기반 배포 방식을 채택하기로 결정했습니다. 아무래도 아직 테스트 기법이나 버그 대응에 다들 미숙하기 때문에 중간 QA 과정을 빠르게라도 거치는 과정이 필요하다고 생각했고, 다음과 같이 CI / CD flow를 만들어 보았습니다. main에서 브랜치를 딴 후 기능을 개발한다. main 브랜치에 PR를 날린다. 젠킨스에서 테스트를 포함한 빌드가 수행된다. 소나큐브가 정적리포트를 분석한다. 코드 리뷰를 반영한다. PR을 main에 머지한다. 개발 서버로 자동 배포된다. 개발 서버를 기준으로 QA를 진행한다. 빠른 시간 안에 운영 서버로 수동 배포한다. 각자 PR이 merge가 되면 팀원에게 개발 서버를 사용하겠다고 밝히고 테스트하는 중간 QA 과정이 생긴 것이죠. 문제가 없으면 완료 사인을 내고, 하루 동안 QA에 대해 이상이 없으면 비로소 운영 서버로 수동 배포를 합니다. 바로 바로 main 브랜치에 커밋이 푸쉬될 때마다 배포가 되는 것이 아니라, 마치 기차처럼 커밋이 이어져있다가 한꺼번에 배포되는 것이죠. 이 방법도 도입 초기 단계이므로 시도해보면서 개선해 나가려고 합니다. 정리 진정한 CI의 의미를 이해하고 다들 제대로된 CI를 수행하기 위해 노력하고 있습니다. 아직도 배워야할 내용이 많지만, 올바른 방향을 찾았다고 생각이 듭니다. 여러분이 생각하시는 CI가 무엇인지, 그리고 우리 팀이 고민하고 있는 문제에 대해 조언하고 싶은 내용이 있다고 언제든지 댓글로 의견을 제시해 주시면 감사하겠습니다 🙏",BE
826,"안녕하세요? 제이온입니다. 오늘은 @ModelAttribute에 설명보다는 개발하면서 느낀 주의 사항에 대해 설명드리려고 합니다. @ModelAttribute 바인딩이 되지 않는 이슈 우테코 다라쓰 팀 프로젝트 도중 @RequestParam 여러 개 쓰기 귀찮아서 @ModelAttribute를 사용하여 Dto를 바인딩하기로 결정했습니다. 저는 이 어노테이션을 파라미터 앞에 쓰면 단순히 여러 개의 파라미터를 하나의 객체로 바인딩해 주는 정도로만 이해하고 있었습니다. 그래서 제가 평소에 만들듯이 요청 Dto를 정의하고, 해당 객체 옆에 @ModelAttribute를 붙여주었습니다. @Getter
@NoArgsConstructor
@AllArgsConstructor
public class CommentReadSecretCommentRequest {

private Long guestUserId;

private String guestUserPassword;
} 하지만, 위 객체에는 파라미터로 들어온 id와 password가 제대로 바인딩이 되지 않았습니다. 여기서 @NoArgsConstructor을 제거해 주니 정상적으로 바인딩이 되었고, 이유를 찾아 보았습니다. 먼저, Spring은 URL 파라미터 또는 POST Form Data 형태의 파라미터를 위와 같은 특정 클래스의 자동으로 바인딩해 주는데, 이때 반환되는 객체를 커맨드 객체라고 부릅니다. 그리고 커맨드 객체를 구성하기 위한 매핑 규칙은 다음과 같습니다. 1. NoArgsConstructor과 AllArgsConstructor 둘 다 있는 경우 NoArgsConstructor 호출하고, setter 호출하여 param을 필드에 각각 초기화한다. 2. AllArgsConstructor만 있는 경우 AllArgsConstructor 호출하여 param을 필드에 각각 초기화한뒤, setter 호출하여 param을 필드에 각각 다시 초기화하여 덮어 씌운다. 즉, 제가 처음에 설계한 객체는 매핑 규칙 1번에 해당하지만 setter가 존재하지 않아서 param들을 필드에 초기화해줄 수 없던 것이었습니다. 그래서 기존에 붙여 두었던 @NoArgsConstructor을 제거하였습니다. 번외로, Controller의 파라미터 옆에 @ModelAttribute를 붙이지 않아도 자동으로 커맨드 객체를 구성해 준다고 합니다. 스프링 단에서 int나 String같은 단순한 자료형은 @RequestParam으로 인식하고, 그 외의 복잡한 객체는 @ModelAttribute로 인식한다는데, 그럼에도 스프링은 간단한 숫자나 문자로된 요청도 복잡한 객체로 인식할 여지가 있으므로 어노테이션을 붙여주는 것이 좋다고 생각합니다. 정리 평소에 요청 Dto를 Jackson 라이브러리를 통해 역직렬화하는 경우가 많아서 위의 3가지 어노테이션을 주로 사용하여 객체를 구성했었습니다. 이제는 @ModelAttribute에서의 매핑 규칙을 이해하였으므로 요청 Dto를 설계할 때, 사용 목적에 맞게 설계할 수 있을 듯합니다.",BE
827,"팀 프로젝트가 나에게 남긴 것 레벨 2 방학과 레벨 3 모의 면접 레벨 2 방학 때 제이슨에게 크루 피드백을 받은 적이 있다. 크루 피드백은 페어 프로그래밍이나 협업 미션을 같이 한 크루들이 나에 대해 어떻게 생각하는지에 대한 피드백을 의미한다. 칭찬도 있었지만, 대부분의 크루들은 나에게 조급함을 버리라는 조언을 해 주었다. 레벨 1과 레벨 2를 돌아보니, 미션 기한에 맞춰 너무 급하게 기능을 구현하려고 하지 않았나 생각이 들었다. 레벨 3 모의 면접에서는 이론적인 정답을 말하려는 습관 때문에 깊이가 얕은 대답을 많이 했다. 면접관이셨던 포비께서는 자신의 경험을 바탕으로 대답하라고 말씀해 주셨고, 평소에 깊이 있는 학습을 해야 한다고 알려 주셨다. 레벨 2동안 조급함으로 인해 제대로 이해하지 않고 넘어갔던 개념들이 생각났다. 우리 팀의 문화 그전까지는 워라벨보다는 무조건 온종일 개발해서 요구 사항을 구현하는 것이 최우선이었다. 하지만 위에서 언급한 단점들을 인식한 후로 여유있게 개발하기로 결심했다. 그래서 팀원들에게 오전 10시에서 오후 6시까지 공식적인 협업 시간을 두자고 제안했다. 다행히 팀원들도 팀플과 개인 공부 시간 혹은 휴식을 분리하는 것에 찬성했다. 물론 열정이 넘치는 팀원들이라서 오후 6시 이후에도 활발히 이슈가 열리고 채팅방에서 토론이 오가곤 한다. 그럼에도, 쉴 사람은 눈치 보지 않고 쉴 수 있는 환경이 조성되어서 개발과 휴식의 선을 분리할 수 있었다. 우리 팀은 최소한만의 기능을 개발하고 여유 시간을 확보하여 부가 기능을 개발하는 애자일한 방식을 추구한다. 그래서 정해진 기간 안에 구현하고도 시간이 남았고, 덕분에 놓치고 넘어간 개념을 심화 학습할 수 있었다. 현재의 나 레벨 1, 2에 비해 기능 구현 속도도 더디고 개발에 투자하는 시간도 적어졌다. 과거의 나는 현재의 나에게 게을러진 것이 아니냐고 지적할 것이다. 하지만, 하나의 기능을 구현하기 위하여 관련된 개념을 깊게 학습한다. ""왜 되는 것이지"", ""왜 이렇게 만들었을까?""라는 의문이 생기면 반드시 해결하려고 노력한다. 또한, 충분한 휴식을 통해 다음날 개발에 더욱 집중할 수 있다. 크루들이 조언해 주었던 ""개발은 단거리 경주가 아니다.""라는 말을 절실하게 느낀다. 결론적으로 현재의 나는 과거의 나에게 ""기능 구현 속도보다는 더 깊이 있는 학습을 지향하고 있어!""라고 반박할 것이다. 이러한 습관을 갖추고 기업 면접에 임하니, 나의 경험을 기반으로 답변할 수 있었다. 암기식 지식이 아닌 나만의 스토리를 어필하니까 공포의 대상이었던 면접에 재미가 생겼다. 앞으로도 조급함을 버리고 깊이 있는 학습을 추구하는 습관을 유지하여 더욱 성장하려고 한다. 마지막으로 나의 단점을 투명화할 수 있도록 도와준 우리 다라쓰 팀원들 아론, 제리, 우기, 곤이, 도비에게 감사하다는 말을 하고 싶다.",NON_TECH
828,"안녕하세요? 제이온입니다. 오늘로 레벨 3가 끝났고, 저의 우테코 생활도 종료되었습니다. 레벨 1, 2, 3동안 돌아보는 최종 회고는 추후 회사 들어가기 전에 작성하도록 하고, 오늘 있었던 일 정도만 기록하려고 합니다. 운영 브랜치 통합 이슈 어제 말한 대로 develop/be 브랜치로부터 release 브랜치를 만들고, develop/fe 브랜치로부터 pull 땡겨온 뒤 main 브랜치로 pr을 날리는 방식을 적용했습니다. 하지만, release -> main pr에서 온갖 컴플릭트가 발생했습니다. 일단은 병합 기준을 전부 release 브랜치로 정해서 넘겼지만, 매번 통합할 때마다 컴플릭트를 해결하는 것은 불편하기도 하고 실수할 여지가 있었습니다. 그래서 이 부분을 팀원과 협의하였습니다. 이 일의 원인은 운영 브랜치와 개발 브랜치가 동기화되지 않았기 때문입니다. develop/be에는 backend 디렉토리만 있고, develop/fe에는 frontend 디렉토리만 있는데 main에는 backend, frontend 디렉토리가 모두 존재했습니다. 그래서 앞으로 통합 flow을 다음과 같이 정리했습니다. develop/be , devleop/fe 두 개를 합친 release 브랜치를 딴다. release → develop/be, develop/fe, main (스쿼시 머지) realease 삭제 (자동) 최신화된 develop/be , devleop/fe에서 feature를 따서 새로운 작업을 한다. 핵심은 2번입니다. 운영과 개발 브랜치 모두 동기화되어야하므로 배포하기 위해 통합된 내역을 운영 브랜치만 pr을 날리는 것이 아니라, 개발 브랜치에도 각각 날려서 backend, frontend 디렉토리가 동기화되게 만들어야 했습니다. 결국, develop/be에는 backend 디렉토리 외에 frontend 디렉토리까지 존재해야하므로 develop/be, develop/fe로 분리한 것이 어느 정도 의미가 없어졌습니다. 이 부분은 사용해 보면서 통합할 수도 있습니다. 다만.... 이 과정에서 한 가지 큰 문제가 발생했습니다. develop/be에서의 CI / CD는 젠킨스가 담당하고 있고, 위에서부터 4개의 workflow는 운영용 CI / CD입니다. 즉, develop/be에 pr이 들어왔을 때는 젠킨스 및 delete_branch_on_close_pr.yml만 작동해야하는데 위의 4개 yml까지 작동하는 이슈가 생겼습니다. on:
pull_request:
branches: [ main ] 보시다 시피, 해당 yml들에는 pr이 main 브랜치로 들어왔을 때만 작동하는데 참 이상합니다. 천천히 원인을 찾아보려고 합니다. 레벨 3 종료 이후에는 데모 데이 및 방학식을 진행했습니다. 특히, 방학식에서는 마지막으로 크루들에게 인사하는 시간을 가졌습니다. 벌써 나가야하는 것이 참 아쉽고, 아직 친해지지 못한 크루들도 많아서 아쉬웠습니다. 거기다가 코로나로 인해 대부분의 시간을 온라인으로 보냈으니.. 살면서 이 시간이 후회가 남을 수도 있을 것 같습니다. 그럼에도, 현역 산업기능요원이 내년부터 막히기 때문에 갈 수 밖에 없었습니다. 코로나가 좀 진정되면 꼭 잠실에 올라가서 크루들과 만나고 싶습니다. 방학식이 끝나고 다라쓰 팀원 및 공원 코치님과 랜선 회식을 했습니다. 고기를 먹는 곤이, 아론, 제리를 보니 상당히 부러웠고, 입사만 좀 늦었어도 오프라인으로라도 만났을 텐데 여러모로 온라인으로만 봐야하는 현실이 좋지 않았습니다. 그런 생각도 잠시, 짓궃은 우리 형들 장난때문에 가슴이 철렁하는 일이 있었습니다. 자세한 일은 적지 않겠지만, 이런 몰카가 괘씸하긴 한데 저를 위해 세네시간 쓰면서 깜짝 이벤트를 해 준 것은 참 고마웠습니다. 다음에 꼭 이 일은 갚도록 하겠습니다 ^^ 아무튼.. 가기 전에 큰 추억을 만들어서 좋았고, 우리 다라쓰 팀원들은 꼭 다음 달에 보러 가려고 합니다 ㅎㅎ 정리 오늘 부로 저는 우테코 3기 수료생이 되었습니다. 아직도 얼떨떨하고 당장 다음 주 화요일에 입사를 하는 것도 신기합니다. 앞으로 현업 개발자가 되어 우테코에서 배운 개념을 잘 적용하여 성장하고 싶습니다. 저에게 많은 도움을 주셨던 크루들 및 코치님들께 정말 감사합니다!",BE
829,"안녕하세요? 제이온입니다. 레벨 3도 이제 하루 밖에 남지 않았습니다. 우리 팀은 최종 스프린트 동안 계획한 기능들을 대부분 구현해서 이번 주는 상대적으로 여유로웠습니다. 저만 그런 건지는 모르겠는데, 덕분에 잘 쉬면서 기존 코드를 유지보수하였습니다. 몇 가지 이슈를 해결한 이야기와 main 브랜치로 develop 브랜치를 통합한 이야기 정도만 작성하려고 합니다. 댓글 통계 기능 이슈 문제 없이 잘 돌아간다고 생각했으나, 한 가지 버그가 생겼습니다. 바로, 사용자가 마지막 일 또는 마지막 달로 날짜를 조회했을 때 해당 마지막 일 또는 마지막 달에 댓글이 없다면 날짜 자체가 표시가 안 되는 것이죠. 정상 결과라면 ""2021-08-01: 0개"" 이런 식으로 나와야 합니다. 이것은 제가 반복문에서 실수가 있었습니다. while (!localDate.equals(end)) {
if (isExistDailyStat(commentStats, localDate)) {
continue;
}
noneMonthCommentStats.add(new CommentStat(localDate.toString(), DEFAULT_COMMENT_COUNT));
localDate = localDate.plusDays(1L);

if (localDate.equals(end) && !isExistDailyStat(commentStats, localDate)) {
noneMonthCommentStats.add(new CommentStat(localDate.toString(), DEFAULT_COMMENT_COUNT));
}
} 이런 식으로 사용자가 시작 날짜로 입력한 localDate와 종료 날짜로 입력한 end를 비교하여 같지 않들 때까지 localDate를 다음 날로 올리는 방식으로 로직을 구현했습니다. 다만, 처음부터 localDate와 end의 값이 같을 수 있으므로 마지막 조건문을 아래로 내려 주었습니다. while (!localDate.equals(end)) {
if (isExistDailyStat(commentStats, localDate)) {
continue;
}
noneMonthCommentStats.add(new CommentStat(localDate.toString(), DEFAULT_COMMENT_COUNT));
localDate = localDate.plusDays(1L);
}
if (!isExistDailyStat(commentStats, localDate)) {
noneMonthCommentStats.add(new CommentStat(localDate.toString(), DEFAULT_COMMENT_COUNT));
} 이런 식으로 마지막 날에는 댓글 데이터가 있는지 없는지 체크하여, 댓글 데이터가 없다면 해당 날짜에는 댓글이 0개있다고 표시하여 리스트에 담는 것입니다. 추가로, 굳이 저렇게 조건문을 뺄 수 밖에 없던 이유는 12월 다음에는 13월이 아니고 1월이 돌아가거나 31일 다음에는 1일로 돌아가는 현상이 있어서 그렇습니다. 운영 브랜치 통합 현재 우리는 develop/be와 develop/fe로 나누어서 개발을 진행하고 있고, 개발용 인프라가 따로 구축이 되어 있는 상태입니다. 그리고 QA도 어느 정도 진행돼서 이제는 운영 브랜치인 main으로 통합할 때가 왔습니다. 참고로 main에는 BE와 FE용 workflows를 따로 정의하고 pr이 날아오면 CI, push할 때는 CD가 작동하도록 만들어 두었습니다. 이때 두 develop 브랜치를 합쳐서 main 브랜치로 pr을 날리기 위하여 release 브랜치를 임시로 생성했습니다. 그리고 release 브랜치는 develop/be로 부터 checkout을 하고 develop/fe를 pull 땡겼습니다. 다만, 여기서 문제가 발생했습니다. develop/be와 develop/fe의 workflows 파일 내용이 달라서 컴플릭트가 생긴 것이죠. 그래서 저는 처음에 어차피 main 브랜치에서 안 쓰는 workflows들이니까 그냥 날려버리고 통합한 뒤 main 브랜치로 pr 날리는 방법을 택했습니다. 하지만, 제리는 나중에 통합할 때마다 컴플릭트가 발생하면 잘못 해결할 수도 있고 번거로우므로, main 브랜치에서 안 쓰는 파일이더라도 develop/be와 develop/fe의 충돌되는 파일 내용을 같게 해주자고 말했습니다. 저는 처음에 필요 없는 파일을 굳이 가질 필요가 있나 생각이 들었지만, 제 의견을 따르게 되었을 때 사이드 이펙트가 꽤 커서 제리의 의견을 따라갔습니다. develop/be의 CI/CD는 젠킨스에서 동작하므로 pr이 merge되었을 때 자동으로 브랜치를 삭제하는 workflow만 develop/fe와 맞춰 주면 되었습니다. name: Delete branch on close pr
on:
pull_request:
types: [ closed ]
branches: [ develop/be, develop/fe ]

jobs:
delete-branch:

runs-on: ubuntu-latest

steps:
- name: delete branch
uses: SvanBoxel/delete-merged-branch@main
env:
GITHUB_TOKEN: ${{ secrets.MY_REPO_PAT }} 꽤나 간단합니다. branches를 develop/be, develop/fe로 맞춰 주는 것이죠. 다음으로, develop/fe의 CI / CD는 깃허브 액션이 작동합니다. 그래서 관련 workflows들이 존재합니다. 이것들도 main 브랜치에는 피해가 가지 않도록 적절히 수정했습니다. 원래는 크루들이 develop/fe가 아니라 실수로 main으로 보내게 되면 CI가 작동하지 않고, develop/fe로 경로를 바꿔도 CI가 작동하지 않는 불편함이 있었는데 이를 'edited' 키워드를 통해 해결했습니다. name: Darass PR Checker [FE]

on:
pull_request:
types: [ opened, edited ]
branches: [ develop/fe ] 이러한 pull_request 타입과 관련된 내용은 이곳에서 확인하실 수 있습니다. 이제 컴플릭트는 발생하지 않습니다. 그래서 마음 놓고 release 브랜치를 develop/be로부터 생성하고, develo/fe로부터 pull 땡기면 됩니다. 마지막으로 release 브랜치를 main 브랜치로 pr을 날리면 정상적으로 BE와 FE에 대해 CI가 작동하고, merge되면 둘다 배포가 됩니다. 정리 이제 정말 운영 서버에도 우리 프로젝트가 배포되었습니다. 아마 조만간 우리 다라쓰 팀에서 만든 댓글 모듈은 제 블로그에도 달아 볼 생각입니다.",BE
830,"안녕하세요? 제이온입니다. 그동안 구현한 내용이 많았는데 바쁘다는 핑계로 기록할 내용을 미뤄버렸습니다. 노션에 학습 부채 기록장이 미루라고 만든 건 아닌데, 이만큼이나 쌓이게 됐네요.. 시간 잘 쪼개서 채워나가야겠습니다. 프로젝트 내의 댓글 조회하는 기능 우리 다라쓰 서비스를 이용하는 유저는 관리자 페이지를 이용할 수 있습니다. 특정 프로젝트 내의 댓글을 조회하며 관리할 수 있고, 댓글에 관한 통계 기능을 제공하자는 계획을 세웠습니다. 먼저, 저번 주 평일 동안은 프로젝트 내의 댓글을 조회하는 기능을 만들기 위해 노력했습니다. 더 디테일하게 말하자면, 특정 프로젝트 안에 있고 시작 날짜와 종료 날짜 사이의 댓글들을 조회할 수 있어야 하는 기능과 댓글 검색 기능을 제공해야 합니다. 우선 전자부터 이야기해 보겠습니다. Page<Comment> findByProjectSecretKeyAndCreatedDateBetween(String projectSecretKey, LocalDateTime startDate,
LocalDateTime endDate, Pageable pageable); 꽤 심플합니다. 위는 CommentRepository 객체인데 메소드 이름으로 충분히 제가 원하는 기능을 구현할 수 있었습니다. 특히 특정 날짜 사이 댓글을 알기 위해서 ""Between"" 키워드를 사용했습니다. 여기서 사용된 요청 모델은 아래와 같습니다. @Getter
@AllArgsConstructor
public class CommentReadRequestInProject {

private String sortOption;

private String projectKey;

@DateTimeFormat(iso = ISO.DATE)
private LocalDate startDate;

@DateTimeFormat(iso = ISO.DATE)
private LocalDate endDate;

private Integer page;

private Integer size;
} 해당 기능을 구현하기 위해 가장 많이 시간을 쓴 곳은 다름아닌 날짜 처리였습니다. 프론트에서 시작 날짜와 종료 날짜를 문자열로 보내는데, 문자열을 그대로 LocalDate로 매핑할 수가 없었습니다. 그래서 구글링을 하다 보니 @DateTimeFormat()을 알게 되었고, ""yyyy-MM-dd"" 형태로 문자열을 LocalDate 형태로 매핑해 줄 수 있었습니다. 다만, 6월 31일 같은 유효하지 않은 날짜를 입력하면 기다란 에러가 발생하는데 이것을 에러 핸들링을 하는 방법을 도무지 알 수가 없었습니다. 이게 다 에러 메시지입니다.. 하하 물론 중반부에 있는 ""default message"" 키워드로 보아 그 부분만 추출해 낼 수 있을 것 같긴합니다. 그래도 메시지 자체가 커스텀으로 바꿀 수가 없었습니다. messages.properties라던지 여러 갖가지 방안을 강구했는데 끝내 실패해서 일단 프론트 측에서 에러 처리를 잘 해달라고만 말해두었습니다. 후자는 검색 기능입니다. 정확히는 프로젝트 내의 존재하고 시작 날짜와 종료 날짜 사이에 있고, 특정 키워드를 내용으로 갖고 있는 댓글을 검색하는 것입니다. 예상했듯이 JPA 메소드에 조건 하나만 넣어주면 끝납니다. Page<Comment> findByProjectSecretKeyAndContentContainingAndCreatedDateBetween(String projectSecretKey,
String keyword, LocalDateTime startDate, LocalDateTime endDate, Pageable pageable); 'XXXContaining'를 사용하면 MySQL에서 like과 동일한 역할을 수행합니다. 요청 모델에서 동일하게 @DateTimeFormat을 사용하여 날짜 매핑을 해 주었습니다. 프로젝트 내의 댓글 통계 기능 금, 토, 일은 여기에 시간을 갈아 넣었습니다. 우리 서비스의 관리자 페이지는 댓글의 통계 기능도 제공합니다. 예를 들어 2021년 8월 1일부터 2021년 8월 9일 까지의 댓글 일별 통계를 알고 싶다고 하면, 각 일자 별로 달린 댓글의 개수가 나옵니다. 일별 통계 외에 시간별과 월별 통계를 제공합니다. MySQL 기준으로는 date() 함수를 통해 날짜 데이터를 연월일로 만들 수 있으며, month() 함수를 통해 날짜 데이터를 연월, 그리고 hour() 함수를 통해 연월일 시간으로 만들 수 있습니다. 하지만, 이상하게도 JPA에서는 해당 날짜 함수를 인식하지 못했습니다. 구글링을 해 보았지만, 저와 비슷한 현상을 겪는 사람들이 많았고 결국 substring()으로 해결하기로 했습니다. @Query(""select substring(c.createdDate, :beginIndex, :length) as date, count(c) as count from Comment c ""
+ ""where c.project.secretKey=:projectSecretKey and c.createdDate between :startDate and :endDate group by date"")
List<Object[]> findDateCount(@Param(""projectSecretKey"") String projectSecretKey,
@Param(""startDate"") LocalDateTime startDate, @Param(""endDate"") LocalDateTime endDate,
@Param(""beginIndex"") Integer beginIndex, @Param(""length"") Integer length); group by를 사용하여 특정 프로젝트 내의 특정 기간 동안 생긴 댓글을 시간별, 일별, 월별 통계로 나타냈습니다. 주의할 점은 JPA에서 substring 함수는 beginIndex부터 endIndex까지 데이터를 자르는 것이 아니라 beginIndex부터 특정 length만큼 데이터를 자른다는 것입니다. 이 방법은 통계를 손쉽게 가져오지만, 한 가지 문제점이 있습니다. 바로 데이터가 존재하는 날짜만 표시한다는 것이죠. 예를 들어, 사용자가 2021년 4월부터 2021년 8월까지의 댓글 월별 통계를 알고 싶습니다. 그런데, 데이터가 8월에만 존재한다면 4월부터 7월까지는 아예 표시가 되지 않고, {""2021-08"": 10}과 같이 데이터가 날아 옵니다. 이것은 그래프로 통계를 내야하는 입장으로서 적합하지 않습니다. 그렇다고 10년치 날짜 더미 데이터 테이블을 만들어서 위 테이블과 join하는 것은 쓸데없는 데이터를 DB에 저장해야해서 마음에 들지 않았습니다. 그래서 어느 정도 애플리케이션에서 로직을 짜기로 했습니다. 우선, 해당 메소드는 추상화되어 있습니다. beginIndex와 length 값에 따라 시간별인지 일별인지 월별인지 정해지기 때문이죠. 그래서 다음과 같이 Repository 단에서 전략 패턴을 사용했습니다. public interface CommentCountStrategy {

boolean isCountable(String period);

List<Stat> calculateCount(String projectKey, LocalDateTime startDate, LocalDateTime endDate);
}

@Component
@RequiredArgsConstructor
public class CommentCountStrategyByDaily implements CommentCountStrategy {

private static final String DATE = ""DAILY"";
private static final Integer BEGIN_INDEX = 1;
private static final Integer LENGTH = 10;

private final CommentRepository commentRepository;

@Override
public boolean isCountable(String periodicity) {
return DATE.equals(periodicity.toUpperCase(Locale.ROOT));
}

@Override
public List<Stat> calculateCount(String projectKey, LocalDateTime startDate, LocalDateTime endDate) {
List<Stat> stats = commentRepository.findDateCount(projectKey, startDate, endDate, BEGIN_INDEX, LENGTH).stream()
.map(objects -> new Stat((String) objects[0], (Long) objects[1]))
.collect(Collectors.toList());

List<Stat> noneDailyStats = getStatByNoneDaily(startDate, endDate, stats);
stats.addAll(noneDailyStats);
stats.sort(Comparator.comparing(Stat::getDate));
return stats;
}

private List<Stat> getStatByNoneDaily(LocalDateTime startDate, LocalDateTime endDate, List<Stat> stats) {
List<Stat> noneMonthStats = new ArrayList<>();
LocalDate start = LocalDate.from(startDate);
LocalDate end = LocalDate.from(endDate);

outer:
for (LocalDate localDate = start; !localDate.equals(end); localDate = localDate.plusDays(1L)) {
for (Stat stat : stats) {
if (localDate.toString().equals(stat.getDate())) {
continue outer;
}
}
noneMonthStats.add(new Stat(localDate.toString(), 0L));
}
return noneMonthStats;
}
}

@Component
@RequiredArgsConstructor
public class CommentCountStrategyByHourly implements CommentCountStrategy {

private static final String DATE = ""HOURLY"";
private static final Integer BEGIN_INDEX = 12;
private static final Integer LENGTH = 2;

private final CommentRepository commentRepository;

@Override
public boolean isCountable(String periodicity) {
return DATE.equals(periodicity.toUpperCase(Locale.ROOT));
}

@Override
public List<Stat> calculateCount(String projectKey, LocalDateTime startDate, LocalDateTime endDate) {
List<Stat> stats = commentRepository.findDateCount(projectKey, startDate, endDate, BEGIN_INDEX, LENGTH).stream()
.map(objects -> new Stat(String.valueOf(Integer.parseInt(String.valueOf(objects[0]))), (Long) objects[1]))
.collect(Collectors.toList());

List<Stat> noneHourlyStats = getStatByNoneHourly(startDate, endDate, stats);
stats.addAll(noneHourlyStats);
stats.sort(Comparator.comparingInt(s -> Integer.parseInt(s.getDate())));
return stats;
}

private List<Stat> getStatByNoneHourly(LocalDateTime startDate, LocalDateTime endDate, List<Stat> stats) {
List<Stat> noneHourlyStats = new ArrayList<>();

outer:
for (int localTime = LocalTime.MIN.getHour(); localTime <= LocalTime.MAX.getHour(); localTime++) {
for (Stat stat : stats) {
if (localTime == Integer.parseInt(stat.getDate())) {
continue outer;
}
}
noneHourlyStats.add(new Stat(String.valueOf(localTime), 0L));
}
return noneHourlyStats;
}
}

@Component
@RequiredArgsConstructor
public class CommentCountStrategyByMonthly implements CommentCountStrategy {

private static final String DATE = ""MONTHLY"";
private static final Integer BEGIN_INDEX = 1;
private static final Integer LENGTH = 7;

private final CommentRepository commentRepository;

@Override
public boolean isCountable(String periodicity) {
return DATE.equals(periodicity.toUpperCase(Locale.ROOT));
}

@Override
public List<Stat> calculateCount(String projectKey, LocalDateTime startDate, LocalDateTime endDate) {
List<Stat> stats = commentRepository.findDateCount(projectKey, startDate, endDate, BEGIN_INDEX, LENGTH).stream()
.map(objects -> new Stat((String) objects[0], (Long) objects[1]))
.collect(Collectors.toList());

List<Stat> noneMonthStats = getStatByNoneMonth(startDate, endDate, stats);
stats.addAll(noneMonthStats);
stats.sort(Comparator.comparing(Stat::getDate));
return stats;
}

private List<Stat> getStatByNoneMonth(LocalDateTime startDate, LocalDateTime endDate, List<Stat> stats) {
List<Stat> noneMonthStats = new ArrayList<>();
YearMonth startYearMonth = YearMonth.from(startDate);
YearMonth endYearMonth = YearMonth.from(endDate);

outer:
for (YearMonth yearMonth = startYearMonth; !yearMonth.equals(endYearMonth); yearMonth = yearMonth.plusMonths(1L)) {
for (Stat stat : stats) {
if (yearMonth.toString().equals(stat.getDate())) {
continue outer;
}
}
noneMonthStats.add(new Stat(yearMonth.toString(), 0L));
}
return noneMonthStats;
}
} 코드가 길긴 한데 중요한 것은 하나입니다. JPA에서 얻어지는 날짜 데이터 외에 데이터가 없는 달은 0개로 만들어서 리스트에 추가한다는 것입니다. 여기서 Stat 객체는 날짜 필드와 카운트 필드를 갖고 있습니다. 정리 기능 하나를 구현하기 위해서 새로 알게 된 개념이 참 많은 것 같습니다. 지금은 바빠서 더 깊은 학습은 하지 못했으나, 이번 주는 기능 구현보다는 유지 보수에 초점을 맞추기로 해서 차근 차근 정리해 나가야겠습니다.",BE
858,"이전 포스팅: nGrinder와 Pinpoint를 이용한 성능 / 부하 테스트1 - 테스트 계획 nGrinder와 Pinpoint를 이용한 성능 / 부하 테스트2 - 시나리오 작성 nGrinder와 Pinpoint를 이용한 성능 / 부하 테스트3 - Smoke 테스트 & Load 테스트 이 포스팅은 개선 방법에 대한 자세한 내용은 다루지 않고, 개선 내용과 개선 결과에 대해서만 다룹니다. 📝 개선 전 결과 최대 트래픽의 load test의 경우입니다. 🌊 Connection pool 조정 최대 트래픽 load test 시 모든 요청이 목표 MTT(50 ~ 100ms)를 넘어버렸습니다. 이 문제는 connection 문제였습니다. VUser가 22명이었기 때문에, 넉넉잡아 connection pool을 30정도로 늘려봤습니다. 그 후 다시 load test를 돌렸습니다. 그래프만 봐도 일단 시나리오 전체 평균값이긴 하지만 MTT가 많이 줄어든 것을 볼 수 있습니다. pinpoint를 한번 살펴보겠습니다. 전반적인 요청들은 성능 향상이 있는게 보입니다만 가장 문제였던 리뷰어 목록을 조회하는 요청은 오히려 시간이 더 올라갔네요 흠.. 조회쿼리 개선을 하면 나아질 거라 생각합니다. 우선 connection pool을 조정하면서 얻으려고 했던 이점인 getConnection 비용은 잘 없어진 걸 볼 수 있습니다. 🛠 조회쿼리 개선 1) 로그인 조회 쿼리 로그인 조회 쿼리는 where 절에 oauth_id 칼럼을 사용하여 member를 찾고 있습니다. 따라서 oauth_id 칼럼에 인덱스를 걸어주었습니다. 2) 리뷰어 목록 조회 쿼리 리뷰어 목록 조회 쿼리 이 녀석이 가장 문제입니다. 이 쿼리는 페이지네이션이 적용되어 있습니다. 전체 페이지 수를 알기위한 count 쿼리, 조건에 맞는 리뷰어 목록을 가져오는 쿼리 이 2가지 쿼리를 개선해야했습니다. 인덱스를 걸어 해결해보려 하였으나 잘 적용되지 않았습니다. 따라서 join문을 줄여보기로 했습니다. 기존에는 필터링 조건이 있든 없든 무조건 여러 테이블이 join 되면서 문제가 발생하였습니다. 이를 필터링 조건이 추가될 경우만 join을 추가해주는 식으로 변경하였습니다. 조회 쿼리를 개선하고 나니 평균 테스트 시간이 많이 줄어들었습니다. 리뷰어 목록 요청은 여전히 목표치에는 도달하지 못했습니다. pinpoint도 살펴보겠습니다. 아직 리뷰어 조회쿼리는 오래걸리지만, 그래도 이전에 비하면 훨씬 좋아진 것을 볼 수 있습니다. 나머지는 모두 목표치에 도달했습니다. ➗ DB replication 하나의 DB를 사용할때에 비해 부하를 줄여보기 위해, DB replication을 진행해보고 테스트를 해보기로 했습니다. 조회쿼리를 담당하는 slave db를 2개 만들어 주었습니다. (각 slave의 connection pool은 15개씩으로 바꿔주었습니다.) [DB, Spring] Replication 적용하기 이전보다 좋은 성능을 볼 수 있습니다. pinpoint를 한 번 보겠습니다. 위의 동그라미 친 부분은 모두 리뷰어 목록 조회 요청입니다. 이전보다 확실히 빨라지긴 했는데, 양극화가 되어 있습니다. 각각 요청을 살펴보니 확실히 차이가 납니다. DataSource를 살펴보니 slave1과 slave2가 골고루 쓰이지 않은 것을 볼 수 있습니다. 하나의 slave로 리뷰어 목록 조회 요청이 쏠리게 되어 해당 slave에서 처리하는 요청이 상대적으로 오래걸린 것으로 추측됩니다. 나중에 한 번 방법을 생각해봐야겠습니다.🤔 ✍🏻 느낀점 결국은 리뷰어 목록 조회 요청은 목표까지 줄이지 못했습니다.😭😭 후에 더 공부한 후에 다시 쿼리 개선을 해보고 싶습니다. 이번에 처음 성능 테스트를 해보면서 제대로 한 것인가 라는 생각이 많이 들었습니다. 지금 제가 이해한것도 틀릴 수도 있다고 생각합니다. 틀린 부분이 있다면 알려주시면 감사하겠습니다.🙏 추가적으로 아래의 생각을 하게 되었습니다. 1) 시나리오는 짧게 가져가거나, api 별로 해보는 게 좋을 것 같다. 2) 페이지네이션이 포함된 요청이 있는 경우 실제로 있을 법한 범위를 잡는 게 좋을 것 같다. 1)의 이유는 시나리오를 길게 잡으니 오래 걸리는 요청에 의해 TPS 값이 api에 상관없이 모두 똑같이 나와 별 의미없는 지표가 되어버렸다는 생각이 들어서 입니다. 어차피 어디서 오래걸리는지를 파악하기 위해서는 각 api가 얼마나 걸리는지를 확인해보는게 더 좋을 것 같습니다. 2)의 이유는 이번에 페이지 30000을 잡아서 테스트를 해봤는데, 실제로 페이지 30000 요청이 있을 것인지에 대한 의문이 들었습니다. 어느 정도 예상 가능한 수치로 테스트 해보는게 더 좋을 것 같습니다. reference 2. 페이징 성능 개선하기 - 커버링 인덱스 사용하기",BE
859,"이전 포스팅: nGrinder와 Pinpoint를 이용한 성능 / 부하 테스트1 - 테스트 계획 nGrinder와 Pinpoint를 이용한 성능 / 부하 테스트2 - 시나리오 작성 테스트 범위 데이터 개수 각 테이블 당 20만개 🔎 사전 지식 nGrinder를 통해 테스트를 진행하면 아래와 같은 형식의 그래프를 결과값으로 볼 수 있습니다. nGrinder의 TPS는 Test Per Seconds의 약자로 초당 몇 번의 테스트가 일어났는지를 뜻합니다. 위의 그래프 중 TPS 그래프는 초당 몇번의 테스트가 일어났는지를 나타냅니다. 이상적으로 TPS는 VUser가 증가하는 것에 비례해서 늘어납니다. 그러다가 어느 시점에 TPS가 더 이상 증가하지 않게 되는데 이 지점에 도달하는 순간 사용자의 응답시간에 영향을 미칩니다.(이 지점에서 오히려 TPS가 떨어질 수도 있는데 이 경우 서버 튜닝이 제대로 되지 않은 것입니다.) 처리해야할 요청은 늘어나는데 처리량은 멈춰있기 때문입니다. 이런경우 scale up이나 scale out으로 TPS를 증가시킬 수 있습니다. 두번째 그래프인 평균 테스트 시간 그래프는 한 번의 요청부터 응답까지 몇 초가 걸렸는지를 나타냅니다. 이 두 가지 요소를 중점적으로 살펴보면 됩니다. 하지만 이 그래프에는 문제가 하나 있습니다. 위의 그래프들은 시나리오에 포함된 모든 요청에 대한 평균값을 나타냅니다. 따라서 제 시나리오를 예시로 들면 시나리오 안에 총 7개의 요청이 있기 때문에 7개 요청의 평균값을 그래프로 나타낸 것입니다. 따라서 특정 요청에 대한 결과를 보기는 힘듭니다. 이는 로그 파일이나 CSV 파일을 통해 파악할 수 있습니다. 로그 파일 & CSV 파일 활용하기 로그 파일은 테스트가 끝난 후 확인할 수 있습니다. 좀 더 자세한 정보는 CSV 파일로 확인할 수 있습니다. CSV파일은 상세보고서를 클릭 하신 후, 우측 상단의 CSV 다운로드를 누르시면 다운받을 수 있습니다. CSV 파일은 각 가로로 각 요청당 결과값을 볼 수 있습니다. Description부터 다음 요청의 Description 전까지가 해당 요청의 결과값입니다. TPS-${요청번호}, Mean_Test_Time_(ms)-${요청번호}와 같은 형식으로 해당 요청의 결과를 볼 수 있습니다. CSV의 세로는 시간을 나타냅니다. Pinpoint 활용하기 각 요청에 대한 처리 시간은 Pinpoint를 활용하여 자세히 확인할 수 있습니다. 부하 테스트를 진행한 시간대를 설정해주면 아래와 같이 시간대에 따른 요청을 초록색 점으로 나타냅니다. 하나의 점이 하나의 요청입니다. 초록색 점들을 드래그 해보면 각 요청에 대한 세부 정보(각 메서드 당 실행 시간 등)를 확인할 수 있습니다. 📈 Smoke Test 최소한의 부하를 견딜 수 있는지 smoke 테스트를 먼저 진행해보겠습니다. 최소한의 부하이므로 테스트 시간은 10분 정도만 진행하겠습니다. 스크립트는 이전 포스팅에서 작성한 스크립트를 이용합니다. VUser: 2 테스트 시간: 10분 목표는 아래와 같습니다. TPS(Throughput): 11.3 ~ 34 이상 MTT(Latency): 50 ~ 100ms 이하 결과 (앞 부분 그래프는 제가 예열을 하지 않고 바로 돌려서 발생한 부분 같습니다ㅎㅎ.. 10분 경부터 보시면 될 것 같습니다.) TPS는 데이터의 개수가 많아서인지 VUser가 2명 밖에 안되는데 생각보다 낮게 나왔습니다. WAS에서 병목이 일어난건지, DB에서 병목이 일어난건지 잘 모르겠지만 데이터의 수가 많기 때문에 DB쪽이 아닐까 추측이 듭니다. 데이터 수가 많아서 테스트 요청 자체도 적게 가고 따라서 TPS 값이 낮게 나온다고 생각이 들어 DB Replication 등 DB 관련 최적화를 진행해보고 다시 측정해보겠습니다. TPS는 분명 각 요청 마다 처리량이 다를텐데 왜 모든 요청이 같게 나왔을까요? 시나리오는 1번부터 7번까지 차례대로 수행되는데, 중간에 오래걸리는 요청이 있는 경우 다른 요청이 빨리 끝나는 요청이라고 해도, 한 묶음으로 수행되므로 테스트 자체의 수가 적게 발생하여 모든 요청의 TPS가 같게 나온 것입니다.. 그래서 TPS 지표가 크게 의미가 없게 되었습니다.. 제가 이렇게 테스트를 해보면서 느낀점이 시나리오 자체를 짧게 가져가거나, 아니면 각각 API 별로 테스트해보는게 더 효과적일 것 같다는 생각을 하게되었습니다.(아마 다음에 성능 테스트를 할 땐 그렇게 할 것 같네요 😭) MTT를 위주로 살펴보겠습니다. 다른 요청들은 괜찮은데, 3번째 요청인 리뷰어 목록을 조회하는 요청의 시간이 많이 걸리는 것을 볼 수 있습니다. Pinpoint도 한 번 살펴보겠습니다. 대부분의 요청은 100ms 아래로 보여지는데, 특정 요청들이 2000ms 정도 걸리는 것을 볼 수 있습니다. 위의 로그 파일에서 살펴본 리뷰어 목록을 조회하는 요청이라고 추측됩니다. 드래그를 해서 한 번 살펴보겠습니다. TeacherFilterRepository 클래스의 findAll()메서드에서 시간이 오래걸리는 것을 파악할 수 있습니다. 이 조회 쿼리를 개선해야 해당 요청의 성능이 좋아질 것이라는 것을 알 수 있습니다. TODO findAll 조회 쿼리 개선 📈 Load Test 평균 트래픽일 경우와 최대 트래픽일 경우를 나눠서 테스트해보겠습니다. 테스트 시간은 30분으로 진행합니다. 평균 트래픽 VUser: 7 테스트 시간: 30분 최대 트래픽 VUser: 22 테스트 시간: 30분 두 테스트 모두 목표는 아래와 같습니다. TPS: 11.3 ~ 34 이상 MTT: 50 ~ 100ms 이하 성능 유지 기간: 30분 평균 트래픽 결과 이미 smoke 테스트에서도 문제가 있었던 리뷰어 목록을 조회하는 요청이 엄청난 시간이 걸리게 되었고, 로그인 요청도 꽤 시간이 늘어났습니다. Pinpoint로도 살펴보겠습니다. TeacherFilterRepository 클래스의 findAll()메서드의 문제가 심각하다는 걸 알 수 있습니다. 로그인 과정에서 일어나는 조회쿼리 또한 개선이 필요해보입니다. 최대 트래픽 결과 (이유는 모르겠지만 요약 로그 파일도 따로 나오지 않았네요.) Pinpoint를 살펴보겠습니다. 운영 중 최대 트래픽(가정)일 경우 전반적으로 모든 요청의 시간이 목표를 오버해버렸습니다.. 🤦🏻‍♂️ 당연히 이미 문제가 되었던 쿼리는 더 성능이 안좋아졌고, 새로운 문제도 찾을 수 있었습니다. connection 과정에서 많은 비용이 소모되는 걸 볼 수 있었습니다. 다른 요청들도 모두 시간이 늘어났으므로 모두 확인해보니 findAll과 login을 제외한 요청은 전부 connection에서 비용이 확 증가한 것이었습니다. connection pool도 조정해야겠습니다. TODO findAll 조회 쿼리 개선 login 내부 조회 쿼리 개선 connection pool 조정 🛠 개선 목표 smoke 테스트와 load 테스트를 통해 개선해 나갈 방향을 잡아보았습니다. connection pool 조정 findAll 조회 쿼리 개선 login 내부 조회 쿼리 개선 DB replication 적용 다음 포스팅에서는 개선을 통해 smoke, load 테스트 결과가 어떻게 바뀌는지 알아보겠습니다. 개선을 통해 어느정도 만족할 만한 결과가 나온다면 stress 테스트를 통해 한계점을 찾아보겠습니다. Reference 우아한테크코스 TPS 지표 이해하기",BE
860,"이전 포스팅: nGrinder와 Pinpoint를 이용한 성능 / 부하 테스트1 - 테스트 계획 📝 테스트 스트립트 작성 테스트를 진행하기 위해서는 먼저 시나리오 기반으로 스크립트를 작성해야합니다. nGrinder는 Groovy나 Jython를 지원합니다. Grinder 이용 nGrinder는 Grinder라는 오픈소스 기반에서 개발되었습니다. Grinder의 TCPProxy를 이용해서 직접 스크립트를 작성하는 것이 아닌 사용자의 인터넷 요청 액션을 recording하여 자동으로 스크립트를 만들 수 있습니다. nGrinder에서도 제공했던 기능이라고 하는데 현재는 안된다고 하네요. 저는 MAC 기준으로 진행하겠습니다 1) 먼저 Grinder를 다운 받고 압축을 풀어줍니다. https://sourceforge.net/projects/grinder/ 2) Firefox 프록시 설정하기 Firefox의 설정에 들어간 후, 프록시를 검색해줍니다. 그리고 수동 프록시 설정을 아래와 같이 127.0.0.1로 해줍니다. 3) Grinder 압축을 푼 폴더가 있는 경로에서 아래의 명령어를 입력합니다. java -classpath ./grinder-3.11/lib/grinder.jar net.grinder.TCPProxy -console -http > ${스크립트 이름}

// example
java -classpath ./grinder-3.11/lib/grinder.jar net.grinder.TCPProxy -console -http > grinder.py 이때 주의할 점이 저는 java11을 사용중이었는데 자바 버전 오류가 났습니다. 자바 버전을 1.8로 바꿔주니까 정상적으로 작동했습니다. 이렇게 아래와 같이 TCPProxy Console이 뜨면 성공입니다. 4) 설정해 준 Firefox를 이용해서 원하는 인터넷 액션을 수행하고 TCPProxy의 stop를 누릅니다. 5) 스크립트가 생성된 것을 확인할 수 있습니다. 이 스크립트를 nGrinder에 올려서 사용할 수 있습니다. 그러나 이 방식은 Jython으로 생성되어 저 같은 경우는 파악하기가 좀 힘들었고, WAS만 테스트 하고 싶었는데, .html, .js와 같은 요청까지 모두 들어가게 되는 단점이 있었습니다. 그래서 이번에는 Groovy를 이용하여 직접 스크립트를 작성하는 방법을 사용했습니다. 직접 작성 1) nGrinder controller 웹에 접속하여 스크립트 만들기를 클릭한다. 2) 그럼 예시 스크립트가 나옵니다. 이를 커스텀해서 사용하면 됩니다. 자바를 사용하셨다면 생긴 것이 비슷하여 나름 쉽게 작성하실 수 있습니다. 동작 방식이나, 문법 등은 구글신에게.. 예시) 제가 아래의 시나리오를 테스트하기 위해 만든 스크립트를 첨부합니다. 로그인 - 언어 기술 목록 조회 - 리뷰어 목록 조회 - 리뷰어 단일 조회 - 내가 받은 리뷰 목록 조회 - 내가 리뷰한 리뷰 목록 조회 - 리뷰 상세 조회 @RunWith(GrinderRunner)
class TestRunner {
public static GTest test1
public static GTest test2
public static GTest test3
public static GTest test4
public static GTest test5
public static GTest test6
public static GTest test7

public static HTTPRequest request
public static Map<String, String> headers = [:]
public static Map<String, Object> params = [:]
public static List<Cookie> cookies = []

@BeforeProcess
public static void beforeProcess() {
HTTPRequestControl.setConnectionTimeout(300000)

test1 = new GTest(1, ""GET /login/oauth?providerName={provider}&code={code}"")
test2 = new GTest(2, ""GET /languages"")
test3 = new GTest(3, ""GET /teachers?language=java"")
test4 = new GTest(4, ""GET /teachers/{id}"")
test5 = new GTest(5, ""GET /reviews/teacher/{id}"")
test6 = new GTest(6, ""GET /reviews/student/{id} with token"")
test7 = new GTest(7, ""GET /reviews/{id}"")

request = new HTTPRequest()

grinder.logger.info(""before process."")
}

@BeforeThread
public void beforeThread() {
test1.record(this, ""test1"")
test2.record(this, ""test2"")
test3.record(this, ""test3"")
test4.record(this, ""test4"")
test5.record(this, ""test5"")
test6.record(this, ""test6"")
test7.record(this, ""test7"")

grinder.statistics.delayReports = true
grinder.logger.info(""before thread."")
}

private String accessToken
private String userId = ""1""

@Test
public void test1() {
def slurper = new JsonSlurper()
def toJSON = { slurper.parseText(it) }

HTTPResponse response = request.GET(""http://3.36.68.56:8080/login/oauth?providerName=github&code=a"", params)

def result = response.getBody(toJSON);

accessToken = result.accessToken
userId = result.id.toString()

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}

@Test
public void test2() {
HTTPResponse response = request.GET(""http://3.36.68.56:8080/languages"", params)

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}

@Test
public void test3() {
HTTPResponse response = request.GET(""http://3.36.68.56:8080/teachers?language=java&page=30000"", params)

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}

@Test
public void test4() {
HTTPResponse response = request.GET(""http://3.36.68.56:8080/teachers/2"", params)

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}

@Test
public void test5() {
HTTPResponse response = request.GET(""http://3.36.68.56:8080/reviews/teacher/"" + userId)

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}

@Test
public void test6() {
headers[""Authorization""] = ""Bearer "" + accessToken
request.setHeaders(headers)
HTTPResponse response = request.GET(""http://3.36.68.56:8080/reviews/student/"" + userId)

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}

@Test
public void test7() {
HTTPResponse response = request.GET(""http://3.36.68.56:8080/reviews/1"", params)

if (response.statusCode == 301 || response.statusCode == 302) {
grinder.logger.warn(""Warning. The response may not be correct. The response code was {}."", response.statusCode)
} else {
assertThat(response.statusCode, is(200))
}
}
}
이때 주의할 점이 있습니다. 저와 같이 하나의 스크립트에 여러 요청이 포함된 경우. 각 테스트의 이름을 test1, test2 이런식으로 해주시는 것이 좋습니다. 그 이유는 제가 해본 결과 각 테스트가 알파벳 순으로 실행되기 때문입니다. 참고로 테스트의 순서에 따라 공유된 변수를 가져다 쓸 수 있습니다.(위의 accessToken, userId 참고) 3) 스크립트 작성 완료 후 우측 상단의 검증을 눌러 Error가 발생하지 않는다면 스크립트 작성이 완료됩니다. 이번 포스팅에서는 시나리오 스크립트를 작성해보았습니다. 다음 포스팅에서는 실제 테스트를 진행하면서 개선해가는 과정을 포스팅하려고 합니다.",BE
861,"이 포스팅에서는 nGrinder설치와 Pinpoint설치 과정에 대해서는 다루지 않습니다! 처음 진행해보는 부하테스트이므로 잘못된 부분이 있을 수 있습니다. 혹시 그런부분이 있다면 알려주시면 감사하겠습니다🙏 우아한테크코스 과정중에 누구나 코드리뷰를 받을 수 있도록 리뷰어와 리뷰이를 매칭해주는 코드리뷰 플랫폼인 👀코드봐줘 서비스를 개발하고 있습니다. 누구나 코드리뷰를 받을 수 있는 👀 코드봐줘 (GitHub: https://github.com/woowacourse-teams/2021-drop-the-code) 일정에 맞게 계획했던 기능들은 거의 구현이 된 상태여서(아직 필요한 기능들이 많지만..) 서비스의 성능을 파악할 필요가 있었습니다. 이번 포스팅에서는 nGrinder와 Pinpoint를 사용하여 코드봐줘 서비스의 성능/부하테스트를 진행하여 분석하고 이를 토대로 개선해가는 과정을 기록하려 합니다. ❓왜 nGrinder & Pinpoint? 성능/부하테스트 도구로는 nGrinder, k6, Apache JMeter, Gatling, Locust 등 여러 가지가 있습니다. 제대로 된 테스트를 위해서는 아래의 3가지 기능을 지원하는게 좋습니다. 시나리오 기반의 테스트가 가능해야 합니다. 동시 접속자 수, 요청 간격, 최대 Throughput 등 부하를 조정할 수 있어야 합니다. 부하 테스트 서버 스케일 아웃을 지원하는 등 충분한 부하를 줄 수 있어야 합니다. 저는 그중에 nGrinder를 선택했는데요. 그 이유는 다음과 같습니다. 위의 3가지 기능 제공(스크립트를 통한 시나리오 작성 가능, 부하 조정 가능, Agent를 이용하여 충분한 부하를 줄 수 있음) 한글 지원 groovy문법 지원 사용하기 편함 추가적으로 모니터링을 위해 Pinpoint를 사용했습니다. 현재 인프라 구조, 요청수를 한 눈에 볼 수 있음 요청에 대한 성공/실패, 응답 시간을 그래프로 볼 수 있음 콜스택을 이용하여 병목 지점을 쉽게 확인할 수 있음 📝 테스트 계획 바로 테스트에 들어가기 전에 테스트 계획부터 세워야합니다. 부하테스트는 시스템의 응답 성능과 한계치를 파악하기 위한 테스트입니다. 서비스가 어느 트래픽까지 처리할 수 있는지(어느 정도의 부하를 견디는지) → 어떻게 대응할지 알 수 있음 병목이 발생하는 부분이 어디인지 → 어떻게 개선할지 알 수 있음 이를 파악하기 위한 지표는 아래와 같습니다. Users: 동시에 사용할 수 있는 사람 TPS: 일정 시간 동안 얼마나 처리할 수 있는지 한 명만 사용해도 처리량이 좋지 않으면 → scale up 부하 증가시 문제 → scale out Time: 얼마나 빠른지 테스트 환경 성능/부하 테스트는 실제 사용자가 접속하는 환경과 유사해야합니다. 따라서 동일하게 AWS를 이용하여 인프라를 구성했습니다. 또 데이터의 양도 동일해야합니다. 하지만 코드봐줘는 현재 사용량이 거의 없기 때문에 임의로 많은 데이터를 넣어서 진행합니다. OAuth 로그인과 같이 외부와 소통하는 로직이 포함된 경우 별도의 서버를 구성해야합니다. 객체를 Mocking하는 경우 Http Connection Pool, Connection Thread 등을 미사용하게 되고 IO가 발생하지 않습니다. 같은 애플리케이션에 Dummy Controller를 구성하는 경우 테스트 시스템의 자원과 리소스를 같이 사용하므로 테스트의 신뢰성이 떨어집니다. 코드봐줘는 Github OAuth 로그인이 포함되어 있으므로 별도의 Fake OAuth 서버를 만들어 진행합니다. 전제조건 먼저 전제조건을 정해야합니다. 테스트하려는 Target 시스템의 범위를 정해야 합니다. 부하 테스트시에 저장될 데이터 건수와 크기를 결정하세요. 서비스 이용자 수, 사용자의 행동 패턴, 사용 기간 등을 고려하여 계산합니다. 목푯값에 대한 성능 유지기간을 정해야 합니다. 서버에 같이 동작하고 있는 다른 시스템, 제약 사항 등을 파악합니다. 1) Target 시스템의 범위 2가지 범위로 테스트를 해보려고 합니다. Was부터 시작하는 범위 Reverse Proxy를 포함한 범위 2) 데이터 수 아직 개발중인 서비스이기 때문에 서비스 이용자 수, 사용자의 행동 패턴, 사용 기간 등의 정보가 없습니다. 따라서 이번에는 임의로 데이터 수를 정하고 테스트를 진행하려합니다. 코드봐줘 서비스의 주요 기능은 리뷰어 조회, 리뷰 요청입니다. 따라서 해당 기능에서 사용되는 member, teacher_profile, reivew, teacher_language, teacher_skill 테이블에 각각 20만개의 초기 더미 데이터를 넣고 진행합니다. 3) 목푯값에 대한 성능 유지기간 주요 기능의 성능이 30분 동안 유지되는 것을 목표로 테스트 & 개선해보려고 합니다. 4) 제약 사항 딱히 서버에 같이 동작하고 있는 다른 시스템, 제약 사항 등이 없으므로 넘어가겠습니다. 테스트 종류 크게 테스트는 3종류가 있습니다. 1) Smoke 테스트 VUser: 1 ~ 2 최소의 부하로 시나리오를 검증해봅니다. 2) Load 테스트 평소 트래픽과 최대 트래픽일 때 VUser를 계산 후 시나리오를 검증해봅니다. 결과에 따라 개선해보면서 테스트를 반복합니다. 3) Stress 테스트 최대 사용자 혹은 최대 처리량인 경우의 한계점을 확인하는 테스트입니다. 점진적으로 부하를 증가시켜봅니다. 테스트 이후 시스템이 수동 개입 없이 자동 복구되는지 확인해봅니다. 성능 목표 설정 성능 목표를 정하기 위해서는 VUser를 구해야합니다.(Load 테스트인 경우) VUser를 구하기 위해서는 아래와 같은 지표들이 필요합니다. DAU(일일 활동 사용자 수) 피크시간대 집중률(최대 트래픽 / 평소 트래픽) 1명당 1일 평균 요청 수 코드봐줘의 경우 실제 사용자가 거의 없기 때문에 정확하지는 않지만 similarweb을 이용해 비슷한 서비스의 트래픽을 보고 지표들을 가정하여 진행해보려고 합니다.(사이트 주소를 입력하시면 간단한 트래픽 정보를 볼 수 있습니다.) 위 사이트는 한달에 약 210만명이 방문하고 있습니다. 따라서 정확하지는 않겠지만 DAU를 210만/30(한달)로 계산하여 7만명으로 잡고 계산하겠습니다. 피크시간대 집중률은 저녁 시간대에 늘어날 것으로 예상은 되지만, 최대 트래픽과 평소 트래픽의 차이가 크게는 없을 것이라고 생각하여 3배로, 1명당 1일 평균 요청 수는 14정도로 가정하고 진행하겠습니다. DAU(일일 활동 사용자 수): 70000 피크시간대 집중률(최대 트래픽 / 평소 트래픽): 3 1명당 1일 평균 요청 수: 14 위의 정보를 가지고 throughput(1일 평균 rps ~ 1일 최대 rps)를 계산할 수 있습니다. DAU * 1명당 1일 평균 접속수 = 1일 총 접속수 → 70000 * 14 = 980000 1일 총 접속수 / 86400(초 / 일) = 1일 평균 rps → 980000 / 86400 = 약 11.3 1일 평균 rps * (최대 트래픽 / 평소 트래픽) = 1일 최대 rps → 11.3 * 3 = 약 34 즉 throughput은 11.3(1일 평균 rps) ~ 34(1일 최대 rps)가 나오게 됩니다. 이 정보를 통해서 VUser를 계산할 수 있습니다. VUser: (목표 rps * T) / R R: 시나리오에 포함된 요청의 수 T: 시나리오 완료 시간보다 큰 값(VUser 반복을 완료하는데 필요한 시간보다 큰 값) T = (R * 왕복시간(http_req_duration)) + 지연시간(내부망일 경우 추가하기! - 외부에서 처리되는 시간을 보정해주기 위함!) 왕복 시간 = 얼마 안에 끝나야 하는지 이번에 테스트해 볼 시나리오에 포함된 요청의 수는 7, http_req_duration을 0.5s, 지연시간을 1s로 설정한다면 아래와 같은 결과가 도출됩니다. R = 7 T = (7 * 0.5) + 1 = 4.5 평소 트래픽 VUser: (11.3 * 4.5) / 8 = 약 7 최대 트래픽 VUser: (34 * 4.5) / 8 = 약 22 보통 VUser(active user)가 80정도면 헤비한 트래픽이 발생하는 것이라고 생각한다고 합니다. 보통은 10 ~ 15명 정도로도 충분하다고 합니다. 테스트 목표 구한 값을 토대로 각 테스트별 목표를 정리해보겠습니다. 1) Smoke 테스트 VUser: 1 ~ 2 Throughput: 11.3 ~ 34 이상 Latency: 50 ~ 100ms 이하 2) Load 테스트 평소 트래픽 VUser: 7 최대 트래픽 VUser: 22 Throughput: 11.3 ~ 34 이상 Latency: 50 ~ 100ms 이하 성능 유지 기간: 30분 3) Stress 테스트 VUser: 점진적으로 증가시켜보기 시나리오 시나리오 대상은 접속 빈도 높거나 서버 리소스 소비량이 많거나 DB를 사용하는 기능을 선택해보는게 좋습니다. 코드봐줘의 경우 가장 접속 빈도가 높으면서 DB를 많이 사용하는 부분은 리뷰어 조회, 리뷰 조회일 것입니다. 이번에는 전체적인 조회에 대한 시나리오를 진행하려고 합니다. 1) 시나리오1 로그인 - 언어 기술 목록 조회 - 리뷰어 목록 조회 - 리뷰어 단일 조회 - 내가 받은 리뷰 목록 조회 - 내가 리뷰한 리뷰 목록 조회 - 리뷰 상세 조회 추후에 CUD관련 부분도 테스트 해보려고 합니다. 2) 시나리오2 로그인 - 언어 기술 목록 조회 - 리뷰어 목록 조회 - 리뷰어 선택 - 리뷰 요청 테스트 계획 템플렛 퀄리티는 구리지만 구글 스프레드 시트로 테스트 계획 템플렛을 만들어봤습니다. 🔎 테스트 계획 템플렛 🌱nGrinder & Pinpoint 환경 nGrinder nGrinder는 Controller와 Agent로 나눠져 있습니다. Controller 부하테스트를 위한 웹 인터페이스 제공 테스트 스크립트 작성 테스트 결과를 수집하여 통계로 보여줌 Agent Controller의 명령을 받아 실제로 target에 부하를 발생시킴 CPU, memory 모니터링도 가능 저는 하나의 Controller와 3개의 Agent를 둬서 테스트를 진행합니다. Pinpoint Pinpoint 구성환경입니다. 숫자는 요청 수를 나타냅니다. 이번 포스팅에서는 테스트 계획을 세워봤습니다. 다음 포스팅에서는 시나리오를 바탕으로 스크립트를 작성해보겠습니다. Reference 우아한테크코스 스프링캠프 2015:[B-2]: 내가 써본 nGrinder",BE
877,"코드리뷰 매칭 플랫폼 개발 중 알림 기능이 필요했다. 리뷰어 입장에서는 새로운 리뷰 요청이 생겼을 때 모든 리뷰가 끝나고 리뷰이의 피드백이 도착했을 때 리뷰이 입장에서는 리뷰 요청이 거절되었을 때 리뷰 요청이 수락되었을 때 리뷰어가 리뷰를 완료했을 때 공통적으로 새로운 채팅이 있을 때 위의 경우 알림을 보내줘야한다고 생각했다. 로그인한 상태가 아니라면 로그인 했을 때 받은 알림을 모두 보여주면 되지만, 로그인한 상태라면 실시간으로 알림을 받길 원했다. 이 실시간 알림 기능을 구현하기 위해 SSE(Server Sent Events)를 이용했다. 📂 고려 기술 실시간 웹 애플리케이션 개발 시 사용되는 몇 가지 방법이 있다. polling(client pull) 클라이언트가 일정한 주기로 서버에 업데이트 요청을 보내는 방법. 지속적인 HTTP 요청이 발생하기 때문에 리소스 낭비가 발생한다. websocket(server push) 실시간 양방향 통신을 위한 스펙으로 서버와 브라우저가 지속적으로 연결된 TCP라인을 통해 실시간으로 데이터를 주고받을 수 있도록 하는 HTML5 사양이다. 연결지향 양방향 전이중 통신이 가능하며 채팅, 게임, 주식 차트 등에 사용된다. polling은 주기적으로 HTTP 요청을 수행하지만, websocket은 연결을 유지하여 서버와 클라이언트 간 양방향 통신이 가능하다. SSE(server push) 이벤트가 [서버 -> 클라이언트] 방향으로만 흐르는 단방향 통신 채널이다. SSE는 클라이언트가 polling과 같이 주기적으로 http 요청을 보낼 필요없이 http 연결을 통해 서버에서 클라이언트로 데이터를 보낼 수 있다. 이 3가지 방법 중 SSE를 이용하여 실시간 알림 기능을 구현하기로 결정했다. polling은 지속적인 요청을 보내야하므로 리소스 낭비가 심할 것 같았고, 실시간 알림같은 경우는 서버에서 클라이언트 방향으로만 데이터를 보내면 되기 때문에 websocket처럼 양방향 통신은 필요없었다. 따라서 웹 소켓에 비해 가볍고 서버 -> 클라이언트 방향을 지원하는 SSE를 선택했다. 🔎 SSE 자세히 알아보기 SSE는 서버의 데이터를 실시간, 지속적으로 클라이언트에 보내는 기술이다. 위의 그림처럼 클라이언트에서 처음 HTTP 연결을 맺고 나면 서버는 클라이언트로 계속하여 데이터를 전송할 수 있다. 일반적으로 HTTP 요청은 하나의 [요청 - 응답] 과정을 거치고 연결을 종료한다. 하지만 파일 전송과 같이 연결 상태를 유지하고 계속 데이터를 보내는 경우도 있다. SSE는 이와 같이 한 번 연결 후 서버에서 클라이언트로 데이터를 계속해서 보낼 수 있다. 특징 websocket과 달리 별도의 프로토콜을 사용하지 않고 HTTP 프로토콜만으로 사용이 가능하며 훨씬 가볍다. 접속에 문제가 있으면 자동으로 재연결을 시도한다. 최대 동시 접속 수는 HTTP/1.1의 경우 브라우저 당 6개이며, HTTP/2는 100개까지 가능하다. IE를 제외한 브라우저에서 지원된다.(Polyfills을 사용하면 가능하다고 한다.) 이벤트 데이터는 UTF-8 인코딩된 문자열만 지원한다. 일반적으로 JSON으로 마샬링하여 전송한다. 클라이언트에서 페이지를 닫아도 서버에서 감지하기가 어렵다. Event Stream 형태 sse 연결을 통해 도착하는 데이터의 형태를 살펴보자 // 두 줄 이상의 연속된 줄은 하나의 데이터 조각으로 간주됨.
// 마지막 행을 제외한 줄은 
(마지막 행은 

)

// 1
data: first line



// 2
data: first line

data: second line

 // 고유 id 같이 보내기.
// id 설정 시 브라우저가 마지막 이벤트를 추적하여 서버 연결이 끊어지면
// 특수한 HTTP 헤더(Last-Event-ID)가 새 요청으로 설정됨.
// 브라우저가 어떤 이벤트를 실행하기에 적합한 지 스스로 결정할 수 있게 됨.

id: 12345

data: first line

data: second line

 // JSON 형식 예시

data: {

data: ""msg"": ""hello world"",

data: ""id"": 12345

data: }

 // event 이름 설정

id: 12345

event: sse

data: {""msg"": ""hello world"", ""id"": 12345}

 🌱 SSE in Spring spring에서 sse을 어떻게 적용하는지 알아보자. spring framework 4.2부터 SSE 통신을 지원하는 SseEmitter 클래스가 생겼다. spring framework 5부터 WebFlux를 이용해서도 sse 통신을 할 수 있지만, SseEmitter를 사용하여 구현해보려고 한다. reference에도 언급했지만 서버단 구현은 https://jsonobject.tistory.com/558 이 블로그를 많이 참고했습니다! 클라이언트 구현 위에서 설명한대로 SSE 통신을 하기 위해서는 처음에는 클라이언트에서 서버로 연결이 필요하다. 클라이언트에서 서버로 sse 연결 요청을 보내기 위해서 자바스크립트는 EventSource를 제공한다. 간단하게 숫자를 입력하고 로그인 버튼을 누르면 해당 숫자(유저 id)로 sse 연결을 맺고 후에 서버에서 해당 유저와의 sse 연결을 통해 데이터가 날라오면 브라우저 알림을 띄우는 코드이다. <!DOCTYPE html>
<html lang=""en"">
<head>
<meta charset=""UTF-8"">
<title>Notification Test Page</title>
</head>
<body>
<input type=""text"" id=""id""/>
<button type=""button"" onclick=""login()"">로그인</button>
</body>
</html>
<script type=""text/javaScript"">
function login() {
const id = document.getElementById('id').value;

const eventSource = new EventSource(`/subscribe/` + id);

eventSource.addEventListener(""sse"", function (event) {
console.log(event.data);

const data = JSON.parse(event.data);

(async () => {
// 브라우저 알림
const showNotification = () => {

const notification = new Notification('코드 봐줘', {
 body: data.content
});

setTimeout(() => {
 notification.close();
}, 10 * 1000);

notification.addEventListener('click', () => {
 window.open(data.url, '_blank');
});
}

// 브라우저 알림 허용 권한
let granted = false;

if (Notification.permission === 'granted') {
granted = true;
} else if (Notification.permission !== 'denied') {
let permission = await Notification.requestPermission();
granted = permission === 'granted';
}

// 알림 보여주기
if (granted) {
showNotification();
}
})();
})
}
</script>
서버 - 컨트롤러 구현 서버에서는 EventSource를 통해 날아오는 요청을 처리할 컨트롤러가 필요하다. sse 통신을 하기 위해서는 MIME 타입을 text/event-stream로 해줘야한다. 현재는 편의를 위해 /subscribe/{id}와 같이 직접 유저의 id를 받게 하였지만, 실제 적용시에는 access-token을 활용할 계획이다. @RestController
public class NotificationController {

private final NotificationService notificationService;

public NotificationController(NotificationService notificationService) {
this.notificationService = notificationService;
}

/**
* @title 로그인 한 유저 sse 연결
*/
@GetMapping(value = ""/subscribe/{id}"", produces = ""text/event-stream"")
public SseEmitter subscribe(@PathVariable Long id,
 @RequestHeader(value = ""Last-Event-ID"", required = false, defaultValue = """") String lastEventId) {
return notificationService.subscribe(id, lastEventId);
}
} 추가적으로 Last-Event-ID라는 헤더를 받고 있는 것을 볼 수 있다. 이 헤더는 항상 담겨있는 것은 아니다. 만약 sse 연결이 시간 만료 등의 이유로 끊어졌을 경우에 알림이 발생하면 어떻게 될까? 그 시간 동안 발생한 알림은 클라이언트에 도달하지 못할 것이다. 이를 방지하기 위한 것이 Last-Event-ID 헤더이다. 이 헤더는 클라이언트가 마지막으로 수신한 데이터의 id값을 의미한다. 이를 이용하여 유실된 데이터를 다시 보내줄 수 있다. 밑에서 자세히 설명할 예정이다. 서버 - 서비스(SSE 연결) 구현 유저의 id와, Last-Event-ID값이 아래의 subscribe()로 넘어온다. 먼저 코드를 보고 설명을 하겠다. @Service
public class NotificationService {
private static final Long DEFAULT_TIMEOUT = 60L * 1000 * 60;

private final EmitterRepository emitterRepository;

public NotificationService(EmitterRepository emitterRepository) {
this.emitterRepository = emitterRepository;
}

public SseEmitter subscribe(Long userId, String lastEventId) {
// 1
String id = userId + ""_"" + System.currentTimeMillis();

// 2
SseEmitter emitter = emitterRepository.save(id, new SseEmitter(DEFAULT_TIMEOUT));

emitter.onCompletion(() -> emitterRepository.deleteById(id));
emitter.onTimeout(() -> emitterRepository.deleteById(id));

// 3
// 503 에러를 방지하기 위한 더미 이벤트 전송
sendToClient(emitter, id, ""EventStream Created. [userId="" + userId + ""]"");

// 4
// 클라이언트가 미수신한 Event 목록이 존재할 경우 전송하여 Event 유실을 예방
if (!lastEventId.isEmpty()) {
Map<String, Object> events = emitterRepository.findAllEventCacheStartWithId(String.valueOf(userId));
events.entrySet().stream()
.filter(entry -> lastEventId.compareTo(entry.getKey()) < 0)
.forEach(entry -> sendToClient(emitter, entry.getKey(), entry.getValue()));
}

return emitter;
}

// 3
private void sendToClient(SseEmitter emitter, String id, Object data) {
try {
emitter.send(SseEmitter.event()
 .id(id)
 .name(""sse"")
 .data(data));
} catch (IOException exception) {
emitterRepository.deleteById(id);
throw new RuntimeException(""연결 오류!"");
}
}
} [코드의 1번 부분] subscribe()를 보면 id값을 ${user_id}_${System.currentTimeMillis()} 형태로 사용하는 것을 볼 수 있다. 이렇게 사용하는 이유가 Last-Event-ID 헤더와 상관이 있다. Last-Event-ID헤더는 클라이언트가 마지막으로 수신한 데이터의 id값을 의미한다고 했다. id값과 전송 데이터를 저장하고 있으면 이 값을 이용하여 유실된 데이터 전송을 다시 해줄 수 있다. 하지만 만약 id값을 그대로 사용한다면 어떤 문제가 있을까? id값을 그대로 사용한다면 Last-Event-Id값이 의미가 없어진다. Last-Event-Id = 3

{3, data1}
{3, data3}
{3, data2}

=> 어떤 데이터까지 제대로 전송되었는지 알 수 없다. 데이터의 id값을 ${userId}_${System.currentTimeMillis()} 형태로 두면 데이터가 유실된 시점을 파악할 수 있으므로 저장된 key값 비교를 통해 유실된 데이터만 재전송 할 수 있게 된다. Last-Event-Id = 3_1631593143664

{3_1631593143664, data1}
{3_1831593143664, data3}
{3_1731593143664, data2}

=> data1 까지 제대로 전송되었고, data2, data3을 다시 보내야한다. 이런 이유로 인해 id값을 ${user_id}_${System.currentTimeMillis()}로 두는 것이다. [코드의 2번 부분] 클라이언트의 sse연결 요청에 응답하기 위해서는 SseEmitter 객체를 만들어 반환해줘야한다. SseEmitter 객체를 만들 때 유효 시간을 줄 수 있다. 이때 주는 시간 만큼 sse 연결이 유지되고, 시간이 지나면 자동으로 클라이언트에서 재연결 요청을 보내게 된다. id를 key로, SseEmitter를 value로 저장해둔다. 그리고 SseEmitter의 시간 초과 및 네트워크 오류를 포함한 모든 이유로 비동기 요청이 정상 동작할 수 없다면 저장해둔 SseEmitter를 삭제한다. [코드의 3번 부분] 연결 요청에 의해 SseEmitter가 생성되면 더미 데이터를 보내줘야한다. sse 연결이 이뤄진 후, 하나의 데이터도 전송되지 않는다면 SseEmitter의 유효 시간이 끝나면 503응답이 발생하는 문제가 있다. 따라서 연결시 바로 더미 데이터를 한 번 보내준다. [코드의 4번 부분] 1번 부분과 관련이 있는 부분이다. Last-Event-ID값이 헤더에 있는 경우, 저장된 데이터 캐시에서 id 값과 Last-Event-ID값을 통해 유실된 데이터들만 다시 보내준다. 서버 - 서비스(데이터 전송) 구현 위의 부분은 클라이언트와 서버가 sse 연결을 맺는 부분이었고, 이제 실제로 서버에서 클라이언트로 일방적인 데이터를 보내는 부분을 하나 구현해보려고 한다. 맨 처음에 얘기한 예시들 중 새로운 리뷰 요청이 생겼을 때 리뷰어에게 알림을 보내는 예시를 구현해보자. 아래 코드는 실제로 클라이언트에 데이터를 전송하는 부분이다. 클라이언트에 보낼 데이터 형식인 Notification 객체를 만들고, 현재 로그인 한 유저의 id값을 통해 SseEmitter를 모두 가져온다. 그 후, 데이터 캐시에도 저장해주고, 실제로 데이터 전송도 한다. @Service
public class NotificationService {
// ...

public void send(Member receiver, Review review, String content) {
Notification notification = createNotification(receiver, review, content);
String id = String.valueOf(receiver.getId());

// 로그인 한 유저의 SseEmitter 모두 가져오기
Map<String, SseEmitter> sseEmitters = emitterRepository.findAllStartWithById(id);
sseEmitters.forEach(
(key, emitter) -> {
// 데이터 캐시 저장(유실된 데이터 처리하기 위함)
emitterRepository.saveEventCache(key, notification);
// 데이터 전송
sendToClient(emitter, key, NotificationResponse.from(notification));
}
);
}

private Notification createNotification(Member receiver, Review review, String content) {
return Notification.builder()
 .receiver(receiver)
 .content(content)
 .review(review)
 .url(""/reviews/"" + review.getId())
 .isRead(false)
 .build();
}

private void sendToClient(SseEmitter emitter, String id, Object data) {
try {
emitter.send(SseEmitter.event()
 .id(id)
 .name(""sse"")
 .data(data));
} catch (IOException exception) {
emitterRepository.deleteById(id);
throw new RuntimeException(""연결 오류!"");
}
}
} 실제로 알림을 보내고 싶은 로직에서 send 메서드를 호출해주면 된다. @Service
public class ReviewService {
// ...
@Transactional
public Long create(LoginMember loginMember, ReviewRequest reviewRequest) {
// ...
notificationService.send(teacher, savedReview, ""새로운 리뷰 요청이 도착했습니다!"");

return savedReview.getId();
} 아직 실제 서버에 적용되지는 않았지만, 로컬에서 리뷰 요청을 했을 경우 알림이 뜨는 걸 볼 수 있다! 상황 설명: 로그인 한 리뷰어의 id가 5인경우, 5번 리뷰어에게 누군가 새로운 리뷰 요청을 했을 경우. 확장성 고려하기! 위의 구조는 단일 WAS 사용시는 문제없이 동작한다. 하지만 만약 여러 WAS를 사용하는 경우 클라이언트는 알림을 받지 못할 수 있다. 이런 경우를 대비하여 Redis의 pub/sub을 활용해 보려고 한다. (추가하기) 🚀 트러블 슈팅 EventSource에 토큰 담기 (추가하기) SSE 테스트 (추가하기) 데이터 캐시 비우기 (추가하기) 브라우저가 닫힌 경우 (추가하기) 📄 reference https://jsonobject.tistory.com/558 https://codeburst.io/polling-vs-sse-vs-websocket-how-to-choose-the-right-one-1859e4e13bd9 https://boxfoxs.tistory.com/403 https://surviveasdev.tistory.com/entry/%EC%9B%B9%EC%86%8C%EC%BC%93-%EA%B3%BC-SSEServer-Sent-Event-%EC%B0%A8%EC%9D%B4%EC%A0%90-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B3%A0-%EC%82%AC%EC%9A%A9%ED%95%B4%EB%B3%B4%EA%B8%B0 https://www.baeldung.com/spring-server-sent-events https://medium.com/javarevisited/building-scalable-facebook-like-notification-using-server-sent-event-and-redis-9d0944dee618",BE
878,"이 글은 캡틴 포비의 <이슈 관리, 일정 추정> 강의 내용을 정리하고, 적용해본 경험을 담은 글입니다. 💬 여기서부터는 포비(박재성 개발자님)의 목소리로 읽어주세요 :) 우아한테크코스에서는 본과정 시작 전 입문 테스트 단계인 프리코스 때부터도 To Do List(a.k.a 구현기능목록)를 만드는 연습을 하게 된다. To Do List를 만드는 작업은 바로 이슈를 만드는 작업이라고도 할 수 있다. 이렇게 기능 목록을 작은 단위로 쪼개고 그들의 우선순위를 정하는 것은 상당히 중요한 '설계' 과정이다. 이슈를 어떻게 관리하는지는 굉장히 핵심적인 부분이고, 이슈관리를 잘하는 것은 후속작업을 위해서도 중요하다. 그러나 개발자들은 이슈를 제대로 관리하지 않고 바로 개발에 들어가는 경향이 있다. 프로젝트 시작에 앞서 어떻게 이슈를 관리하고 일하는 것이 효과적일지 고민해보자. 👨🏻‍🏫 이슈 생성, 우선순위 부여 프로젝트에서 작업할 이슈를 어떻게 관리하는 것이 좋을까? 이슈관리를 할 때 가장 중요한 것은 이 이슈들을 충분히 작은 단위로 만들어 독립된 단위로 만드는 것이다. 그렇게 해야 누군가에게 이슈를 배정해서 그걸로 끝낼 수 있다. 예를 들어, '스터디를 하겠다'는 너무 단위가 크다. 더 디테일하게 하나하나 충분히 작은 단위로 쪼개야 한다. 우선순위도 중요하다. 이슈에 대한 우선순위 관리가 잘 되어있어야지만 개인별로 작업 배분이 가능하다. 너무 빠르게 개발로 들어가려고 하지 마시고, 이슈들을 설계하는데 많은 시간을 투자했으면 좋겠다. 👨🏻‍🏫 업무 분담 그렇다면 업무 배분은 어떻게 관리하는 것이 좋을까? 똑같은 일을 하더라도 내가 주도적으로 하면 즐겁게 하지만 남이 시키면 하기 싫어진다. push 방식 보통은 팀장이 주도하면서 구성원들의 업무 현황을 본 다음에 일괄적으로 업무를 배분한다. 사실 업무를 배분하는 입장에서도 되게 재미없고 스트레스를 받는다. 왜냐하면 어쩔 수 없이 되게 재미없는 업무가 있는데 이 지루하고 반복적인 업무를 누군가에게는 줘야 하는 부담이 있기 때문이다. pull 방식 한편, 수평적인 관계에서도 업무 배분이 어려울 수 있다. 그럴 때는 담당자가 명확한 이슈를 처음에 배분해놓고, 명확하지 않으면 업무를 배정하지 않고 우선순위만 정해놓는 방법이 있다. 구성원 중 한 명이 맡고 있던 업무가 끝났으면, 남은 이슈 중 우선순위가 가장 높은 업무를 자기에게 무조건 당겨온다. 새로운 기술로 치고나가는 팀원들이 재미있는 업무를 다 해결하게 되는 경향이 있다. 실은 배려심이 강하고 허드렛일이고 다른 사람이 챙기지 않기 때문에 챙기는 사람이야말로 팀워크를 형성하는데 중요한 역할을 한다. 본인이 팀에서 어떻게 일하는지 보고, 내가 팀에서 너무 재밌는 일만 하려는 성향이 있는 게 아닌가 그러면 경계심을 가지고 본인이 일하는 방식을 돌아보자. 허드렛일도 나서서 하는 모습을 보이면 좋겠다. 그런 허드렛일을 나눠야지만 같이 성장하고 팀워크도 생긴다. Push 방식 Pull 방식 리더가 프로젝트 전체 조율 및 업무 배정 작업자가 명확한 경우를 제외하고는 우선순위에 따라 스스로가 본인의 업무를 할당 대부분의 수직 구조 회사의 업무 배정 방식 수평구조에 적합한 방식 👨🏻‍🏫 일정 추정 각 이슈별 일정 추정은 어떻게 하는 것이 좋을까? 경험 많은 시니어에게 일방적으로 맡기는 게 좋을까? 아니면 경험이 없는 사람이 하는게 좋을까? 일정 추정이 힘든 이유는 해당 도메인에 대한 지식이 낮기 때문이다. 힘든 작업. 도메인 지식이 이미 있는 소프트웨어인 경우에는 그나마 일정 추정을 잘하지만, 도메인 지식이 없는 경우 시니어 개발자는 상당히 공격적으로 일정을 추정하는 경향이 있다. 그래서 오히려 약간 보수적으로 잡는 주니어가 추정을 잘하는 경우도 있다. 일정 추정 에피소드 1 - 팀장과 팀원 팀원: (마음 속으로 5일 정도 걸릴 것으로 예상함) 팀장: 이 기능 하렴 되지 않아? 내가 예전에는 말이야... 팀원: 하루는 너무 짧고... 최소 3일은 필요합니다. 팀장: 3일도 너무 긴데... 2일로 잡으면 안 될까? 팀원: (ㅠ.ㅠ) 예. (오늘부터 야근해야겠군.) 위와 같은 대화를 통해서는 정해진 일정은 사실은 확신할 수 없는 일정이다. 팀원 입장에서는 팀장이 더 일정을 더 잘 추정할 것 같다는 생각도 들고, 너무 보수적으로 잡으면 실력이 없어 보일까 봐 걱정도 되고, 또 팀장에게 의견을 크게 낼 수 없기도 결국 이런 리스크를 만들게 된다. (구구: 3일 만에 만들면 3일이면 만드는구나라고 생각하게 되는 문제도 있습니다.) 일정 추정 에피소드 2 - 개발자와 기획자 개발자: 5일 정도 예상해요. 기획자: 그렇게 많이요? 버튼 하나 추가하는데... (의심의 눈초리...) 개발자: DB가 어떻고, 테스트가 어떻고, 리팩토링이 어쩌네... (이런 것도 모르냐? 기술 공부 좀 해라) 기획자: (멍한 표정. 의심이 가지만) 그렇게 하시죠... 위와 같은 대화를 통해서는 서로에 대한 신뢰만 상실하게 될 뿐이다. 일정 추정을 잘~ 하기 위해서 '플래닝 포커'를 통해 스토리 포인트를 산정하는 방법을 시도해볼 수 있다. (플래닝 포커 앱도 있음) 플래닝 포커는 대략적인 기획을 완료해 기능목록과 우선순위가 결정된 후에 진행한다. 스프린트를 시작하는 시점에 진행한다. 예를 들어 마일스톤이 2주 단위이면, 2주 중 가장 첫째 날에 플래닝 포커를 진행하면 된다. 해당 마일스톤에서 진행 가능한 기능 목록을 결정하고 스토리 포인트 산정을 진행한다. 상대적인 스토리포인트스토리 포인트 1점을 기준으로 다른 이슈의 스토리 포인트를 정한다. 참고로 스토리 포인트는 5 이하로 정해지는 것이 좋다. 5보다 크다면 이슈가 너무 크게 정해진 것이고 더 잘게 쪼갤 필요가 있다. 포비 수업자료 중 - 실제 플래닝 포커 모습 / 기능 목록 정리 예시 💬 여기서부터는 제 목소리로 읽어주세요 :) 🚀 적용해보기 '여기서만나' 팀 프로젝트가 본격적으로 시작되면서, 강의 중 요약해주신 다음의 순서를 따라 본격적으로 이슈 관리에 도전해보았다. 1. 작업 목록 이슈를 정리한다. 2. 작업 목록의 우선순위를 결정한다. 3. 각 작업의 스토리 포인트를 산정한다. 4. 모든 작업을 이슈관리시스템에 등록한다. (e.g 깃허브) 1. 이슈 정리 우선은 우선순위나 일정에 대한 고려 없이 일단 쭈욱- 적어보는 것으로 시작했다. 아주 조금이라도 구체화된 아이디어를 모두 나열하니 상당히 양이 많았다. '여기서만나' 팀 노션에 정리한 이슈 목록 2. 우선순위 부여 그러고 나서 각 기능의 우선순위를 다음 3가지로 구분했다. 1번. 우리 서비스에 필수적인 기능 2번. 선택적으로 구현해도 되는데, 있으면 좀 많이 좋은 기능 3번. 선택적으로 구현해도 되는데, 역시 안 해도 될 것 같은 기능 한편, 6주라는 고정된 시간 안에 완성을 해야 했기 때문에 우선순위를 결정하기 위해서는 프로덕트 그 자체에 대한 고민뿐만 아니라 우리들에게 주어진 시간 리소스도 고려해야 했다. 그래서 구체적인 일정 산정에 들어가기 전이긴 하지만, 우선순위 구분과 함께 각 '스프린트 별'로 이슈 카드를 배치해서 총 6주라는 타임라인 안에서 각 이슈가 소화될 수 있을지 개략적으로 감을 잡을 수 있도록 했다. 첫 스프린트에는 오로지 MVP 완성만을 목표로 하다 보니, 1번의 설명 그대로 서비스에 필수적인 기능만 담게 되었다. '여기서만나' 팀 노션 - 기능 우선순위 3. 스토리포인트 산정 우리 팀은 스토리 포인트 1점은 1시간으로 정하고, 30분 단위까지만 쪼개기로 했다. 이렇게 하니까 시간 단위로 딱 떨어져 직관적으로 이해할 수 있었다. 또 0.5 단위로 나누어 떨어져 표기할 때도 별 고민 없이 깔끔하게 표기할 수 있었다. 예를 들어 [1.5]라고 적혀있으면 1시간 30분 걸리는 이슈로 추정한 것이다. 스토리 포인트 산정은 페어끼리 자체적으로 수행했다. 나와 나의 페어(심바)는 구현 기능목록을 순차적으로 훑으면서 상대적인 포인트를 산정했다. 로그인 기능은 '2점', 그보다 간단한 내비게이션 바 라우팅 기능은 '1점'과 같은 식이었다. (돌아보니 매우 주먹구구...) 4. 이슈 등록 우리 팀은 소스코드 저장소인 깃허브에서 지원하는 이슈관리 시스템을 이용하기로 했다. 무료이고, 접근성이 좋기 때문이다. 팀 repository에 우리 팀에게 필요한 라벨을 생성하고, 마일스톤도 등록했다. 그리고 이슈를 등록했다. 이슈의 제목 맨 앞부분에 대괄호에 산정한 스토리 포인트를 적는 것을 우리 팀의 이슈 제목 컨벤션으로 맞추었다. '여기서만나' 프로젝트 이슈 프로젝트의 진행 현황을 한눈에 파악할 수 있는 칸반 보드도 생성했다. Projects 메뉴에서 등록할 수 있다. PR이 올라오거나, 리뷰가 진행되거나 merge 되었을 때 등등 자동으로 카드가 이동되어 편리했다. '여기서만나' 프로젝트 칸반보드 + 사후 평가 스프린트 1에서 소요될 것으로 예상한 스토리 포인트는 총 23.5 점이었다. 스토리 포인트 산정을 마치고 페어에게 순진무구하게 얘기한 기억이 난다. ???: ""우리 딱 4일이면 끝나겠는데요? 너무 여유로워서 필수 기능 말고 선택 기능도 욕심낼 수 있을 것 같아요."" '여기서만나' 팀 노션 - 스프린트1 소요시간 기록 실제로는 47.5 포인트가 소요되었다. 스토리 포인트를 추정하는 과정에서 미처 고려하지 못했던 것들이 많았다. 우선, UI를 디자인하는 시간을 빼먹었다. 한 페이지의 UI를 구성하기 위해 figma 페이지에서 작업하는 시간이 1시간 이상되는데 이 시간을 전혀 고려하지 못했다. 또, 새로운 기술을 학습하는 시간을 고려하지 못했다. dev툴로 비동기 데이터를 간편하게 보고 로딩 UI나, 캐싱 구현을 편하게 하기 위해 react-query 를 도입했는데, 처음 사용하는 api를 익히고 삽질하는데 드는 시간이 만만치 않았다. 지도 정보를 표시하기 위해 사용한 카카오 api 의 예제를 확인하고 적용해보는데도 시간이 많이 들었다. 마지막으로, 버그를 해결하는 시간을 따로 마련하지 않은 것도 실수였다. 스프린트 1 때는 구현만 하면 끝날 것이라고 철석같이 믿고 새로 생긴 이슈를 대응하는 버퍼 시간에 대한 고려가 전혀 없었다. 이렇게 누락된 요소가 많다보니 예상한 시간보다 2배가 더 걸릴 만했다. 발표자료 - 스프린트1 프론트엔드 이슈완료 현황 & 원래 일정 추정은 이런거라고 말씀해주시는 서윗 포비와 2배 정도면 잘된거라고 말씀해주시는 서윗 공원 스프린트 3까지 마친 지금은 버그가 없을 수는 없다는 사실을 받아들이고, 일정 산출 시 버퍼 구간을 마련하는 노하우가 생겼다. 아직도 일정 추정은 너무 어렵지만, 마지막 스프린트 때는 드물게 딱 맞게 일정을 산정한 적도 있었다. 분명 이슈 관리와 일정 추정은, 단기간 내에 습득할 수 있는 역량이 아니다. 그래서 캡틴 포비께서 우리 새싹들이 조금이나마 빨리 직접 부딪혀보고 이런 고민을 시작해보라는 의미에서 강의를 진행해주신 게 아닐까 하는 생각이 든다. 어떻게 이슈를 관리하고 일하는 것이 효과적일지 계속 고민을 이어가면서 언젠가는 일정 추정의 마스터가 되어보자. 💪",NON_TECH
881,"프로젝트를 하면서 페어인 심바와 가장 많이 했던 말 중 하나는 ""역시 UI는 너무 어려워""였다. UI 개발은 정답이 뭔지도 모르겠고, 시간도 많이 소요되었다. 코드 짤 시간이 상당량 줄어드는 게 pain point였지만 그래도 재미있는 작업이었다. 언제 또 이런 경험을 해볼 수 있을까 싶은 마음으로 재밌게 고통받았다. JS 로직 작성 시간 못지않게 UI 개발에 시간을 들였다보니 간단하게나마 글로 남기고 싶다는 생각이 들었다. 그래서 figma에서 작업한 흔적을 모아 글로 엮어보았다. 초기 프로토타입 (개발 전) 발표를 위해 간단하게 제작한 프로토타입이다. figma가 아닌 PPT에서 도형으로 만들었다. 반나절만에 만들어서 현재 결과물과 차이가 있다. PPT 작업물 - UI 프로토타입 @ 중간지점 검색 기능 참석자추가 폼 UI 직접 입력 버튼이 사라지고 기본으로 직접 입력이 되도록 바뀐 것이 주요한 변화이다. '팔로잉 목록'에서 '간편 추가'라고 버튼 이름이 바뀌기도 했다. 우리 서비스에서 사용하는 '팔로워-팔로잉' 개념은 개발하는 우리도 헷갈려서 문제가 있다고 느꼈고, 친구 추가 메커니즘을 새로 정의하면서 사용하지 않기로 했다. 피그마 작업물 - 참석자 추가 폼 UI 주소 검색 모달 UI 가장 처음 만든 모달 UI이다. UI 자체는 크게 고민이 없었다. 그보다 이미 있는 Input 컴포넌트를 어떻게 활용해서 svg 아이콘으로 된 버튼까지 딸린 Input으로 만들지가 더 고민이었다. 피그마 작업물 - 주소검색 모달 UI 지도 핀 UI 우리 프로젝트의 theme 색상 중 여러 색 조합, 크기 조합으로 지도에 띄워보고, 전체적인 밸런스나 가시성이 괜찮은 조합으로 선택했다. 피그마 작업물 - 지도 핀 UI 장소 카테고리 Chip UI 여기서만나 UI를 개발하면서 가장 큰 도움이 된 레퍼런스는 단연, 네이버 지도, 카카오 지도 그리고 구글 지도이다. 이 중 장소 카테고리 Chip UI로는 네이버의 장소 카테고리 chip을 아주 많이 참고했다. 개발 초기부터 생각했던 디자인이라 UI 개발 자체에 큰 문제는 없었다. 코드로 UI를 구현하고 나니 미처 생각하지 못한 문제가 생겼다. 사진처럼 참석자 출발지 Pin이 가려지는 현상이 왕왕 발견되었다. Chip의 위치를 옮기기엔 저 위치가 너무도 최적의 위치이다 보니 지도의 boundary를, 지도의 배율에 따라 Chip의 마진 + 높이만큼 수동으로 buffer를 주는 코드를 추가해서 해결했다. <네이버> 장소 카테고리 Chip <여기서만나> 장소 카테고리 Chip UI 경로 표시 UI 참석자를 선택하는 컨트롤러를 어디에 넣을지가 가장 고민되었다. 상단, 하단, 좌측, 우측 다 넣어봤는데 상단이 제일 깔끔하고 흐름도 자연스럽다고 판단돼서 상단에 배치했다. 피그마에서 작업할 때 아바타 일러스트 중 가장 정감이 가는 무스를 예시로 가져다 썼었다. 피그마 작업물 - 경로 표시 UI 반응형 UI 모바일에서는 한정된 화면 크기로 인해 '지도'와 '경로 정보'를 한 번에 표시할 수 없었다. 고민 끝에 Drawer 를 추가하는 방식으로 표현하기로 결정했다. 스르륵 올라오는 애니메이션을 넣어주니 꽤 앱스러운(?) 느낌이 나서 만족스러웠다. 반응형 UI 로딩바 UI 로딩바 디자인은 페어 심바가 말해준 figma 로딩 UI를 많이 참고했다. 사용자에게 정말 진행되고 있다는 느낌을 주고 싶어서 keyframes를 한땀한땀 작성한 기억이 남는다. 만들 때는 로딩 스피너랑 로딩 바를 계속 보니까 멀미가 나서 고생했다. 그래도 의도한 것처럼 '무언가가 진행되고 있다'라는 느낌이 잘 나온 것 같다. 로딩바 표시 UI @ 로그인 기능 로그인 UI 로그인 페이지는 어떤 소셜 로그인을 선택할지만 고르는 아주 간단한 페이지이다. 카카오, 네이버의 디자인 가이드라인을 지켜서 버튼을 넣어야하는 경험이 새로웠다. 페이지의 내용물이 너무 간단한 나머지 휑해보여서 밋밋하지 않게 일러스트를 그려 넣게 되었다. 로그인 페이지 UI @ 친구관리 기능 친구 카드 UI 친구 카드는 정렬은 처음에 두 칸짜리 grid로 디자인을 정하고 구현까지 마쳤는데, 산만한 느낌이 들어 한 칸짜리 flex로 변경했다. 이렇게 UI 디자인 초안을 잡고 나서 코드를 작성하면서 디자인이 변경되는 경우가 많았다. 페어 심바와 둘이서 만든 디자인이니 수정이 매우 자유로웠다. 한 번에 좋은 디자인을 떠올리기 어렵다 보니 프로젝트 후반부에는 figma에서 어느 정도 작업을 하면, 코드에 녹여보면 더 좋은 배합이 생각날 테니 이 정도로 만족하고 코드 작성 단계로 넘어가기도 했다. 친구 카드 UI 친구 검색 UI 페어 심바가 찾아준 카카오 아이디 검색 UI를 많이 참고했다. 이제 보니 참고한 수준이 아니라 복-붙 수준인 것 같다. ㅎㅎ 친구 검색 UI @ 게시판 기능 게시판 UI 게시판에서는 라벨을 어떻게 처리할지에 시간을 들였다. 디자인 일관성을 유지하려고 게시글 라벨을 filled로 꾸며봤는데, 사용자가 라벨이 아니라 버튼같은 오해할 수도 있고 전체적인 밸런스감이 안좋다고 생각해서 outlined로 정하게 되었다. 피그마 작업물 - 게시글 조회 UI 게시글 조회/작성 UI 게시글 조회나 작성 UI는 게시판에 비해 간단해서 금방 결정되었다. 게시글 조회에서 제목을 포함한 게시글 정보의 레이아웃은 tistory의 UI를 많이 참고했다. tistory - 게시글 정보 피그마 작업물 - 게시글 조회/작성 UI",NON_TECH